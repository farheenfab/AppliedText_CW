{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farheenfab/AppliedText_CW/blob/main/CW1-generate_dataset.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F20AA Coursework 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect\n",
    "import shutil\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the api service name, version and developer key for the api call.\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "# AIzaSyC8VqY2cYxX7jOouIF076rpM8lvT1ZBJu4\n",
    "# AIzaSyAWj_uzrhZL18X32S_P79pT1wnSYGpuA4k\n",
    "DEVELOPER_KEY = \"AIzaSyAWj_uzrhZL18X32S_P79pT1wnSYGpuA4k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/search/list#parameters\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/comments/list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a class called api_handler which contains functions such as `get_video_details()`, `get_videos()`, `get_video_df()`, `get_comments()`, `get_comment_replies()`, `get_comments_df()`, `create_video_df_from_search()`, `create_video_df()`. These functions help us by either manually retrieving the videos and comments or by automatically curating the videos and comments using the product given to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class api_handler:\n",
    "    def __init__(self, api_service_name, api_version, developer_key):\n",
    "        self.client = googleapiclient.discovery.build(api_service_name,\n",
    "                                                    api_version,\n",
    "                                                    developerKey=developer_key)\n",
    "        \n",
    "    # Search for videos details given id\n",
    "    def get_video_details(self, videoId, part=\"snippet\"):\n",
    "        request = self.client.videos().list(\n",
    "            part=part,\n",
    "            id=videoId\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response:\n",
    "            video_details = response['items'][0]\n",
    "            snippet=video_details['snippet']\n",
    "            snippet['videoId']=videoId\n",
    "            snippet['id']=videoId\n",
    "            snippet['publishTime']=video_details.get('snippet', {}).get('publishedAt', {})\n",
    "            snippet['thumbnails']=video_details.get('snippet', {}).get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "            return snippet\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Search for videos given query\n",
    "    def get_videos(self,query,maxResults=5,part=\"snippet\"):\n",
    "        request = self.client.search().list(\n",
    "            part=part,\n",
    "            maxResults=maxResults,\n",
    "            # higher view count is likely to be more relevent \n",
    "            order=\"viewCount\",\n",
    "            q=query,  \n",
    "            # american region videos \n",
    "            regionCode=\"US\",\n",
    "            # english videos\n",
    "            relevanceLanguage=\"en\",\n",
    "            type=\"video\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        return response\n",
    "    \n",
    "    # Format Response from get_videos to dataframe\n",
    "    def get_video_df(response):\n",
    "        items=[]\n",
    "        for item in response['items']:\n",
    "            snippet=item.get('snippet', {})\n",
    "            items+=[{\n",
    "                'title':snippet.get('title', ''),\n",
    "                'videoId':item.get('id', {}).get('videoId', ''),\n",
    "                'channelTitle':snippet.get('channelTitle', ''),\n",
    "                'publishTime':snippet.get('publishTime', ''),\n",
    "                'description':snippet.get('description', ''),\n",
    "                'thumbnails':snippet.get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "                }]\n",
    "        df=pd.DataFrame(items)\n",
    "        return df\n",
    "    \n",
    "    # Get comments from video\n",
    "    def get_comments(self,videoId,part=\"snippet\",maxResults=100,maxResultsDepth=100):\n",
    "        all_comments = []\n",
    "        f = 0\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0:\n",
    "            request = self.client.commentThreads().list(\n",
    "                part=part,\n",
    "                videoId=videoId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                order='relevance',\n",
    "                moderationStatus='published',\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if 'items' in response:\n",
    "                all_comments+=[response]\n",
    "                for item in response['items']:\n",
    "                    # extract the comment ID to get replies\n",
    "                    comment_id = item.get('snippet',{}).get('topLevelComment',{}).get('id','')\n",
    "                    if item.get('snippet',{}).get('totalReplyCount',0)>2:\n",
    "                        if f == 0:\n",
    "                            print('getting replies:',item.get('snippet',{}).get('totalReplyCount',0))\n",
    "                            f = 1\n",
    "                        replies = self.get_comment_replies(comment_id, maxResults=maxResultsDepth)\n",
    "                        all_comments += replies\n",
    "\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "    \n",
    "    # Get replies from comment \n",
    "    def get_comment_replies(self, commentId, part=\"snippet\", maxResults=100):\n",
    "        all_comments = []\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0 and (nextPageToken != None or len(all_comments)==0):\n",
    "\n",
    "            request = self.client.comments().list(\n",
    "                part=part,\n",
    "                parentId=commentId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "            if 'items' in response and len(response['items'])>0:\n",
    "                for item in response['items']:\n",
    "                    modified_response = {\n",
    "                        'items': [\n",
    "                            {\n",
    "                                'id':item.get('id'),\n",
    "                                'snippet': {\n",
    "                                    'topLevelComment': {\n",
    "                                        'snippet': item.get('snippet','')\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    all_comments += [modified_response]\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "\n",
    "    # Format response from get_comments to dataframe\n",
    "    def get_comments_df(response, video,product):\n",
    "        comments = []\n",
    "        for pages in response:\n",
    "            for item in pages['items']:\n",
    "                comment = item.get('snippet', {}).get('topLevelComment', {}).get('snippet', {})\n",
    "                comments.append([\n",
    "                        product,\n",
    "                        video.get('title', ''),\n",
    "                        video.get('videoId', ''),\n",
    "                        video.get('channelTitle', ''),\n",
    "                        video.get('publishTime', ''),\n",
    "                        video.get('description', ''),\n",
    "                        video.get('thumbnails', ''),\n",
    "                        item.get('id', ''),  \n",
    "                        comment.get('parentId', ''),  \n",
    "                        comment.get('authorDisplayName', '')[1:],  \n",
    "                        comment.get('publishedAt', ''),\n",
    "                        comment.get('updatedAt', ''),\n",
    "                        comment.get('likeCount', ''),\n",
    "                        comment.get('textDisplay', '')\n",
    "                    ])\n",
    "\n",
    "        df = pd.DataFrame(comments,\n",
    "            columns=['product', 'v_title', 'v_videoId',\n",
    "                    'v_channelTitle', 'v_publishTime',\n",
    "                    'v_description', 'v_thumbnail',\n",
    "                    'c_id','c_parentId',\n",
    "                    'c_author', 'c_published_at',\n",
    "                    'c_updated_at', 'c_like_count',\n",
    "                    'c_text'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Search for videos related to products iteratively\n",
    "    # Collect comments from each video and place it into an array\n",
    "    def create_video_df_from_search(self, products,\n",
    "                                    number_of_videos_per_product=5,\n",
    "                                    number_of_comments_per_video=100\n",
    "                                    ,number_of_replies_per_comment=100):\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            # get 25 first videos with the highest viewer counts \n",
    "            response = self.get_videos(query=product, maxResults=number_of_videos_per_product)\n",
    "            # Convert results to df\n",
    "            videos_df = api_handler.get_video_df(response)\n",
    "            # For each video get a maximum of 100 comments\n",
    "            # and place comments into an array\n",
    "            for _, video in videos_df.iterrows():\n",
    "                try:\n",
    "                    response = self.get_comments(video['videoId'], maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment)\n",
    "                    comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                except:\n",
    "                    # Function fails as the API returns 403 if the channel has comments disabled\n",
    "                    # place an empty entry instead it can be deleted later\n",
    "                    comments_df = pd.DataFrame(np.zeros((1, 14)),\n",
    "                                                columns=['product', 'v_title', 'v_videoId',\n",
    "                                                        'v_channelTitle', 'v_publishTime',\n",
    "                                                        'v_description', 'v_thumbnail',\n",
    "                                                        'c_id','c_parentId',\n",
    "                                                        'c_author', 'c_published_at',\n",
    "                                                        'c_updated_at', 'c_like_count',\n",
    "                                                        'c_text'])\n",
    "                    print('Unable to retrieve comments:', video.get('title', ''))\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "        return multiple_video_comments\n",
    "        \n",
    "    # alternative method by explicitely specifying videos\n",
    "    def create_video_df(self,products,videos,number_of_comments_per_video=100,number_of_replies_per_comment=100):\n",
    "        count=0\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            for video in videos[count]:\n",
    "                response = self.get_comments(video,maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment) \n",
    "                video=self.get_video_details(video)\n",
    "                comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "            count+=1\n",
    "        return multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen the Korean Drama called Squid Game to perform the sentiment analysis on. We specify the product in the products list, create a `api_handler` class object, use the `create_video_df_from_search()` function to automatically curate comments using the YouTube api call, and get a pandas Dataframe in return containing details about the videos and the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "products=[\"Squid Game Korean Drama (2021)\"]\n",
    "\n",
    "youtube=api_handler(api_service_name, api_version, DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting replies: 750\n",
      "getting replies: 521\n",
      "getting replies: 16\n",
      "getting replies: 62\n",
      "getting replies: 64\n",
      "getting replies: 14\n",
      "getting replies: 504\n",
      "getting replies: 101\n",
      "getting replies: 350\n",
      "getting replies: 16\n",
      "getting replies: 5\n",
      "getting replies: 14\n",
      "getting replies: 318\n",
      "getting replies: 25\n",
      "getting replies: 230\n",
      "getting replies: 390\n",
      "getting replies: 16\n",
      "getting replies: 154\n",
      "getting replies: 15\n",
      "getting replies: 461\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008552</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514609</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>435</td>\n",
       "      <td>Your videos are so interesting ❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1712</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>266</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42765</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TCfbJV8URO</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>allure_ism</td>\n",
       "      <td>2021-10-07T18:11:30Z</td>\n",
       "      <td>2021-10-07T18:11:30Z</td>\n",
       "      <td>6</td>\n",
       "      <td>Nobody would pick paper or rock (I mean it dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42766</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFObZDOfQq</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>gabrieleDATass</td>\n",
       "      <td>2021-10-08T19:31:58Z</td>\n",
       "      <td>2021-10-08T19:31:58Z</td>\n",
       "      <td>1</td>\n",
       "      <td>How do you kill someone with paper lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42767</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFPKHqbzEu</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>snowrider9018</td>\n",
       "      <td>2021-10-08T19:38:13Z</td>\n",
       "      <td>2021-10-08T19:38:13Z</td>\n",
       "      <td>2</td>\n",
       "      <td>@@gabrieleDATass sand-paper them to death, idk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42768</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFkpMvsZvz</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>Son9</td>\n",
       "      <td>2021-10-08T22:54:50Z</td>\n",
       "      <td>2021-10-08T22:54:50Z</td>\n",
       "      <td>0</td>\n",
       "      <td>@@gabrieleDATass Shove it down their throat/ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42769</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TGMoOdeXqk</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>theglamstar38</td>\n",
       "      <td>2021-10-09T04:35:29Z</td>\n",
       "      <td>2021-10-09T04:35:29Z</td>\n",
       "      <td>0</td>\n",
       "      <td>what in the danganronpa execution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42770 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "42765  Squid Game Korean Drama (2021)   \n",
       "42766  Squid Game Korean Drama (2021)   \n",
       "42767  Squid Game Korean Drama (2021)   \n",
       "42768  Squid Game Korean Drama (2021)   \n",
       "42769  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "0                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "1                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "2                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "4                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "...                                                  ...          ...   \n",
       "42765  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42766  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42767  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42768  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42769  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "\n",
       "          v_channelTitle         v_publishTime  \\\n",
       "0                MrBeast  2021-11-24T21:00:01Z   \n",
       "1                MrBeast  2021-11-24T21:00:01Z   \n",
       "2                MrBeast  2021-11-24T21:00:01Z   \n",
       "3                MrBeast  2021-11-24T21:00:01Z   \n",
       "4                MrBeast  2021-11-24T21:00:01Z   \n",
       "...                  ...                   ...   \n",
       "42765  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42766  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42767  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42768  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42769  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "42765  The stars of SQUID GAME are faced with yet ano...   \n",
       "42766  The stars of SQUID GAME are faced with yet ano...   \n",
       "42767  The stars of SQUID GAME are faced with yet ano...   \n",
       "42768  The stars of SQUID GAME are faced with yet ano...   \n",
       "42769  The stars of SQUID GAME are faced with yet ano...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "42765  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42766  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42767  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42768  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42769  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "\n",
       "                                                    c_id  \\\n",
       "0                             UgzH8vliQSJKHQMGZjx4AaABAg   \n",
       "1                             UgwDhFNTCbfck5apuUJ4AaABAg   \n",
       "2                             UgzVlS_nKI4aXISU_ep4AaABAg   \n",
       "3                             UgxykcUWbPcLhlL-Gy14AaABAg   \n",
       "4                             Ugxu5B8dQ9-mZpfW-UV4AaABAg   \n",
       "...                                                  ...   \n",
       "42765  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TCfbJV8URO   \n",
       "42766  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFObZDOfQq   \n",
       "42767  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFPKHqbzEu   \n",
       "42768  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFkpMvsZvz   \n",
       "42769  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TGMoOdeXqk   \n",
       "\n",
       "                       c_parentId         c_author        c_published_at  \\\n",
       "0                                          MrBeast  2021-11-24T21:02:45Z   \n",
       "1                                      DoodleChaos  2021-11-24T22:07:54Z   \n",
       "2                                      mukul_editz  2023-12-30T01:55:59Z   \n",
       "3                                      SpamR1_2013  2023-11-27T00:57:21Z   \n",
       "4                                  user-cs9zv3gh1k  2024-01-30T20:17:02Z   \n",
       "...                           ...              ...                   ...   \n",
       "42765  Ugyg_jtreiARzVQq6TB4AaABAg       allure_ism  2021-10-07T18:11:30Z   \n",
       "42766  Ugyg_jtreiARzVQq6TB4AaABAg   gabrieleDATass  2021-10-08T19:31:58Z   \n",
       "42767  Ugyg_jtreiARzVQq6TB4AaABAg    snowrider9018  2021-10-08T19:38:13Z   \n",
       "42768  Ugyg_jtreiARzVQq6TB4AaABAg             Son9  2021-10-08T22:54:50Z   \n",
       "42769  Ugyg_jtreiARzVQq6TB4AaABAg    theglamstar38  2021-10-09T04:35:29Z   \n",
       "\n",
       "               c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z       1008552   \n",
       "1      2021-11-24T22:07:54Z        514609   \n",
       "2      2023-12-30T01:55:59Z           435   \n",
       "3      2023-11-27T00:57:21Z          1712   \n",
       "4      2024-01-30T20:17:02Z           266   \n",
       "...                     ...           ...   \n",
       "42765  2021-10-07T18:11:30Z             6   \n",
       "42766  2021-10-08T19:31:58Z             1   \n",
       "42767  2021-10-08T19:38:13Z             2   \n",
       "42768  2021-10-08T22:54:50Z             0   \n",
       "42769  2021-10-09T04:35:29Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                       Your videos are so interesting ❤  \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "42765  Nobody would pick paper or rock (I mean it dep...  \n",
       "42766            How do you kill someone with paper lmao  \n",
       "42767  @@gabrieleDATass sand-paper them to death, idk...  \n",
       "42768  @@gabrieleDATass Shove it down their throat/ev...  \n",
       "42769                  what in the danganronpa execution  \n",
       "\n",
       "[42770 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_video_comments=youtube.create_video_df_from_search(products,number_of_videos_per_product=20,number_of_comments_per_video=1000,number_of_replies_per_comment=100)\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Analysis, Selection and Labeling:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from:\n",
    "\n",
    "https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis : As emojis do not provide any helpful information they should be removed from the text strings.\n",
    "def remove_emojis(data):\n",
    "    if isinstance(data, str):\n",
    "        # Remove html tags\n",
    "        data = BeautifulSoup(data, \"html.parser\").get_text()\n",
    "        # Remove emote, etc\n",
    "        emoj = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                        \"]+\", re.UNICODE)\n",
    "        # english_words = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "        return re.sub(emoj, '', data)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row containing NA values.\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_2028\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n",
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_2028\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Length Before: 42770\n",
      "DataFrame Length After: 39165\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008552</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514609</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>435</td>\n",
       "      <td>Your videos are so interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1712</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>266</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42765</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TCfbJV8URO</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>allure_ism</td>\n",
       "      <td>2021-10-07T18:11:30Z</td>\n",
       "      <td>2021-10-07T18:11:30Z</td>\n",
       "      <td>6</td>\n",
       "      <td>Nobody would pick paper or rock (I mean it dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42766</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFObZDOfQq</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>gabrieleDATass</td>\n",
       "      <td>2021-10-08T19:31:58Z</td>\n",
       "      <td>2021-10-08T19:31:58Z</td>\n",
       "      <td>1</td>\n",
       "      <td>How do you kill someone with paper lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42767</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFPKHqbzEu</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>snowrider9018</td>\n",
       "      <td>2021-10-08T19:38:13Z</td>\n",
       "      <td>2021-10-08T19:38:13Z</td>\n",
       "      <td>2</td>\n",
       "      <td>@@gabrieleDATass sand-paper them to death, idk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42768</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFkpMvsZvz</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>Son9</td>\n",
       "      <td>2021-10-08T22:54:50Z</td>\n",
       "      <td>2021-10-08T22:54:50Z</td>\n",
       "      <td>0</td>\n",
       "      <td>@@gabrieleDATass Shove it down their throat/ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42769</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Cast of Squid Game ditches tracksuits for suit...</td>\n",
       "      <td>o4EF1NG_xks</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-09-25T01:00:08Z</td>\n",
       "      <td>The stars of SQUID GAME are faced with yet ano...</td>\n",
       "      <td>https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TGMoOdeXqk</td>\n",
       "      <td>Ugyg_jtreiARzVQq6TB4AaABAg</td>\n",
       "      <td>theglamstar38</td>\n",
       "      <td>2021-10-09T04:35:29Z</td>\n",
       "      <td>2021-10-09T04:35:29Z</td>\n",
       "      <td>0</td>\n",
       "      <td>what in the danganronpa execution</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39165 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "42765  Squid Game Korean Drama (2021)   \n",
       "42766  Squid Game Korean Drama (2021)   \n",
       "42767  Squid Game Korean Drama (2021)   \n",
       "42768  Squid Game Korean Drama (2021)   \n",
       "42769  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "0                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "1                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "2                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "4                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "...                                                  ...          ...   \n",
       "42765  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42766  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42767  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42768  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "42769  Cast of Squid Game ditches tracksuits for suit...  o4EF1NG_xks   \n",
       "\n",
       "          v_channelTitle         v_publishTime  \\\n",
       "0                MrBeast  2021-11-24T21:00:01Z   \n",
       "1                MrBeast  2021-11-24T21:00:01Z   \n",
       "2                MrBeast  2021-11-24T21:00:01Z   \n",
       "3                MrBeast  2021-11-24T21:00:01Z   \n",
       "4                MrBeast  2021-11-24T21:00:01Z   \n",
       "...                  ...                   ...   \n",
       "42765  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42766  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42767  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42768  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "42769  Netflix K-Content  2021-09-25T01:00:08Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "42765  The stars of SQUID GAME are faced with yet ano...   \n",
       "42766  The stars of SQUID GAME are faced with yet ano...   \n",
       "42767  The stars of SQUID GAME are faced with yet ano...   \n",
       "42768  The stars of SQUID GAME are faced with yet ano...   \n",
       "42769  The stars of SQUID GAME are faced with yet ano...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "42765  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42766  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42767  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42768  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "42769  https://i.ytimg.com/vi/o4EF1NG_xks/default.jpg   \n",
       "\n",
       "                                                    c_id  \\\n",
       "0                             UgzH8vliQSJKHQMGZjx4AaABAg   \n",
       "1                             UgwDhFNTCbfck5apuUJ4AaABAg   \n",
       "2                             UgzVlS_nKI4aXISU_ep4AaABAg   \n",
       "3                             UgxykcUWbPcLhlL-Gy14AaABAg   \n",
       "4                             Ugxu5B8dQ9-mZpfW-UV4AaABAg   \n",
       "...                                                  ...   \n",
       "42765  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TCfbJV8URO   \n",
       "42766  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFObZDOfQq   \n",
       "42767  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFPKHqbzEu   \n",
       "42768  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TFkpMvsZvz   \n",
       "42769  Ugyg_jtreiARzVQq6TB4AaABAg.9T8TlcU-mJu9TGMoOdeXqk   \n",
       "\n",
       "                       c_parentId         c_author        c_published_at  \\\n",
       "0                                          MrBeast  2021-11-24T21:02:45Z   \n",
       "1                                      DoodleChaos  2021-11-24T22:07:54Z   \n",
       "2                                      mukul_editz  2023-12-30T01:55:59Z   \n",
       "3                                      SpamR1_2013  2023-11-27T00:57:21Z   \n",
       "4                                  user-cs9zv3gh1k  2024-01-30T20:17:02Z   \n",
       "...                           ...              ...                   ...   \n",
       "42765  Ugyg_jtreiARzVQq6TB4AaABAg       allure_ism  2021-10-07T18:11:30Z   \n",
       "42766  Ugyg_jtreiARzVQq6TB4AaABAg   gabrieleDATass  2021-10-08T19:31:58Z   \n",
       "42767  Ugyg_jtreiARzVQq6TB4AaABAg    snowrider9018  2021-10-08T19:38:13Z   \n",
       "42768  Ugyg_jtreiARzVQq6TB4AaABAg             Son9  2021-10-08T22:54:50Z   \n",
       "42769  Ugyg_jtreiARzVQq6TB4AaABAg    theglamstar38  2021-10-09T04:35:29Z   \n",
       "\n",
       "               c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z       1008552   \n",
       "1      2021-11-24T22:07:54Z        514609   \n",
       "2      2023-12-30T01:55:59Z           435   \n",
       "3      2023-11-27T00:57:21Z          1712   \n",
       "4      2024-01-30T20:17:02Z           266   \n",
       "...                     ...           ...   \n",
       "42765  2021-10-07T18:11:30Z             6   \n",
       "42766  2021-10-08T19:31:58Z             1   \n",
       "42767  2021-10-08T19:38:13Z             2   \n",
       "42768  2021-10-08T22:54:50Z             0   \n",
       "42769  2021-10-09T04:35:29Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                        Your videos are so interesting   \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "42765  Nobody would pick paper or rock (I mean it dep...  \n",
       "42766            How do you kill someone with paper lmao  \n",
       "42767  @@gabrieleDATass sand-paper them to death, idk...  \n",
       "42768  @@gabrieleDATass Shove it down their throat/ev...  \n",
       "42769                  what in the danganronpa execution  \n",
       "\n",
       "[39165 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove emojis from the text to be analyzed\n",
    "multiple_video_comments['c_text']=multiple_video_comments['c_text'].apply(remove_emojis)\n",
    "\n",
    "df_length_before = len(multiple_video_comments)\n",
    "print(\"DataFrame Length Before:\", df_length_before)\n",
    "\n",
    "# Drop duplicates\n",
    "multiple_video_comments.drop_duplicates(inplace=True)\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)\n",
    "# Drop rows with empty or text length <= 2 comments\n",
    "multiple_video_comments = multiple_video_comments[multiple_video_comments['c_text'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "df_length_after = len(multiple_video_comments)\n",
    "print(\"DataFrame Length After:\", df_length_after)\n",
    "\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://stackoverflow.com/questions/40375366/pandas-to-csv-checking-for-overwrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        # Define keywords related to the TV show\n",
    "        self.tv_show_keywords = ['Squid Game', 'Gi-hun', 'Sang-woo', 'Player', 'Red light, green light', 'Honeycomb',\n",
    "                            'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                            'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho'\n",
    "                            'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                            'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali', ]\n",
    "        # Setting threshold value for validating the relevance of the comment\n",
    "        self.threshold = 1\n",
    "\n",
    "    # Tokenize text and remove stop words\n",
    "    def preprocess_text(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    # Matching function to check relevance of the comments\n",
    "    def match_keywords(self, tokens):\n",
    "        return [token for token in tokens if token in self.tv_show_keywords]\n",
    "\n",
    "    # Scoring function to calculate how many tokens matched\n",
    "    def calculate_score(self, tokens):\n",
    "        return len(tokens)\n",
    "\n",
    "    # Validate function to validate the relevance based on threshold\n",
    "    def validate_relevance(self, score):\n",
    "        return score >= self.threshold\n",
    "\n",
    "    def filter_comments(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        irrelevant_keywords = ['HYVE', 'crypto', 'promotion', 'ad', 'spam', 'advertisement', 'spoiler', 'leak', 'promo', 'off-topic', 'clickbait',\n",
    "                            'self-promotion', '0:', '1:', '2:', '3:', '4:', '5:', '6:', '7:',\n",
    "                            '8:', '9:', '10:', '11:', '12:', '13:', '14:', '15:']\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                if detect(row['c_text']) == 'en' and not any(keyword in row['c_text'] for keyword in irrelevant_keywords):\n",
    "                    comments.append(row)\n",
    "                    c += 1\n",
    "            except Exception as e:  # Catch any exception\n",
    "                pass\n",
    "        print(\"Number of Filtered Comments: \", c)\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])  # Create a new DataFrame from the list of rows\n",
    "        new_df = new_df.sort_values(by = ['c_like_count'], ascending = False)\n",
    "        new_df.drop_duplicates(inplace=True)\n",
    "        new_df = new_df[:4000]\n",
    "        return new_df\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        for index, row in df.iterrows():\n",
    "            processed_text = self.preprocess_text(row['c_text'])\n",
    "            matched_keywords = self.match_keywords(processed_text)\n",
    "            score = self.calculate_score(matched_keywords)\n",
    "            is_relevant = self.validate_relevance(score)\n",
    "            if is_relevant == 1:\n",
    "                comments.append(row)\n",
    "                c += 1\n",
    "\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])\n",
    "        print(\"Number of Processed Comments: \", c)\n",
    "        new_df = self.filter_comments(new_df)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Processed Comments:  3534\n",
      "Number of Filtered Comments:  2268\n"
     ]
    }
   ],
   "source": [
    "# p = preprocessing()\n",
    "# new_df = p.preprocess(multiple_video_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product           7022\n",
       "v_title           7022\n",
       "v_videoId         7022\n",
       "v_channelTitle    7022\n",
       "v_publishTime     7022\n",
       "v_description     7022\n",
       "v_thumbnail       7022\n",
       "c_id              7022\n",
       "c_parentId        7022\n",
       "c_author          7022\n",
       "c_published_at    7022\n",
       "c_updated_at      7022\n",
       "c_like_count      7022\n",
       "c_text            7022\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df =multiple_video_comments.drop_duplicates()[multiple_video_comments['c_like_count']>5]\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling comments using Sentiment Lexicon - VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(c_text):\n",
    "    sentiment_Score = sentiment_lexicon.polarity_scores(c_text)\n",
    "    return sentiment_Score['compound']\n",
    "\n",
    "def check_sentiment(sentiment_score):\n",
    "    if sentiment_score > 0.00:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0.00:\n",
    "        return \"Negative\"\n",
    "    elif sentiment_score == 0:\n",
    "        return \"Neutral \"\n",
    "def remove_usernames(text):\n",
    "    pattern = r\"@@\\w+\"\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text  \n",
    " \n",
    "\n",
    "new_df['c_text'] = new_df['c_text'].apply(remove_usernames)\n",
    "new_df['sentiment_score'] = new_df['c_text'].apply(get_sentiment_score)\n",
    "new_df['sentiment'] = new_df['sentiment_score'].apply(check_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Comments for each polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0018  Negative           -lq1xk3dd4m ignore him, he's just jealous. I am Chinese descent myself and absolutely love the squid game\n",
      "    -0.0018  Negative           I really like the cast. Sadly the fact that even there are season 2, they won’t be together again\n",
      "    -0.0026  Negative           well it doesnt look like a fantasy world to me\n",
      "    -0.0027  Negative           I literally know nothing about this series  but in insta I saw a scene of the series where the doll kills I was very amazed , now I am going to watch it\n",
      "    -0.0042  Negative           hoyeon and yoomi's smiles are just so contagious\n",
      "    -0.0087  Negative           Hae-Soo's \"Yes\" was so cute I'm gonna die\n",
      "    -0.0145  Negative           This is scary as heck. I’m definitely not falling asleep tonight.\n",
      "    -0.0196  Negative           Hopefully this show doesn't gain a toxic fanbase.\n",
      "    -0.024   Negative           aw thats sucks :( i kinda hope it is released in a DV one day hopefully in the far future\n",
      "    -0.0258  Negative           The Ji-yeong + Sae-byeok marble scene was my favorite. Actually had me crying\n",
      "     0       Neutral            that guy who sacrificed himself on purpose for 456 in marbles was an absolute gigachad\n",
      "     0       Neutral            Mirip game benteng takeshi \"blueberry hill\"\n",
      "     0       Neutral            Pas ngeliat awal terpikir \"money heist\" wkwkwk\n",
      "     0       Neutral            Pelajaran berharga yang bisa di petik dari film ini adalah masalah sikap, \"Jangan pernah Berhutang\" karena hutang itu perbudakan modern (modern slavery), dan jangan pernah berspekulasi (berjudi).\n",
      "     0       Neutral            ternyata korea bisa bikin film keren begini ya.orang2 kan tau nya hanya drama2 romantis yg garing. semoga makin banyak deh film2 begini\n",
      "     0       Neutral            1:39 udh jadi beban malah ambil uang ibunya dasar beban\n",
      "     0       Neutral            Di menit 3:17 itu dia yang peranin film\"TRAIN TO BUSAN \" Kan?\n",
      "     0       Neutral            yg paling seru catat pin ATM nya yg juara 0456, wkwkwk siapa tau lo bisa bobol tuh ATM\n",
      "     0       Neutral            dulu sekali=money heist\n",
      "                                Dulu=train to busan\n",
      "                                Sekarang=squd game\n",
      "                                Nanti apa\n",
      "     0       Neutral            Terbaik sekali\n",
      "     0.9921  Positive           Honestly Kdramas are actually quite addicting. If your interested in exploring more and wanna try one in a different tone/ genre but still has really great acting, ost and directing like Squid game, I’d recommend “ It’s Okay not to be Okay “, it’s not a thriller but more of an actual drama ( some romance and humour too ) that tackles a lot of situations related to mental health issues. It has a pretty amazing storyline and the actors in this Kdrama are top notch. I wouldn’t say it’s more child friendly ( cause it’s definitely not meant for kids under 15 ) but it’s not something on the crazy level of murder/ killings like squid game. You can also find it on Netflix\n",
      "     0.9912  Positive           It’s been about 2 days since I finished this show and it has genuinely not left my mind ever since. I usually never leave reviews on shows/movies I watch either, so me leaving one for this show speaks volumes to just how much I enjoyed Squid Game. The acting is beyond phenomenal in my opinion and evoked emotions in me that I had never felt in any other show before. (this review is spoiler free, don’t worry.) Every little detail that went into the production value, the acting, the plot, the character development, the set designs, EVERYTHING was just absolutely amazing. I went into this show thinking I would mostly enjoy it and that would be it, however I did not expect it to impact me as much as it did. The hype Squid Game is receiving at the moment is very much deserved, and I HIGHLY recommend this show to anyone who hasn’t seen it/needs something new to binge on. (really, I finished all 9 episodes in one sitting. over 9 hours.) I have been recommending this show to everyone I know so far and even if you aren’t the biggest fan of violent/bloody shows I genuinely believe to give this one a chance, you won’t be disappointed. This show has something for everyone; it’s emotional, dramatic, comedic at times, and has enough blood scenes for suspense/thrill lovers to enjoy. I really cannot explain enough just how much I love this show, it’s one of my favorites on Netflix now that’s for sure. Kudos to the cast and team behind this show as well, they’ve worked so hard and all the hard work that went into production is rightfully paying off. So if you have been looking for a sign on whether or not to watch Squid Game, this is your sign.\n",
      "     0.9905  Positive           Love how the cast has great chemistry even out side their acting bubble they just seem so close and comfortable with each other  This series was amazing and I am really happy as kdrama lover that it’s number one in the United States right now it deserves it am so excited for season two I am sure it’s gonna be confirmed because people loved it so much just as  th last kingdom and sweet home  which am also looking forward to\n",
      "     0.988   Positive           Brilliant and mindblowing from every point! All actors in show are so talented and played marvelously! Soundtrack is amazing! Colors and details are so great! True Masterpiece! Desire for 2nd season is huge but ready to wait in hope that it will be the same quality. For the moment can enjoy and analyse the fabulous 1st season.\n",
      "     0.988   Positive           This series is so amazing. Like it's so dark but there's a comedic side and that made it so even better to watch. So glad that this series happened to think that it took 10 years. The actors are all amazing. And it amazed me also when I learned that this is Jung Ho Yeon's first acting. She nailed it. Congrats to all the people behind Squid Game\n",
      "     0.9878  Positive           I totally agree. Music can transform your emotions and thoughts like a light switch. Music is one of the most important and crucial ingredients in bringing forth emotions and has the powerful ability to change the audience’s emotional state!\n",
      "                                That’s power.\n",
      "\n",
      "                                Btw, side note, I think the Minecraft comment was someone essentially saying, “You’re probably some dumb kid playing video games” (since Minecraft is a very popular game where a lot of younger kids start out with playing Minecraft) side note: I love Minecraft and I’m not a kid but that’s not the point lol)  But basically, the Minecraft comment was just a simple insult to another user trying to undermine their intelligence and judgment. Lol Hopefully this makes sense.\n",
      "\n",
      "                                Anywho! I absolutely agree with everything you said about the impact and importance of music! Well said!!\n",
      "\n",
      "                                PS. I think you meant add not ad. I hate ads lmao\n",
      "     0.9878  Positive           After watching Squid Game, Idk how to explain if I like that ending but overall the drama is my type and I'm super in love with this concept survival game. The cast acting were brilliant, the filming set and filmography were excellent. And CAMEO(s), YES chef kiss!\n",
      "     0.9872  Positive           Whoever believes in the Lord Jesus Christ shall not perish but have everlasting life repent for the kingdom of God is at hand! Jesus is returning the Bible says in Romans 10:9 that whoever believes in the Lord Jesus Christ and accept him into your heart and as your Lord and Savior you shall be saved repent for the kingdom of God is at hand! Jesus is returning repent God bless Jesus loves you amen\n",
      "     0.9863  Positive           They kinda still awkward with eachother but I still enjoy watching this cause they so funny like I just find it funny HAHAHAH plus WI HA JOONNN SO HANDSOME\n",
      "     0.9838  Positive           I'm really amazed of Jung Ho Yeon's acting on the series.\n",
      "                                There she was like a total badass woman and so so COOL!\n",
      "                                But in real she's so innocent, lovable, funny and cute.\n",
      "                                All of them are so cute together!\n"
     ]
    }
   ],
   "source": [
    "def select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('sentiment')\n",
    "\n",
    "    # iterate over each polarity group\n",
    "    for sentiment, group in grouped:\n",
    "        # sort comments by sentiment score pick top 10\n",
    "        top_group_comments = group.sort_values(by='sentiment_score', ascending=False).head(top_n)[['sentiment_score', 'sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "# # top 10 comments for each polarity\n",
    "# for sentiment_score, sentiment, comment in top_comments:\n",
    "#     print(f\"Sentiment: {sentiment}, Sentiment Score: {sentiment_score}, Comment: {comment}\")\n",
    "\n",
    "# making it pretty~~~\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(top_comments, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Lexicon using TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0018  Negative           -lq1xk3dd4m ignore him, he's just jealous. I am Chinese descent myself and absolutely love the squid game\n",
      "    -0.0018  Negative           I really like the cast. Sadly the fact that even there are season 2, they won’t be together again\n",
      "    -0.0026  Negative           well it doesnt look like a fantasy world to me\n",
      "    -0.0027  Negative           I literally know nothing about this series  but in insta I saw a scene of the series where the doll kills I was very amazed , now I am going to watch it\n",
      "    -0.0042  Negative           hoyeon and yoomi's smiles are just so contagious\n",
      "    -0.0087  Negative           Hae-Soo's \"Yes\" was so cute I'm gonna die\n",
      "    -0.0145  Negative           This is scary as heck. I’m definitely not falling asleep tonight.\n",
      "    -0.0196  Negative           Hopefully this show doesn't gain a toxic fanbase.\n",
      "    -0.024   Negative           aw thats sucks :( i kinda hope it is released in a DV one day hopefully in the far future\n",
      "    -0.0258  Negative           The Ji-yeong + Sae-byeok marble scene was my favorite. Actually had me crying\n",
      "     0       Neutral            that guy who sacrificed himself on purpose for 456 in marbles was an absolute gigachad\n",
      "     0       Neutral            Mirip game benteng takeshi \"blueberry hill\"\n",
      "     0       Neutral            Pas ngeliat awal terpikir \"money heist\" wkwkwk\n",
      "     0       Neutral            Pelajaran berharga yang bisa di petik dari film ini adalah masalah sikap, \"Jangan pernah Berhutang\" karena hutang itu perbudakan modern (modern slavery), dan jangan pernah berspekulasi (berjudi).\n",
      "     0       Neutral            ternyata korea bisa bikin film keren begini ya.orang2 kan tau nya hanya drama2 romantis yg garing. semoga makin banyak deh film2 begini\n",
      "     0       Neutral            1:39 udh jadi beban malah ambil uang ibunya dasar beban\n",
      "     0       Neutral            Di menit 3:17 itu dia yang peranin film\"TRAIN TO BUSAN \" Kan?\n",
      "     0       Neutral            yg paling seru catat pin ATM nya yg juara 0456, wkwkwk siapa tau lo bisa bobol tuh ATM\n",
      "     0       Neutral            dulu sekali=money heist\n",
      "                                Dulu=train to busan\n",
      "                                Sekarang=squd game\n",
      "                                Nanti apa\n",
      "     0       Neutral            Terbaik sekali\n",
      "     0.9921  Positive           Honestly Kdramas are actually quite addicting. If your interested in exploring more and wanna try one in a different tone/ genre but still has really great acting, ost and directing like Squid game, I’d recommend “ It’s Okay not to be Okay “, it’s not a thriller but more of an actual drama ( some romance and humour too ) that tackles a lot of situations related to mental health issues. It has a pretty amazing storyline and the actors in this Kdrama are top notch. I wouldn’t say it’s more child friendly ( cause it’s definitely not meant for kids under 15 ) but it’s not something on the crazy level of murder/ killings like squid game. You can also find it on Netflix\n",
      "     0.9912  Positive           It’s been about 2 days since I finished this show and it has genuinely not left my mind ever since. I usually never leave reviews on shows/movies I watch either, so me leaving one for this show speaks volumes to just how much I enjoyed Squid Game. The acting is beyond phenomenal in my opinion and evoked emotions in me that I had never felt in any other show before. (this review is spoiler free, don’t worry.) Every little detail that went into the production value, the acting, the plot, the character development, the set designs, EVERYTHING was just absolutely amazing. I went into this show thinking I would mostly enjoy it and that would be it, however I did not expect it to impact me as much as it did. The hype Squid Game is receiving at the moment is very much deserved, and I HIGHLY recommend this show to anyone who hasn’t seen it/needs something new to binge on. (really, I finished all 9 episodes in one sitting. over 9 hours.) I have been recommending this show to everyone I know so far and even if you aren’t the biggest fan of violent/bloody shows I genuinely believe to give this one a chance, you won’t be disappointed. This show has something for everyone; it’s emotional, dramatic, comedic at times, and has enough blood scenes for suspense/thrill lovers to enjoy. I really cannot explain enough just how much I love this show, it’s one of my favorites on Netflix now that’s for sure. Kudos to the cast and team behind this show as well, they’ve worked so hard and all the hard work that went into production is rightfully paying off. So if you have been looking for a sign on whether or not to watch Squid Game, this is your sign.\n",
      "     0.9905  Positive           Love how the cast has great chemistry even out side their acting bubble they just seem so close and comfortable with each other  This series was amazing and I am really happy as kdrama lover that it’s number one in the United States right now it deserves it am so excited for season two I am sure it’s gonna be confirmed because people loved it so much just as  th last kingdom and sweet home  which am also looking forward to\n",
      "     0.988   Positive           Brilliant and mindblowing from every point! All actors in show are so talented and played marvelously! Soundtrack is amazing! Colors and details are so great! True Masterpiece! Desire for 2nd season is huge but ready to wait in hope that it will be the same quality. For the moment can enjoy and analyse the fabulous 1st season.\n",
      "     0.988   Positive           This series is so amazing. Like it's so dark but there's a comedic side and that made it so even better to watch. So glad that this series happened to think that it took 10 years. The actors are all amazing. And it amazed me also when I learned that this is Jung Ho Yeon's first acting. She nailed it. Congrats to all the people behind Squid Game\n",
      "     0.9878  Positive           I totally agree. Music can transform your emotions and thoughts like a light switch. Music is one of the most important and crucial ingredients in bringing forth emotions and has the powerful ability to change the audience’s emotional state!\n",
      "                                That’s power.\n",
      "\n",
      "                                Btw, side note, I think the Minecraft comment was someone essentially saying, “You’re probably some dumb kid playing video games” (since Minecraft is a very popular game where a lot of younger kids start out with playing Minecraft) side note: I love Minecraft and I’m not a kid but that’s not the point lol)  But basically, the Minecraft comment was just a simple insult to another user trying to undermine their intelligence and judgment. Lol Hopefully this makes sense.\n",
      "\n",
      "                                Anywho! I absolutely agree with everything you said about the impact and importance of music! Well said!!\n",
      "\n",
      "                                PS. I think you meant add not ad. I hate ads lmao\n",
      "     0.9878  Positive           After watching Squid Game, Idk how to explain if I like that ending but overall the drama is my type and I'm super in love with this concept survival game. The cast acting were brilliant, the filming set and filmography were excellent. And CAMEO(s), YES chef kiss!\n",
      "     0.9872  Positive           Whoever believes in the Lord Jesus Christ shall not perish but have everlasting life repent for the kingdom of God is at hand! Jesus is returning the Bible says in Romans 10:9 that whoever believes in the Lord Jesus Christ and accept him into your heart and as your Lord and Savior you shall be saved repent for the kingdom of God is at hand! Jesus is returning repent God bless Jesus loves you amen\n",
      "     0.9863  Positive           They kinda still awkward with eachother but I still enjoy watching this cause they so funny like I just find it funny HAHAHAH plus WI HA JOONNN SO HANDSOME\n",
      "     0.9838  Positive           I'm really amazed of Jung Ho Yeon's acting on the series.\n",
      "                                There she was like a total badass woman and so so COOL!\n",
      "                                But in real she's so innocent, lovable, funny and cute.\n",
      "                                All of them are so cute together!\n"
     ]
    }
   ],
   "source": [
    "def text_blob_sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def texblob_check_sentiment(score):\n",
    "    if score == 0:\n",
    "        return 'Neutral'\n",
    "    elif score < 0.00:\n",
    "        return 'Negative'\n",
    "    elif score > 0.00:\n",
    "        return 'Positive'\n",
    "\n",
    "new_df['textblob_score'] = new_df['c_text'].apply(text_blob_sentiment_score)\n",
    "new_df['textblob_sentiment'] = new_df['textblob_score'].apply(texblob_check_sentiment)\n",
    "\n",
    "def textblob_select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('textblob_sentiment')\n",
    "\n",
    "    for sentiment, group in grouped:\n",
    "        top_group_comments = group.sort_values(by='textblob_score', ascending=False).head(top_n)[['textblob_score', 'textblob_sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "textblob_top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(textblob_top_comments, headers=headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "42731    None\n",
       "42734    None\n",
       "42739    None\n",
       "42741    None\n",
       "42765    None\n",
       "Length: 7022, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['positive', 'negative', 'neutral']\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def col_to_txt(row):\n",
    "    sentiment = row['sentiment']  \n",
    "    c_text = row['c_text']\n",
    "    file_name = f\"{sentiment}_{row.c_id}.txt\"  \n",
    "    folder = f\"{sentiment.strip()}\"  \n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(c_text)\n",
    "\n",
    "new_df.apply(col_to_txt, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the root directory containing the positive, negative, and neutral folders\n",
    "root_dir = ''\n",
    "\n",
    "# Define the directories for train and test sets\n",
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "try:\n",
    "    shutil.rmtree(os.path.join(root_dir, train_dir))\n",
    "    shutil.rmtree(os.path.join(root_dir, test_dir))\n",
    "except:\n",
    "    pass\n",
    "# Define the ratio for train-test split\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Iterate through each sentiment folder\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    # Get the list of file paths in the current sentiment folder\n",
    "    files = os.listdir(os.path.join(root_dir, sentiment))\n",
    "    # Shuffle the file paths\n",
    "    random.shuffle(files)\n",
    "    # Calculate the split index based on the split ratio\n",
    "    split_index = int(len(files) * split_ratio)\n",
    "    # Split the files into train and test sets\n",
    "    train_files = files[:split_index]\n",
    "    test_files = files[split_index:]\n",
    "    \n",
    "    # Move train files to train directory\n",
    "    for file in train_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(train_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "    # Move test files to test directory\n",
    "    for file in test_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(test_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Remove the sentiment folders\n",
    "try:\n",
    "    shutil.rmtree('negative')\n",
    "    shutil.rmtree('neutral')\n",
    "    shutil.rmtree('positive')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Text Analytics Pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(train_dir)\n",
    "reviews_test = load_files(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'After finishing the series, seeing all the cast members here smiling and having fun, is what I needed for the soothing of my soul...' 2\n",
      "b'3:14 referensi dari Train To Busan sama sama di stasiun kereta :)' 2\n",
      "b'Director Hwang Dong-hyuk definitely knew what he required from each actor/actress and what he wanted for each scene. It seemed like he gave good directorship and worked well with everyone. He even did the casting job to make sure each actor/actress suitable for his/her role. Everyone from top to bottom made this show worked and succeeded even though some people think otherwise that it was over-rated and not original.' 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(text_train[1],y_train[1])\n",
    "print(text_train[2],y_train[2])\n",
    "print(text_train[3],y_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect = CountVectorizer().fit(reviews_train)\n",
    "X_train = vect.transform(reviews_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_tokens = ['drama', 'film', 'cinema', 'actor', 'actress', 'director', 'plot',\n",
    "                  'scene', 'genre', 'subtitles', 'k-drama', 'kdrama', 'k-movie', 'television',\n",
    "                  'episode', 'screenplay', 'script', 'cinematography', 'soundtrack',\n",
    "                  'OST', 'character', 'plot twist', 'review', 'ratings', 'premiere',\n",
    "                  'streaming', 'watchlist', 'subbed', 'dubbed', 'sequel', 'game', 'song',\n",
    "                  'season', 'trailer', 'casting', 'fanbase', 'recommendation', 'goblin',\n",
    "                  'viewer', 'critic', 'Korean', 'entertainment', 'watched', 'guardian',\n",
    "                  'show', 'squid', 'watch', 'watching', 'acting', 'netflix', 'show','end',\n",
    "                  'squid game', 'gi-hun', 'Sang-woo', 'Player', 'Red light', 'green light', 'Honeycomb',\n",
    "                  'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                  'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho',\n",
    "                  'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                  'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali']\n",
    "\n",
    "product_tokens = [item.lower() for item in product_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    return text_without_punctuation\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # stopwords punctuation etc\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    # split into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    # removes stopwords and numbers and stems from tokens makes sure its all lowercase too\n",
    "    tokens = [stemmer.stem(remove_punctuation(token)) for token in tokens if token.isalnum() and token.lower() not in product_tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.70      0.52       142\n",
      "           1       0.85      0.73      0.79       496\n",
      "           2       0.87      0.83      0.85       768\n",
      "\n",
      "    accuracy                           0.78      1406\n",
      "   macro avg       0.71      0.75      0.72      1406\n",
      "weighted avg       0.82      0.78      0.79      1406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('preprocess', \n",
    "    TfidfVectorizer(\n",
    "                    encoding=\"utf-8\",\n",
    "                    strip_accents='ascii',\n",
    "                    lowercase=True,\n",
    "                    preprocessor=preprocess_text,\n",
    "                    # tokenizer=,\n",
    "                    # analyzer=,\n",
    "                    stop_words='english',\n",
    "                    norm='l2',\n",
    "                    ngram_range=(1, 1),\n",
    "                    max_df=0.1,\n",
    "                    min_df=0.002,\n",
    "                    max_features=370,\n",
    "                    binary=True,\n",
    "                    use_idf=True,\n",
    "                    smooth_idf=True,\n",
    "                    sublinear_tf=True\n",
    "                    )\n",
    "    # CountVectorizer(preprocessor=preprocess_text,ngram_range=(1, 1))\n",
    "     ), \n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "text_clf.fit(text_train, y_train)\n",
    "y_pred = text_clf.predict(text_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Run only till here and check coz the grid search would take long so better to adjust by looking at this only ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.43      0.55       242\n",
      "           1       0.76      0.87      0.81       427\n",
      "           2       0.84      0.89      0.87       737\n",
      "\n",
      "    accuracy                           0.81      1406\n",
      "   macro avg       0.78      0.73      0.74      1406\n",
      "weighted avg       0.80      0.81      0.79      1406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__norm': ['l2','l1'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'vectorizer__use_idf': [True,False],\n",
    "    'vectorizer__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 85: Mean Test Score - 0.7970, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 87: Mean Test Score - 0.7961, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 89: Mean Test Score - 0.7958, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 103: Mean Test Score - 0.7949, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 105: Mean Test Score - 0.7940, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 107: Mean Test Score - 0.7938, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 79: Mean Test Score - 0.7831, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 81: Mean Test Score - 0.7826, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 83: Mean Test Score - 0.7822, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 97: Mean Test Score - 0.7806, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "results = grid_search.cv_results_\n",
    " \n",
    "scores = results['mean_test_score']\n",
    "\n",
    "params = results['params']\n",
    "\n",
    "top_models_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores[i], params[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.43      0.54       242\n",
      "           1       0.76      0.86      0.81       427\n",
      "           2       0.85      0.89      0.87       737\n",
      "\n",
      "    accuracy                           0.80      1406\n",
      "   macro avg       0.78      0.73      0.74      1406\n",
      "weighted avg       0.80      0.80      0.79      1406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_ct = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "parameters_ct = {\n",
    "    'vectorizer': [CountVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    'tfidf__norm': ['l2','l1'],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'tfidf__use_idf': [True,False],\n",
    "    'tfidf__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_ct = GridSearchCV(pipeline_ct, parameters_ct, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_ct.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_ct.best_params_)\n",
    "best_model_ct = grid_search_ct.best_estimator_\n",
    "y_pred_ct = best_model_ct.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 295: Mean Test Score - 0.7981, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 313: Mean Test Score - 0.7981, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 297: Mean Test Score - 0.7970, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 315: Mean Test Score - 0.7970, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 331: Mean Test Score - 0.7970, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 349: Mean Test Score - 0.7970, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 296: Mean Test Score - 0.7968, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 314: Mean Test Score - 0.7968, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 332: Mean Test Score - 0.7961, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 350: Mean Test Score - 0.7961, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x000002C3ABF66D30>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "results_ct = grid_search_ct.cv_results_\n",
    " \n",
    "scores_ct = results_ct['mean_test_score']\n",
    "\n",
    "params_ct = results_ct['params']\n",
    "\n",
    "top_models_indices_ct = sorted(range(len(scores_ct)), key=lambda i: scores_ct[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices_ct:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores_ct[i], params_ct[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualization and Insights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Discussion and conclusion from experiments:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
