{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farheenfab/AppliedText_CW/blob/main/CW1-generate_dataset.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F20AA Coursework 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect\n",
    "import shutil\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the api service name, version and developer key for the api call.\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "# AIzaSyC8VqY2cYxX7jOouIF076rpM8lvT1ZBJu4\n",
    "# AIzaSyAWj_uzrhZL18X32S_P79pT1wnSYGpuA4k\n",
    "DEVELOPER_KEY = \"AIzaSyC8VqY2cYxX7jOouIF076rpM8lvT1ZBJu4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/search/list#parameters\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/comments/list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a class called api_handler which contains functions such as `get_video_details()`, `get_videos()`, `get_video_df()`, `get_comments()`, `get_comment_replies()`, `get_comments_df()`, `create_video_df_from_search()`, `create_video_df()`. These functions help us by either manually retrieving the videos and comments or by automatically curating the videos and comments using the product given to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class api_handler:\n",
    "    def __init__(self, api_service_name, api_version, developer_key):\n",
    "        self.client = googleapiclient.discovery.build(api_service_name,\n",
    "                                                    api_version,\n",
    "                                                    developerKey=developer_key)\n",
    "        \n",
    "    # Search for videos details given id\n",
    "    def get_video_details(self, videoId, part=\"snippet\"):\n",
    "        request = self.client.videos().list(\n",
    "            part=part,\n",
    "            id=videoId\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response:\n",
    "            video_details = response['items'][0]\n",
    "            snippet=video_details['snippet']\n",
    "            snippet['videoId']=videoId\n",
    "            snippet['id']=videoId\n",
    "            snippet['publishTime']=video_details.get('snippet', {}).get('publishedAt', {})\n",
    "            snippet['thumbnails']=video_details.get('snippet', {}).get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "            return snippet\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Search for videos given query\n",
    "    def get_videos(self,query,maxResults=5,part=\"snippet\"):\n",
    "        request = self.client.search().list(\n",
    "            part=part,\n",
    "            maxResults=maxResults,\n",
    "            # higher view count is likely to be more relevent \n",
    "            order=\"viewCount\",\n",
    "            q=query,  \n",
    "            # american region videos \n",
    "            regionCode=\"US\",\n",
    "            # english videos\n",
    "            relevanceLanguage=\"en\",\n",
    "            type=\"video\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        return response\n",
    "    \n",
    "    # Format Response from get_videos to dataframe\n",
    "    def get_video_df(response):\n",
    "        items=[]\n",
    "        for item in response['items']:\n",
    "            snippet=item.get('snippet', {})\n",
    "            items+=[{\n",
    "                'title':snippet.get('title', ''),\n",
    "                'videoId':item.get('id', {}).get('videoId', ''),\n",
    "                'channelTitle':snippet.get('channelTitle', ''),\n",
    "                'publishTime':snippet.get('publishTime', ''),\n",
    "                'description':snippet.get('description', ''),\n",
    "                'thumbnails':snippet.get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "                }]\n",
    "        df=pd.DataFrame(items)\n",
    "        return df\n",
    "    \n",
    "    # Get comments from video\n",
    "    def get_comments(self,videoId,part=\"snippet\",maxResults=100,maxResultsDepth=100):\n",
    "        all_comments = []\n",
    "        f = 0\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0:\n",
    "            request = self.client.commentThreads().list(\n",
    "                part=part,\n",
    "                videoId=videoId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                order='relevance',\n",
    "                moderationStatus='published',\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if 'items' in response:\n",
    "                all_comments+=[response]\n",
    "                for item in response['items']:\n",
    "                    # extract the comment ID to get replies\n",
    "                    comment_id = item.get('snippet',{}).get('topLevelComment',{}).get('id','')\n",
    "                    if item.get('snippet',{}).get('totalReplyCount',0)>2:\n",
    "                        if f == 0:\n",
    "                            print('getting replies:',item.get('snippet',{}).get('totalReplyCount',0))\n",
    "                            f = 1\n",
    "                        replies = self.get_comment_replies(comment_id, maxResults=maxResultsDepth)\n",
    "                        all_comments += replies\n",
    "\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "    \n",
    "    # Get replies from comment \n",
    "    def get_comment_replies(self, commentId, part=\"snippet\", maxResults=100):\n",
    "        all_comments = []\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0 and (nextPageToken != None or len(all_comments)==0):\n",
    "\n",
    "            request = self.client.comments().list(\n",
    "                part=part,\n",
    "                parentId=commentId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "            if 'items' in response and len(response['items'])>0:\n",
    "                for item in response['items']:\n",
    "                    modified_response = {\n",
    "                        'items': [\n",
    "                            {\n",
    "                                'id':item.get('id'),\n",
    "                                'snippet': {\n",
    "                                    'topLevelComment': {\n",
    "                                        'snippet': item.get('snippet','')\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    all_comments += [modified_response]\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "\n",
    "    # Format response from get_comments to dataframe\n",
    "    def get_comments_df(response, video,product):\n",
    "        comments = []\n",
    "        for pages in response:\n",
    "            for item in pages['items']:\n",
    "                comment = item.get('snippet', {}).get('topLevelComment', {}).get('snippet', {})\n",
    "                comments.append([\n",
    "                        product,\n",
    "                        video.get('title', ''),\n",
    "                        video.get('videoId', ''),\n",
    "                        video.get('channelTitle', ''),\n",
    "                        video.get('publishTime', ''),\n",
    "                        video.get('description', ''),\n",
    "                        video.get('thumbnails', ''),\n",
    "                        item.get('id', ''),  \n",
    "                        comment.get('parentId', ''),  \n",
    "                        comment.get('authorDisplayName', '')[1:],  \n",
    "                        comment.get('publishedAt', ''),\n",
    "                        comment.get('updatedAt', ''),\n",
    "                        comment.get('likeCount', ''),\n",
    "                        comment.get('textDisplay', '')\n",
    "                    ])\n",
    "\n",
    "        df = pd.DataFrame(comments,\n",
    "            columns=['product', 'v_title', 'v_videoId',\n",
    "                    'v_channelTitle', 'v_publishTime',\n",
    "                    'v_description', 'v_thumbnail',\n",
    "                    'c_id','c_parentId',\n",
    "                    'c_author', 'c_published_at',\n",
    "                    'c_updated_at', 'c_like_count',\n",
    "                    'c_text'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Search for videos related to products iteratively\n",
    "    # Collect comments from each video and place it into an array\n",
    "    def create_video_df_from_search(self, products,\n",
    "                                    number_of_videos_per_product=5,\n",
    "                                    number_of_comments_per_video=100\n",
    "                                    ,number_of_replies_per_comment=100):\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            # get 25 first videos with the highest viewer counts \n",
    "            response = self.get_videos(query=product, maxResults=number_of_videos_per_product)\n",
    "            # Convert results to df\n",
    "            videos_df = api_handler.get_video_df(response)\n",
    "            # For each video get a maximum of 100 comments\n",
    "            # and place comments into an array\n",
    "            for _, video in videos_df.iterrows():\n",
    "                try:\n",
    "                    response = self.get_comments(video['videoId'], maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment)\n",
    "                    comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                except:\n",
    "                    # Function fails as the API returns 403 if the channel has comments disabled\n",
    "                    # place an empty entry instead it can be deleted later\n",
    "                    comments_df = pd.DataFrame(np.zeros((1, 14)),\n",
    "                                                columns=['product', 'v_title', 'v_videoId',\n",
    "                                                        'v_channelTitle', 'v_publishTime',\n",
    "                                                        'v_description', 'v_thumbnail',\n",
    "                                                        'c_id','c_parentId',\n",
    "                                                        'c_author', 'c_published_at',\n",
    "                                                        'c_updated_at', 'c_like_count',\n",
    "                                                        'c_text'])\n",
    "                    print('Unable to retrieve comments:', video.get('title', ''))\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "        return multiple_video_comments\n",
    "        \n",
    "    # alternative method by explicitely specifying videos\n",
    "    def create_video_df(self,products,videos,number_of_comments_per_video=100,number_of_replies_per_comment=100):\n",
    "        count=0\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            for video in videos[count]:\n",
    "                response = self.get_comments(video,maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment) \n",
    "                video=self.get_video_details(video)\n",
    "                comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "            count+=1\n",
    "        return multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen the Korean Drama called Squid Game to perform the sentiment analysis on. We specify the product in the products list, create a `api_handler` class object, use the `create_video_df_from_search()` function to automatically curate comments using the YouTube api call, and get a pandas Dataframe in return containing details about the videos and the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products=[\"Squid Game Korean Drama (2021)\"]\n",
    "\n",
    "youtube=api_handler(api_service_name, api_version, DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting replies: 749\n",
      "getting replies: 521\n",
      "getting replies: 16\n",
      "getting replies: 62\n",
      "getting replies: 64\n",
      "getting replies: 129\n",
      "getting replies: 14\n",
      "getting replies: 504\n",
      "getting replies: 101\n",
      "getting replies: 350\n",
      "getting replies: 16\n",
      "getting replies: 5\n",
      "getting replies: 3\n",
      "getting replies: 318\n",
      "getting replies: 25\n",
      "getting replies: 230\n",
      "getting replies: 390\n",
      "getting replies: 16\n",
      "getting replies: 154\n",
      "getting replies: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008476</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514569</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>420</td>\n",
       "      <td>Your videos are so interesting ❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1703</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>263</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41315</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>Ugx1tngYSiwMBAxozzR4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>violavio2464</td>\n",
       "      <td>2021-10-17T14:49:24Z</td>\n",
       "      <td>2021-10-17T14:49:24Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Ternyata susah nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41316</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>UgwAprZWhhPwhsFxapR4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>jurate0621</td>\n",
       "      <td>2023-02-28T07:11:45Z</td>\n",
       "      <td>2023-02-28T07:11:45Z</td>\n",
       "      <td>1</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41317</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>Ugxwic5QTKXMCTbXnYp4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>luishenrique8890</td>\n",
       "      <td>2021-11-05T16:37:17Z</td>\n",
       "      <td>2021-11-05T16:37:17Z</td>\n",
       "      <td>0</td>\n",
       "      <td>round 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41318</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>UgxJowxPsEGR6_aPK-h4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>kemsantos6131</td>\n",
       "      <td>2023-08-19T09:27:10Z</td>\n",
       "      <td>2023-08-19T09:27:10Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Dance in the squid game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41319</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>Ugwd_Rok4fvvcrlxiTF4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>figuaadam1818</td>\n",
       "      <td>2021-10-17T19:55:57Z</td>\n",
       "      <td>2021-10-17T19:55:57Z</td>\n",
       "      <td>0</td>\n",
       "      <td>un canal de youtube que acaba de empezar y me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41320 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "41315  Squid Game Korean Drama (2021)   \n",
       "41316  Squid Game Korean Drama (2021)   \n",
       "41317  Squid Game Korean Drama (2021)   \n",
       "41318  Squid Game Korean Drama (2021)   \n",
       "41319  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "0                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "1                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "2                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "4                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "...                                                  ...          ...   \n",
       "41315  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41316  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41317  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41318  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41319  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "\n",
       "      v_channelTitle         v_publishTime  \\\n",
       "0            MrBeast  2021-11-24T21:00:01Z   \n",
       "1            MrBeast  2021-11-24T21:00:01Z   \n",
       "2            MrBeast  2021-11-24T21:00:01Z   \n",
       "3            MrBeast  2021-11-24T21:00:01Z   \n",
       "4            MrBeast  2021-11-24T21:00:01Z   \n",
       "...              ...                   ...   \n",
       "41315    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41316    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41317    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41318    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41319    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "41315  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41316  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41317  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41318  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41319  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "41315  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41316  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41317  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41318  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41319  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "\n",
       "                             c_id c_parentId          c_author  \\\n",
       "0      UgzH8vliQSJKHQMGZjx4AaABAg                      MrBeast   \n",
       "1      UgwDhFNTCbfck5apuUJ4AaABAg                  DoodleChaos   \n",
       "2      UgzVlS_nKI4aXISU_ep4AaABAg                  mukul_editz   \n",
       "3      UgxykcUWbPcLhlL-Gy14AaABAg                  SpamR1_2013   \n",
       "4      Ugxu5B8dQ9-mZpfW-UV4AaABAg              user-cs9zv3gh1k   \n",
       "...                           ...        ...               ...   \n",
       "41315  Ugx1tngYSiwMBAxozzR4AaABAg                 violavio2464   \n",
       "41316  UgwAprZWhhPwhsFxapR4AaABAg                   jurate0621   \n",
       "41317  Ugxwic5QTKXMCTbXnYp4AaABAg             luishenrique8890   \n",
       "41318  UgxJowxPsEGR6_aPK-h4AaABAg                kemsantos6131   \n",
       "41319  Ugwd_Rok4fvvcrlxiTF4AaABAg                figuaadam1818   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z  2021-11-24T21:02:45Z       1008476   \n",
       "1      2021-11-24T22:07:54Z  2021-11-24T22:07:54Z        514569   \n",
       "2      2023-12-30T01:55:59Z  2023-12-30T01:55:59Z           420   \n",
       "3      2023-11-27T00:57:21Z  2023-11-27T00:57:21Z          1703   \n",
       "4      2024-01-30T20:17:02Z  2024-01-30T20:17:02Z           263   \n",
       "...                     ...                   ...           ...   \n",
       "41315  2021-10-17T14:49:24Z  2021-10-17T14:49:24Z             0   \n",
       "41316  2023-02-28T07:11:45Z  2023-02-28T07:11:45Z             1   \n",
       "41317  2021-11-05T16:37:17Z  2021-11-05T16:37:17Z             0   \n",
       "41318  2023-08-19T09:27:10Z  2023-08-19T09:27:10Z             0   \n",
       "41319  2021-10-17T19:55:57Z  2021-10-17T19:55:57Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                       Your videos are so interesting ❤  \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "41315                                 Ternyata susah nya  \n",
       "41316                                                lol  \n",
       "41317                                            round 6  \n",
       "41318                            Dance in the squid game  \n",
       "41319  un canal de youtube que acaba de empezar y me ...  \n",
       "\n",
       "[41320 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_video_comments=youtube.create_video_df_from_search(products,number_of_videos_per_product=20,number_of_comments_per_video=1000,number_of_replies_per_comment=100)\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Analysis, Selection and Labeling:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from:\n",
    "\n",
    "https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis : As emojis do not provide any helpful information they should be removed from the text strings.\n",
    "def remove_emojis(data):\n",
    "    if isinstance(data, str):\n",
    "        # Remove html tags\n",
    "        data = BeautifulSoup(data, \"html.parser\").get_text()\n",
    "        # Remove emote, etc\n",
    "        emoj = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                        \"]+\", re.UNICODE)\n",
    "        # english_words = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "        return re.sub(emoj, '', data)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row containing NA values.\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_4520\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n",
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_4520\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Length Before: 41320\n",
      "DataFrame Length After: 37266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008476</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514569</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>420</td>\n",
       "      <td>Your videos are so interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1703</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>263</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41315</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>Ugx1tngYSiwMBAxozzR4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>violavio2464</td>\n",
       "      <td>2021-10-17T14:49:24Z</td>\n",
       "      <td>2021-10-17T14:49:24Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Ternyata susah nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41316</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>UgwAprZWhhPwhsFxapR4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>jurate0621</td>\n",
       "      <td>2023-02-28T07:11:45Z</td>\n",
       "      <td>2023-02-28T07:11:45Z</td>\n",
       "      <td>1</td>\n",
       "      <td>lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41317</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>Ugxwic5QTKXMCTbXnYp4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>luishenrique8890</td>\n",
       "      <td>2021-11-05T16:37:17Z</td>\n",
       "      <td>2021-11-05T16:37:17Z</td>\n",
       "      <td>0</td>\n",
       "      <td>round 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41318</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>UgxJowxPsEGR6_aPK-h4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>kemsantos6131</td>\n",
       "      <td>2023-08-19T09:27:10Z</td>\n",
       "      <td>2023-08-19T09:27:10Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Dance in the squid game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41319</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>Ugwd_Rok4fvvcrlxiTF4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>figuaadam1818</td>\n",
       "      <td>2021-10-17T19:55:57Z</td>\n",
       "      <td>2021-10-17T19:55:57Z</td>\n",
       "      <td>0</td>\n",
       "      <td>un canal de youtube que acaba de empezar y me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37266 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "41315  Squid Game Korean Drama (2021)   \n",
       "41316  Squid Game Korean Drama (2021)   \n",
       "41317  Squid Game Korean Drama (2021)   \n",
       "41318  Squid Game Korean Drama (2021)   \n",
       "41319  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "0                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "1                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "2                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "4                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "...                                                  ...          ...   \n",
       "41315  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41316  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41317  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41318  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "41319  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "\n",
       "      v_channelTitle         v_publishTime  \\\n",
       "0            MrBeast  2021-11-24T21:00:01Z   \n",
       "1            MrBeast  2021-11-24T21:00:01Z   \n",
       "2            MrBeast  2021-11-24T21:00:01Z   \n",
       "3            MrBeast  2021-11-24T21:00:01Z   \n",
       "4            MrBeast  2021-11-24T21:00:01Z   \n",
       "...              ...                   ...   \n",
       "41315    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41316    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41317    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41318    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "41319    STREAM WARS  2021-10-08T08:46:18Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "41315  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41316  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41317  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41318  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "41319  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "41315  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41316  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41317  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41318  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "41319  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "\n",
       "                             c_id c_parentId          c_author  \\\n",
       "0      UgzH8vliQSJKHQMGZjx4AaABAg                      MrBeast   \n",
       "1      UgwDhFNTCbfck5apuUJ4AaABAg                  DoodleChaos   \n",
       "2      UgzVlS_nKI4aXISU_ep4AaABAg                  mukul_editz   \n",
       "3      UgxykcUWbPcLhlL-Gy14AaABAg                  SpamR1_2013   \n",
       "4      Ugxu5B8dQ9-mZpfW-UV4AaABAg              user-cs9zv3gh1k   \n",
       "...                           ...        ...               ...   \n",
       "41315  Ugx1tngYSiwMBAxozzR4AaABAg                 violavio2464   \n",
       "41316  UgwAprZWhhPwhsFxapR4AaABAg                   jurate0621   \n",
       "41317  Ugxwic5QTKXMCTbXnYp4AaABAg             luishenrique8890   \n",
       "41318  UgxJowxPsEGR6_aPK-h4AaABAg                kemsantos6131   \n",
       "41319  Ugwd_Rok4fvvcrlxiTF4AaABAg                figuaadam1818   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z  2021-11-24T21:02:45Z       1008476   \n",
       "1      2021-11-24T22:07:54Z  2021-11-24T22:07:54Z        514569   \n",
       "2      2023-12-30T01:55:59Z  2023-12-30T01:55:59Z           420   \n",
       "3      2023-11-27T00:57:21Z  2023-11-27T00:57:21Z          1703   \n",
       "4      2024-01-30T20:17:02Z  2024-01-30T20:17:02Z           263   \n",
       "...                     ...                   ...           ...   \n",
       "41315  2021-10-17T14:49:24Z  2021-10-17T14:49:24Z             0   \n",
       "41316  2023-02-28T07:11:45Z  2023-02-28T07:11:45Z             1   \n",
       "41317  2021-11-05T16:37:17Z  2021-11-05T16:37:17Z             0   \n",
       "41318  2023-08-19T09:27:10Z  2023-08-19T09:27:10Z             0   \n",
       "41319  2021-10-17T19:55:57Z  2021-10-17T19:55:57Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                        Your videos are so interesting   \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "41315                                 Ternyata susah nya  \n",
       "41316                                                lol  \n",
       "41317                                            round 6  \n",
       "41318                            Dance in the squid game  \n",
       "41319  un canal de youtube que acaba de empezar y me ...  \n",
       "\n",
       "[37266 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove emojis from the text to be analyzed\n",
    "multiple_video_comments['c_text']=multiple_video_comments['c_text'].apply(remove_emojis)\n",
    "\n",
    "df_length_before = len(multiple_video_comments)\n",
    "print(\"DataFrame Length Before:\", df_length_before)\n",
    "\n",
    "# Drop duplicates\n",
    "multiple_video_comments.drop_duplicates(inplace=True)\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)\n",
    "# Drop rows with empty or text length <= 2 comments\n",
    "multiple_video_comments = multiple_video_comments[multiple_video_comments['c_text'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "df_length_after = len(multiple_video_comments)\n",
    "print(\"DataFrame Length After:\", df_length_after)\n",
    "\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://stackoverflow.com/questions/40375366/pandas-to-csv-checking-for-overwrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        # Define keywords related to the TV show\n",
    "        self.tv_show_keywords = ['Squid Game', 'Gi-hun', 'Sang-woo', 'Player', 'Red light, green light', 'Honeycomb',\n",
    "                            'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                            'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho'\n",
    "                            'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                            'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali', ]\n",
    "        # Setting threshold value for validating the relevance of the comment\n",
    "        self.threshold = 1\n",
    "\n",
    "    # Tokenize text and remove stop words\n",
    "    def preprocess_text(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    # Matching function to check relevance of the comments\n",
    "    def match_keywords(self, tokens):\n",
    "        return [token for token in tokens if token in self.tv_show_keywords]\n",
    "\n",
    "    # Scoring function to calculate how many tokens matched\n",
    "    def calculate_score(self, tokens):\n",
    "        return len(tokens)\n",
    "\n",
    "    # Validate function to validate the relevance based on threshold\n",
    "    def validate_relevance(self, score):\n",
    "        return score >= self.threshold\n",
    "\n",
    "    def filter_comments(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        irrelevant_keywords = ['HYVE', 'crypto', 'promotion', 'ad', 'spam', 'advertisement', 'spoiler', 'leak', 'promo', 'off-topic', 'clickbait',\n",
    "                            'self-promotion', '0:', '1:', '2:', '3:', '4:', '5:', '6:', '7:',\n",
    "                            '8:', '9:', '10:', '11:', '12:', '13:', '14:', '15:']\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                if detect(row['c_text']) == 'en' and not any(keyword in row['c_text'] for keyword in irrelevant_keywords):\n",
    "                    comments.append(row)\n",
    "                    c += 1\n",
    "            except Exception as e:  # Catch any exception\n",
    "                pass\n",
    "        print(\"Number of Filtered Comments: \", c)\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])  # Create a new DataFrame from the list of rows\n",
    "        new_df = new_df.sort_values(by = ['c_like_count'], ascending = False)\n",
    "        new_df.drop_duplicates(inplace=True)\n",
    "        new_df = new_df[:4000]\n",
    "        return new_df\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        for index, row in df.iterrows():\n",
    "            processed_text = self.preprocess_text(row['c_text'])\n",
    "            matched_keywords = self.match_keywords(processed_text)\n",
    "            score = self.calculate_score(matched_keywords)\n",
    "            is_relevant = self.validate_relevance(score)\n",
    "            if is_relevant == 1:\n",
    "                comments.append(row)\n",
    "                c += 1\n",
    "\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])\n",
    "        print(\"Number of Processed Comments: \", c)\n",
    "        new_df = self.filter_comments(new_df)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Processed Comments:  3317\n",
      "Number of Filtered Comments:  2110\n"
     ]
    }
   ],
   "source": [
    "p = preprocessing()\n",
    "new_df = p.preprocess(multiple_video_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10395</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game (Behind The Scenes) #Shorts</td>\n",
       "      <td>4vb085gEgPc</td>\n",
       "      <td>Behind The Scenes</td>\n",
       "      <td>2022-03-20T16:43:54Z</td>\n",
       "      <td>This video gives you a chance to look BEHIND T...</td>\n",
       "      <td>https://i.ytimg.com/vi/4vb085gEgPc/default.jpg</td>\n",
       "      <td>UgxEeZvLDwVE2jcneuJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>_Taylo_</td>\n",
       "      <td>2022-06-09T00:13:48Z</td>\n",
       "      <td>2022-06-09T00:13:48Z</td>\n",
       "      <td>10273</td>\n",
       "      <td>Oh so the camera-man plays squid game too?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>SQUID GAME | RED LIGHT GREEN LIGHT SCENE</td>\n",
       "      <td>sH4Y450PSVM</td>\n",
       "      <td>memebappe</td>\n",
       "      <td>2021-10-14T18:52:37Z</td>\n",
       "      <td>BUY THE PERFECT CHRISTMAS GIFT    : https://am...</td>\n",
       "      <td>https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg</td>\n",
       "      <td>UgyXDA0Vdld5b5SMG2Z4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>silver-eyedfox7713</td>\n",
       "      <td>2022-01-16T01:30:47Z</td>\n",
       "      <td>2022-01-16T01:30:47Z</td>\n",
       "      <td>8217</td>\n",
       "      <td>This scene is the perfect introduction to how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36524</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgyBW8hKpZ3Tcxf0rJt4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>jeng6786</td>\n",
       "      <td>2021-10-12T02:08:48Z</td>\n",
       "      <td>2021-10-12T02:08:48Z</td>\n",
       "      <td>6886</td>\n",
       "      <td>Most memorable character : Ali. The marble sce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35572</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>Ugzvskgmdrku401Z3hF4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>ballinklong</td>\n",
       "      <td>2021-10-11T14:38:58Z</td>\n",
       "      <td>2021-10-11T14:38:58Z</td>\n",
       "      <td>6671</td>\n",
       "      <td>You realize Ali's personality is the only one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35614</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>Ugwyddf9lv0uoE5z15t4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>LittleCart</td>\n",
       "      <td>2021-10-12T11:58:45Z</td>\n",
       "      <td>2021-10-12T11:58:45Z</td>\n",
       "      <td>6274</td>\n",
       "      <td>Considering how popular Squid Game got, it's s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>SQUID GAME | RED LIGHT GREEN LIGHT SCENE</td>\n",
       "      <td>sH4Y450PSVM</td>\n",
       "      <td>memebappe</td>\n",
       "      <td>2021-10-14T18:52:37Z</td>\n",
       "      <td>BUY THE PERFECT CHRISTMAS GIFT    : https://am...</td>\n",
       "      <td>https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg</td>\n",
       "      <td>UgzQ3WuppfnDm5jt_IN4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>spiderboi7518</td>\n",
       "      <td>2022-09-19T01:15:02Z</td>\n",
       "      <td>2022-09-19T01:15:02Z</td>\n",
       "      <td>0</td>\n",
       "      <td>imagine dying in the first game lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>SQUID GAME | RED LIGHT GREEN LIGHT SCENE</td>\n",
       "      <td>sH4Y450PSVM</td>\n",
       "      <td>memebappe</td>\n",
       "      <td>2021-10-14T18:52:37Z</td>\n",
       "      <td>BUY THE PERFECT CHRISTMAS GIFT    : https://am...</td>\n",
       "      <td>https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg</td>\n",
       "      <td>UgwqGzOMkeU0EstRE3R4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>the.noodle.addict</td>\n",
       "      <td>2022-08-28T13:45:50Z</td>\n",
       "      <td>2022-08-28T13:45:50Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Man ali was a hero in this scene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9637</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>SQUID GAME | RED LIGHT GREEN LIGHT SCENE</td>\n",
       "      <td>sH4Y450PSVM</td>\n",
       "      <td>memebappe</td>\n",
       "      <td>2021-10-14T18:52:37Z</td>\n",
       "      <td>BUY THE PERFECT CHRISTMAS GIFT    : https://am...</td>\n",
       "      <td>https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg</td>\n",
       "      <td>UgzGZyaJO-Tz-KYYyMJ4AaABAg.9Tn-yT0YXv99VLukJbf44V</td>\n",
       "      <td>UgzGZyaJO-Tz-KYYyMJ4AaABAg</td>\n",
       "      <td>ZEROTATIONHD</td>\n",
       "      <td>2021-11-30T01:19:45Z</td>\n",
       "      <td>2021-11-30T01:19:45Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Pro Squid Game Players be like: part 2\\r\\nhttp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26160</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game (2021) Explained in Hindi / Urdu | ...</td>\n",
       "      <td>UMZv8M6_wHU</td>\n",
       "      <td>Movies Insight Hindi</td>\n",
       "      <td>2021-10-07T02:15:00Z</td>\n",
       "      <td>Squid Game (2021) Survival drama explained in ...</td>\n",
       "      <td>https://i.ytimg.com/vi/UMZv8M6_wHU/default.jpg</td>\n",
       "      <td>UgzxCv-8etC7o5alTNd4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>HarpreetSingh-zr9fz</td>\n",
       "      <td>2022-10-06T04:32:31Z</td>\n",
       "      <td>2022-10-06T04:32:31Z</td>\n",
       "      <td>0</td>\n",
       "      <td>i watched this squid game series. Tomorrow i c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41318</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Making Of SQUID GAME Part 2 - Best Of Behind T...</td>\n",
       "      <td>U872VWMv4C0</td>\n",
       "      <td>STREAM WARS</td>\n",
       "      <td>2021-10-08T08:46:18Z</td>\n",
       "      <td>\"Squid Game - Season 1\" Making Of (Part 2), Be...</td>\n",
       "      <td>https://i.ytimg.com/vi/U872VWMv4C0/default.jpg</td>\n",
       "      <td>UgxJowxPsEGR6_aPK-h4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>kemsantos6131</td>\n",
       "      <td>2023-08-19T09:27:10Z</td>\n",
       "      <td>2023-08-19T09:27:10Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Dance in the squid game</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2110 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "10395  Squid Game Korean Drama (2021)   \n",
       "8402   Squid Game Korean Drama (2021)   \n",
       "36524  Squid Game Korean Drama (2021)   \n",
       "35572  Squid Game Korean Drama (2021)   \n",
       "35614  Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "9671   Squid Game Korean Drama (2021)   \n",
       "9668   Squid Game Korean Drama (2021)   \n",
       "9637   Squid Game Korean Drama (2021)   \n",
       "26160  Squid Game Korean Drama (2021)   \n",
       "41318  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "10395             Squid Game (Behind The Scenes) #Shorts  4vb085gEgPc   \n",
       "8402            SQUID GAME | RED LIGHT GREEN LIGHT SCENE  sH4Y450PSVM   \n",
       "36524  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "35572  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "35614  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "...                                                  ...          ...   \n",
       "9671            SQUID GAME | RED LIGHT GREEN LIGHT SCENE  sH4Y450PSVM   \n",
       "9668            SQUID GAME | RED LIGHT GREEN LIGHT SCENE  sH4Y450PSVM   \n",
       "9637            SQUID GAME | RED LIGHT GREEN LIGHT SCENE  sH4Y450PSVM   \n",
       "26160  Squid Game (2021) Explained in Hindi / Urdu | ...  UMZv8M6_wHU   \n",
       "41318  Making Of SQUID GAME Part 2 - Best Of Behind T...  U872VWMv4C0   \n",
       "\n",
       "             v_channelTitle         v_publishTime  \\\n",
       "10395     Behind The Scenes  2022-03-20T16:43:54Z   \n",
       "8402             memebappe   2021-10-14T18:52:37Z   \n",
       "36524     Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "35572     Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "35614     Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "...                     ...                   ...   \n",
       "9671             memebappe   2021-10-14T18:52:37Z   \n",
       "9668             memebappe   2021-10-14T18:52:37Z   \n",
       "9637             memebappe   2021-10-14T18:52:37Z   \n",
       "26160  Movies Insight Hindi  2021-10-07T02:15:00Z   \n",
       "41318           STREAM WARS  2021-10-08T08:46:18Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "10395  This video gives you a chance to look BEHIND T...   \n",
       "8402   BUY THE PERFECT CHRISTMAS GIFT    : https://am...   \n",
       "36524  They may have survived the dalgona challenge i...   \n",
       "35572  They may have survived the dalgona challenge i...   \n",
       "35614  They may have survived the dalgona challenge i...   \n",
       "...                                                  ...   \n",
       "9671   BUY THE PERFECT CHRISTMAS GIFT    : https://am...   \n",
       "9668   BUY THE PERFECT CHRISTMAS GIFT    : https://am...   \n",
       "9637   BUY THE PERFECT CHRISTMAS GIFT    : https://am...   \n",
       "26160  Squid Game (2021) Survival drama explained in ...   \n",
       "41318  \"Squid Game - Season 1\" Making Of (Part 2), Be...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "10395  https://i.ytimg.com/vi/4vb085gEgPc/default.jpg   \n",
       "8402   https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg   \n",
       "36524  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "35572  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "35614  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "...                                               ...   \n",
       "9671   https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg   \n",
       "9668   https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg   \n",
       "9637   https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg   \n",
       "26160  https://i.ytimg.com/vi/UMZv8M6_wHU/default.jpg   \n",
       "41318  https://i.ytimg.com/vi/U872VWMv4C0/default.jpg   \n",
       "\n",
       "                                                    c_id  \\\n",
       "10395                         UgxEeZvLDwVE2jcneuJ4AaABAg   \n",
       "8402                          UgyXDA0Vdld5b5SMG2Z4AaABAg   \n",
       "36524                         UgyBW8hKpZ3Tcxf0rJt4AaABAg   \n",
       "35572                         Ugzvskgmdrku401Z3hF4AaABAg   \n",
       "35614                         Ugwyddf9lv0uoE5z15t4AaABAg   \n",
       "...                                                  ...   \n",
       "9671                          UgzQ3WuppfnDm5jt_IN4AaABAg   \n",
       "9668                          UgwqGzOMkeU0EstRE3R4AaABAg   \n",
       "9637   UgzGZyaJO-Tz-KYYyMJ4AaABAg.9Tn-yT0YXv99VLukJbf44V   \n",
       "26160                         UgzxCv-8etC7o5alTNd4AaABAg   \n",
       "41318                         UgxJowxPsEGR6_aPK-h4AaABAg   \n",
       "\n",
       "                       c_parentId             c_author        c_published_at  \\\n",
       "10395                                          _Taylo_  2022-06-09T00:13:48Z   \n",
       "8402                                silver-eyedfox7713  2022-01-16T01:30:47Z   \n",
       "36524                                         jeng6786  2021-10-12T02:08:48Z   \n",
       "35572                                      ballinklong  2021-10-11T14:38:58Z   \n",
       "35614                                       LittleCart  2021-10-12T11:58:45Z   \n",
       "...                           ...                  ...                   ...   \n",
       "9671                                     spiderboi7518  2022-09-19T01:15:02Z   \n",
       "9668                                 the.noodle.addict  2022-08-28T13:45:50Z   \n",
       "9637   UgzGZyaJO-Tz-KYYyMJ4AaABAg         ZEROTATIONHD  2021-11-30T01:19:45Z   \n",
       "26160                              HarpreetSingh-zr9fz  2022-10-06T04:32:31Z   \n",
       "41318                                    kemsantos6131  2023-08-19T09:27:10Z   \n",
       "\n",
       "               c_updated_at  c_like_count  \\\n",
       "10395  2022-06-09T00:13:48Z         10273   \n",
       "8402   2022-01-16T01:30:47Z          8217   \n",
       "36524  2021-10-12T02:08:48Z          6886   \n",
       "35572  2021-10-11T14:38:58Z          6671   \n",
       "35614  2021-10-12T11:58:45Z          6274   \n",
       "...                     ...           ...   \n",
       "9671   2022-09-19T01:15:02Z             0   \n",
       "9668   2022-08-28T13:45:50Z             0   \n",
       "9637   2021-11-30T01:19:45Z             0   \n",
       "26160  2022-10-06T04:32:31Z             0   \n",
       "41318  2023-08-19T09:27:10Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "10395         Oh so the camera-man plays squid game too?  \n",
       "8402   This scene is the perfect introduction to how ...  \n",
       "36524  Most memorable character : Ali. The marble sce...  \n",
       "35572  You realize Ali's personality is the only one ...  \n",
       "35614  Considering how popular Squid Game got, it's s...  \n",
       "...                                                  ...  \n",
       "9671                 imagine dying in the first game lol  \n",
       "9668                   Man ali was a hero in this scene   \n",
       "9637   Pro Squid Game Players be like: part 2\\r\\nhttp...  \n",
       "26160  i watched this squid game series. Tomorrow i c...  \n",
       "41318                            Dance in the squid game  \n",
       "\n",
       "[2110 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling comments using Sentiment Lexicon - VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(c_text):\n",
    "    sentiment_Score = sentiment_lexicon.polarity_scores(c_text)\n",
    "    return sentiment_Score['compound']\n",
    "\n",
    "def check_sentiment(sentiment_score):\n",
    "    if sentiment_score > 0.00:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0.00:\n",
    "        return \"Negative\"\n",
    "    elif sentiment_score == 0:\n",
    "        return \"Neutral \"\n",
    "\n",
    "new_df['sentiment_score'] = new_df['c_text'].apply(get_sentiment_score)\n",
    "new_df['sentiment'] = new_df['sentiment_score'].apply(check_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Comments for each polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0018  Negative           @@user-lq1xk3dd4m ignore him, he's just jealous. I am Chinese descent myself and absolutely love the squid game\n",
      "    -0.0164  Negative           Hmmm so many people died in the game if I would be in their place I would know how to play it and then play the game .papa people .\n",
      "    -0.0258  Negative           If player 1 was the boss then who started the game after his death?\n",
      "                                You explanation  is amazing\n",
      "    -0.0258  Negative           Game of Death- Bruce Lee would win\n",
      "    -0.0258  Negative           This scene is the perfect introduction to how the games are a matter of life and death. The slow build up, the sudden reveal that the dude was really shot, and the ensuing chaos make this the perfect introduction to these games.\n",
      "    -0.0258  Negative           I started to like her in yhe squid game and I cried when she sacrificed herself\n",
      "    -0.0258  Negative           *Obviously our hero will play the game again to find out who is behind it after the old man's death and to stop it forever ...*\n",
      "    -0.0258  Negative           Squid game occurs to some extent in boxing, wrestling etc where normal people including vip's enjoy the pain of others\n",
      "    -0.0263  Negative           @@syungyul for her role as Sae byeok being closed off and cold and monotonous is kindof the point..after the kind of life she has lived she has learned to protect herself and to not trust anyone or open up...the only emotions she shows is mostly for her family and for ji yong in marbles game..\n",
      "    -0.0387  Negative           Sorry but my whole school watches the squid game\n",
      "     0       Neutral            Dalgona candy makers after squid game: \"Business is Boomin\"\n",
      "     0       Neutral            Ali and detective aren't confirmed their death's scene\n",
      "     0       Neutral            @Squid Game 2.0 in my channel bot\n",
      "     0       Neutral            https://youtu.be/p8X71J8cNDA squid game fanart sae-byeok\n",
      "     0       Neutral            Squid Game Dalgona Honeycomb Candy Recipe\n",
      "\n",
      "                                https://youtu.be/GvjsM4_wPu8\n",
      "     0       Neutral            https://youtu.be/p8X71J8cNDA squid game fanart sae-byeok\n",
      "     0       Neutral            Squid Game Dalgona Honeycomb Candy Recipe\n",
      "\n",
      "                                https://youtu.be/GvjsM4_wPu8\n",
      "     0       Neutral            @Squid Game 2.0 in my channel cap\n",
      "     0       Neutral            Squid Game Dalgona Honeycomb Candy Recipe\n",
      "\n",
      "                                https://youtu.be/GvjsM4_wPu8\n",
      "     0       Neutral            @Squid Game 2.0 in my channel\n",
      "     0.9858  Positive           When i saw 240 i was like \"WOW MY FAVORITE\" because she looks like me!! I don't know why? I'm still happy i love squid game! It's the best! And by the way... MERRY CHRISTMAS!!\n",
      "     0.985   Positive           Really soft and wonderful music. I fell in love with the Squid Game series and this special music .. I am from Iran. Travel to Iran. Iranians are very kind and hospitable. I work to show Iran to the world on YouTube. If you are interested in seeing Iran 2021 virtual walking tours, I will be happy to watch my channel videos. My content is of great quality. Thanks for the support.\n",
      "     0.9842  Positive           WOW! THAT'S SO EPIC I ENJOYED SO AMAZING THAT'S THE AMAZING SQUID GAME VIDEO OMG I LOVE IT AND THE END IS SO AWESOME AND AMAZING\n",
      "     0.9825  Positive           Yep.\n",
      "\n",
      "\n",
      "                                The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9815  Positive           Yeh.\n",
      "\n",
      "                                The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9815  Positive           The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9815  Positive           The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9799  Positive           To everyone:guys ur the best thank u for recommending all these amazing shows - the best thing about Netflix is that it really breaks borders and celebrates international drama. I loved every minute of    game and can't wait to check out all the other similar Korean dramas you good folks recommended\n",
      "     0.979   Positive           This was such an amazing show! I loved every bit of it. It was so gripping, so emotional, and very well executed! I can't wait for a second season. He didn't get on the plane and acted as if he was looking for revenge. I loved the marble game the most. As soon as they said they were to be in teams of 2, I knew they were going to play against each other where one in each pair would be eliminated. They of course thought to pair up with their friend and be teammates. Everything about the show was playing on all the different social dynamics within human interactions, and it was done perfectly!\n",
      "     0.9781  Positive           Oh my god yes yes yes yes yes yes yes yes oh my god oh my goodness Bruh Bruh Bruh Bruh Bruh Bruh Bruh Bruh Bruh season two squid game are you kidding me\n"
     ]
    }
   ],
   "source": [
    "def select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('sentiment')\n",
    "\n",
    "    # iterate over each polarity group\n",
    "    for sentiment, group in grouped:\n",
    "        # sort comments by sentiment score pick top 10\n",
    "        top_group_comments = group.sort_values(by='sentiment_score', ascending=False).head(top_n)[['sentiment_score', 'sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "# # top 10 comments for each polarity\n",
    "# for sentiment_score, sentiment, comment in top_comments:\n",
    "#     print(f\"Sentiment: {sentiment}, Sentiment Score: {sentiment_score}, Comment: {comment}\")\n",
    "\n",
    "# making it pretty~~~\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(top_comments, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Lexicon using TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0018  Negative           @@user-lq1xk3dd4m ignore him, he's just jealous. I am Chinese descent myself and absolutely love the squid game\n",
      "    -0.0164  Negative           Hmmm so many people died in the game if I would be in their place I would know how to play it and then play the game .papa people .\n",
      "    -0.0258  Negative           If player 1 was the boss then who started the game after his death?\n",
      "                                You explanation  is amazing\n",
      "    -0.0258  Negative           Game of Death- Bruce Lee would win\n",
      "    -0.0258  Negative           This scene is the perfect introduction to how the games are a matter of life and death. The slow build up, the sudden reveal that the dude was really shot, and the ensuing chaos make this the perfect introduction to these games.\n",
      "    -0.0258  Negative           I started to like her in yhe squid game and I cried when she sacrificed herself\n",
      "    -0.0258  Negative           *Obviously our hero will play the game again to find out who is behind it after the old man's death and to stop it forever ...*\n",
      "    -0.0258  Negative           Squid game occurs to some extent in boxing, wrestling etc where normal people including vip's enjoy the pain of others\n",
      "    -0.0263  Negative           @@syungyul for her role as Sae byeok being closed off and cold and monotonous is kindof the point..after the kind of life she has lived she has learned to protect herself and to not trust anyone or open up...the only emotions she shows is mostly for her family and for ji yong in marbles game..\n",
      "    -0.0387  Negative           Sorry but my whole school watches the squid game\n",
      "     0       Neutral            Dalgona candy makers after squid game: \"Business is Boomin\"\n",
      "     0       Neutral            Ali and detective aren't confirmed their death's scene\n",
      "     0       Neutral            @Squid Game 2.0 in my channel bot\n",
      "     0       Neutral            https://youtu.be/p8X71J8cNDA squid game fanart sae-byeok\n",
      "     0       Neutral            Squid Game Dalgona Honeycomb Candy Recipe\n",
      "\n",
      "                                https://youtu.be/GvjsM4_wPu8\n",
      "     0       Neutral            https://youtu.be/p8X71J8cNDA squid game fanart sae-byeok\n",
      "     0       Neutral            Squid Game Dalgona Honeycomb Candy Recipe\n",
      "\n",
      "                                https://youtu.be/GvjsM4_wPu8\n",
      "     0       Neutral            @Squid Game 2.0 in my channel cap\n",
      "     0       Neutral            Squid Game Dalgona Honeycomb Candy Recipe\n",
      "\n",
      "                                https://youtu.be/GvjsM4_wPu8\n",
      "     0       Neutral            @Squid Game 2.0 in my channel\n",
      "     0.9858  Positive           When i saw 240 i was like \"WOW MY FAVORITE\" because she looks like me!! I don't know why? I'm still happy i love squid game! It's the best! And by the way... MERRY CHRISTMAS!!\n",
      "     0.985   Positive           Really soft and wonderful music. I fell in love with the Squid Game series and this special music .. I am from Iran. Travel to Iran. Iranians are very kind and hospitable. I work to show Iran to the world on YouTube. If you are interested in seeing Iran 2021 virtual walking tours, I will be happy to watch my channel videos. My content is of great quality. Thanks for the support.\n",
      "     0.9842  Positive           WOW! THAT'S SO EPIC I ENJOYED SO AMAZING THAT'S THE AMAZING SQUID GAME VIDEO OMG I LOVE IT AND THE END IS SO AWESOME AND AMAZING\n",
      "     0.9825  Positive           Yep.\n",
      "\n",
      "\n",
      "                                The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9815  Positive           Yeh.\n",
      "\n",
      "                                The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9815  Positive           The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9815  Positive           The north Korea girls and the old man with his friends playing marbles on the game of death was the best scene. A real jerker. They didn't rush like the rest, but took their time to enjoy till their last minutes.\n",
      "\n",
      "                                The old man should have won, but he put on a nice act on being dementia and gave his friend the chance to make a comeback.\n",
      "                                He even revealed it at the end that the friend try to cheat him.\n",
      "\n",
      "                                The moral of the story for me after watching this.. don't be wasteful and greedy.\n",
      "\n",
      "                                Just be happy on what you have.\n",
      "\n",
      "                                I feel like a lot of them don't have to die.\n",
      "     0.9799  Positive           To everyone:guys ur the best thank u for recommending all these amazing shows - the best thing about Netflix is that it really breaks borders and celebrates international drama. I loved every minute of    game and can't wait to check out all the other similar Korean dramas you good folks recommended\n",
      "     0.979   Positive           This was such an amazing show! I loved every bit of it. It was so gripping, so emotional, and very well executed! I can't wait for a second season. He didn't get on the plane and acted as if he was looking for revenge. I loved the marble game the most. As soon as they said they were to be in teams of 2, I knew they were going to play against each other where one in each pair would be eliminated. They of course thought to pair up with their friend and be teammates. Everything about the show was playing on all the different social dynamics within human interactions, and it was done perfectly!\n",
      "     0.9781  Positive           Oh my god yes yes yes yes yes yes yes yes oh my god oh my goodness Bruh Bruh Bruh Bruh Bruh Bruh Bruh Bruh Bruh season two squid game are you kidding me\n"
     ]
    }
   ],
   "source": [
    "def text_blob_sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def texblob_check_sentiment(score):\n",
    "    if score == 0:\n",
    "        return 'Neutral'\n",
    "    elif score < 0.00:\n",
    "        return 'Negative'\n",
    "    elif score > 0.00:\n",
    "        return 'Positive'\n",
    "\n",
    "new_df['textblob_score'] = new_df['c_text'].apply(text_blob_sentiment_score)\n",
    "new_df['textblob_sentiment'] = new_df['textblob_score'].apply(texblob_check_sentiment)\n",
    "\n",
    "def textblob_select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('textblob_sentiment')\n",
    "\n",
    "    for sentiment, group in grouped:\n",
    "        top_group_comments = group.sort_values(by='textblob_score', ascending=False).head(top_n)[['textblob_score', 'textblob_sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "textblob_top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(textblob_top_comments, headers=headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10395    None\n",
       "8402     None\n",
       "36524    None\n",
       "35572    None\n",
       "35614    None\n",
       "         ... \n",
       "9671     None\n",
       "9668     None\n",
       "9637     None\n",
       "26160    None\n",
       "41318    None\n",
       "Length: 2110, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['positive', 'negative', 'neutral']\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def col_to_txt(row):\n",
    "    sentiment = row['sentiment']  \n",
    "    c_text = row['c_text']\n",
    "    file_name = f\"{sentiment}_{row.c_id}.txt\"  \n",
    "    folder = f\"{sentiment.strip()}\"  \n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(c_text)\n",
    "\n",
    "new_df.apply(col_to_txt, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'final_comments_df.csv'\n",
    "# files_present = glob.glob(filename)\n",
    "# # will only write to disk if file doesnt exist\n",
    "# if not files_present:\n",
    "#     new_df.to_csv(filename, index=False)\n",
    "#     new_df\n",
    "# else:\n",
    "#     print (f'File Already Exists. Delete {filename}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the root directory containing the positive, negative, and neutral folders\n",
    "root_dir = ''\n",
    "\n",
    "# Define the directories for train and test sets\n",
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "try:\n",
    "    shutil.rmtree(os.path.join(root_dir, train_dir))\n",
    "    shutil.rmtree(os.path.join(root_dir, test_dir))\n",
    "except:\n",
    "    pass\n",
    "# Define the ratio for train-test split\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Iterate through each sentiment folder\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    # Get the list of file paths in the current sentiment folder\n",
    "    files = os.listdir(os.path.join(root_dir, sentiment))\n",
    "    # Shuffle the file paths\n",
    "    random.shuffle(files)\n",
    "    # Calculate the split index based on the split ratio\n",
    "    split_index = int(len(files) * split_ratio)\n",
    "    # Split the files into train and test sets\n",
    "    train_files = files[:split_index]\n",
    "    test_files = files[split_index:]\n",
    "    \n",
    "    # Move train files to train directory\n",
    "    for file in train_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(train_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "    # Move test files to test directory\n",
    "    for file in test_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(test_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Remove the sentiment folders\n",
    "try:\n",
    "    shutil.rmtree('negative')\n",
    "    shutil.rmtree('neutral')\n",
    "    shutil.rmtree('positive')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Text Analytics Pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(train_dir)\n",
    "reviews_test = load_files(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'The game is called Capitalism.' 1\n",
      "b'@@weeblord69ers Rip-off means just blatantly plagiarizing a piece of work. Just because Squid Game follows similar concepts, it doesn\\xe2\\x80\\x99t mean that it\\xe2\\x80\\x99s exactly copying. The story is WAY different.' 1\n",
      "b\"@@user-kt9yq7iy9j U are right about all the games but I think the 1st game is definitely Japanese. We call it Daruma-san ga koronda. Literally Daruma falls down. It's not a Korean game.\" 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(text_train[1],y_train[1])\n",
    "print(text_train[2],y_train[2])\n",
    "print(text_train[3],y_train[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect = CountVectorizer().fit(reviews_train)\n",
    "X_train = vect.transform(reviews_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_tokens = ['drama', 'film', 'cinema', 'actor', 'actress', 'director', 'plot',\n",
    "                  'scene', 'genre', 'subtitles', 'k-drama', 'kdrama', 'k-movie', 'television',\n",
    "                  'episode', 'screenplay', 'script', 'cinematography', 'soundtrack',\n",
    "                  'OST', 'character', 'plot twist', 'review', 'ratings', 'premiere',\n",
    "                  'streaming', 'watchlist', 'subbed', 'dubbed', 'sequel', 'game', 'song',\n",
    "                  'season', 'trailer', 'casting', 'fanbase', 'recommendation', 'goblin',\n",
    "                  'viewer', 'critic', 'Korean', 'entertainment', 'watched', 'guardian',\n",
    "                  'show', 'squid', 'watch', 'watching', 'acting', 'netflix', 'show','end',\n",
    "                  'squid game', 'gi-hun', 'Sang-woo', 'Player', 'Red light', 'green light', 'Honeycomb',\n",
    "                  'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                  'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho',\n",
    "                  'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                  'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali']\n",
    "\n",
    "product_tokens = [item.lower() for item in product_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    return text_without_punctuation\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # stopwords punctuation etc\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    # split into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    # removes stopwords and numbers and stems from tokens makes sure its all lowercase too\n",
    "    tokens = [stemmer.stem(remove_punctuation(token)) for token in tokens if token.isalnum() and token.lower() not in product_tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.77      0.50        52\n",
      "           1       0.67      0.71      0.69        96\n",
      "           2       0.92      0.72      0.81       276\n",
      "\n",
      "    accuracy                           0.72       424\n",
      "   macro avg       0.66      0.73      0.67       424\n",
      "weighted avg       0.80      0.72      0.74       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('preprocess', \n",
    "    TfidfVectorizer(\n",
    "                    encoding=\"utf-8\",\n",
    "                    strip_accents='ascii',\n",
    "                    lowercase=True,\n",
    "                    preprocessor=preprocess_text,\n",
    "                    # tokenizer=,\n",
    "                    # analyzer=,\n",
    "                    stop_words='english',\n",
    "                    norm='l2',\n",
    "                    ngram_range=(1, 1),\n",
    "                    max_df=0.08,\n",
    "                    min_df=0.0004,\n",
    "                    max_features=370,\n",
    "                    binary=True,\n",
    "                    use_idf=True,\n",
    "                    smooth_idf=True,\n",
    "                    sublinear_tf=True\n",
    "                    )\n",
    "    # CountVectorizer(preprocessor=preprocess_text,ngram_range=(1, 1))\n",
    "     ), \n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "text_clf.fit(text_train, y_train)\n",
    "y_pred = text_clf.predict(text_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Run only till here and check coz the grid search would take long so better to adjust by looking at this only ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.07, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.40      0.53       107\n",
      "           1       0.72      0.59      0.65       101\n",
      "           2       0.70      0.92      0.79       216\n",
      "\n",
      "    accuracy                           0.71       424\n",
      "   macro avg       0.73      0.64      0.66       424\n",
      "weighted avg       0.72      0.71      0.69       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__norm': ['l2','l1'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'vectorizer__use_idf': [True,False],\n",
    "    'vectorizer__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1273: Mean Test Score - 0.6893, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.07, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1417: Mean Test Score - 0.6893, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.07, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1465: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1501: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1609: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1645: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1165: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1201: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1237: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.08, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1309: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "results = grid_search.cv_results_\n",
    " \n",
    "scores = results['mean_test_score']\n",
    "\n",
    "params = results['params']\n",
    "\n",
    "top_models_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores[i], params[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.07, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.40      0.53       107\n",
      "           1       0.72      0.59      0.65       101\n",
      "           2       0.70      0.92      0.79       216\n",
      "\n",
      "    accuracy                           0.71       424\n",
      "   macro avg       0.73      0.64      0.66       424\n",
      "weighted avg       0.72      0.71      0.69       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_ct = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "parameters_ct = {\n",
    "    'vectorizer': [CountVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    'tfidf__norm': ['l2','l1'],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'tfidf__use_idf': [True,False],\n",
    "    'tfidf__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_ct = GridSearchCV(pipeline_ct, parameters_ct, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_ct.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_ct.best_params_)\n",
    "best_model_ct = grid_search_ct.best_estimator_\n",
    "y_pred_ct = best_model_ct.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1183: Mean Test Score - 0.6893, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.07, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1219: Mean Test Score - 0.6893, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.07, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1231: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1240: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1267: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1276: Mean Test Score - 0.6875, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1156: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1165: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1174: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.08, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 1192: Mean Test Score - 0.6869, Parameters - {'classifier': LogisticRegression(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': False, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.003, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x000001C483CC5790>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "results_ct = grid_search_ct.cv_results_\n",
    " \n",
    "scores_ct = results_ct['mean_test_score']\n",
    "\n",
    "params_ct = results_ct['params']\n",
    "\n",
    "top_models_indices_ct = sorted(range(len(scores_ct)), key=lambda i: scores_ct[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices_ct:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores_ct[i], params_ct[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualization and Insights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Discussion and conclusion from experiments:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
