{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farheenfab/AppliedText_CW/blob/main/CW1-generate_dataset.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F20AA Coursework 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect\n",
    "import shutil\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the api service name, version and developer key for the api call.\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyAWj_uzrhZL18X32S_P79pT1wnSYGpuA4k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/search/list#parameters\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/comments/list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a class called api_handler which contains functions such as `get_video_details()`, `get_videos()`, `get_video_df()`, `get_comments()`, `get_comment_replies()`, `get_comments_df()`, `create_video_df_from_search()`, `create_video_df()`. These functions help us by either manually retrieving the videos and comments or by automatically curating the videos and comments using the product given to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class api_handler:\n",
    "    def __init__(self, api_service_name, api_version, developer_key):\n",
    "        self.client = googleapiclient.discovery.build(api_service_name,\n",
    "                                                    api_version,\n",
    "                                                    developerKey=developer_key)\n",
    "        \n",
    "    # Search for videos details given id\n",
    "    def get_video_details(self, videoId, part=\"snippet\"):\n",
    "        request = self.client.videos().list(\n",
    "            part=part,\n",
    "            id=videoId\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response:\n",
    "            video_details = response['items'][0]\n",
    "            snippet=video_details['snippet']\n",
    "            snippet['videoId']=videoId\n",
    "            snippet['id']=videoId\n",
    "            snippet['publishTime']=video_details.get('snippet', {}).get('publishedAt', {})\n",
    "            snippet['thumbnails']=video_details.get('snippet', {}).get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "            return snippet\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Search for videos given query\n",
    "    def get_videos(self,query,maxResults=5,part=\"snippet\"):\n",
    "        request = self.client.search().list(\n",
    "            part=part,\n",
    "            maxResults=maxResults,\n",
    "            # higher view count is likely to be more relevent \n",
    "            order=\"viewCount\",\n",
    "            q=query,  \n",
    "            # american region videos \n",
    "            regionCode=\"US\",\n",
    "            # english videos\n",
    "            relevanceLanguage=\"en\",\n",
    "            type=\"video\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        return response\n",
    "    \n",
    "    # Format Response from get_videos to dataframe\n",
    "    def get_video_df(response):\n",
    "        items=[]\n",
    "        for item in response['items']:\n",
    "            snippet=item.get('snippet', {})\n",
    "            items+=[{\n",
    "                'title':snippet.get('title', ''),\n",
    "                'videoId':item.get('id', {}).get('videoId', ''),\n",
    "                'channelTitle':snippet.get('channelTitle', ''),\n",
    "                'publishTime':snippet.get('publishTime', ''),\n",
    "                'description':snippet.get('description', ''),\n",
    "                'thumbnails':snippet.get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "                }]\n",
    "        df=pd.DataFrame(items)\n",
    "        return df\n",
    "    \n",
    "    # Get comments from video\n",
    "    def get_comments(self,videoId,part=\"snippet\",maxResults=100,maxResultsDepth=100):\n",
    "        all_comments = []\n",
    "        f = 0\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0:\n",
    "            request = self.client.commentThreads().list(\n",
    "                part=part,\n",
    "                videoId=videoId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                order='relevance',\n",
    "                moderationStatus='published',\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if 'items' in response:\n",
    "                all_comments+=[response]\n",
    "                for item in response['items']:\n",
    "                    # extract the comment ID to get replies\n",
    "                    comment_id = item.get('snippet',{}).get('topLevelComment',{}).get('id','')\n",
    "                    if item.get('snippet',{}).get('totalReplyCount',0)>2:\n",
    "                        if f == 0:\n",
    "                            print('getting replies:',item.get('snippet',{}).get('totalReplyCount',0))\n",
    "                            f = 1\n",
    "                        replies = self.get_comment_replies(comment_id, maxResults=maxResultsDepth)\n",
    "                        all_comments += replies\n",
    "\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "    \n",
    "    # Get replies from comment \n",
    "    def get_comment_replies(self, commentId, part=\"snippet\", maxResults=100):\n",
    "        all_comments = []\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0 and (nextPageToken != None or len(all_comments)==0):\n",
    "\n",
    "            request = self.client.comments().list(\n",
    "                part=part,\n",
    "                parentId=commentId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "            if 'items' in response and len(response['items'])>0:\n",
    "                for item in response['items']:\n",
    "                    modified_response = {\n",
    "                        'items': [\n",
    "                            {\n",
    "                                'id':item.get('id'),\n",
    "                                'snippet': {\n",
    "                                    'topLevelComment': {\n",
    "                                        'snippet': item.get('snippet','')\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    all_comments += [modified_response]\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "\n",
    "    # Format response from get_comments to dataframe\n",
    "    def get_comments_df(response, video,product):\n",
    "        comments = []\n",
    "        for pages in response:\n",
    "            for item in pages['items']:\n",
    "                comment = item.get('snippet', {}).get('topLevelComment', {}).get('snippet', {})\n",
    "                comments.append([\n",
    "                        product,\n",
    "                        video.get('title', ''),\n",
    "                        video.get('videoId', ''),\n",
    "                        video.get('channelTitle', ''),\n",
    "                        video.get('publishTime', ''),\n",
    "                        video.get('description', ''),\n",
    "                        video.get('thumbnails', ''),\n",
    "                        item.get('id', ''),  \n",
    "                        comment.get('parentId', ''),  \n",
    "                        comment.get('authorDisplayName', '')[1:],  \n",
    "                        comment.get('publishedAt', ''),\n",
    "                        comment.get('updatedAt', ''),\n",
    "                        comment.get('likeCount', ''),\n",
    "                        comment.get('textDisplay', '')\n",
    "                    ])\n",
    "\n",
    "        df = pd.DataFrame(comments,\n",
    "            columns=['product', 'v_title', 'v_videoId',\n",
    "                    'v_channelTitle', 'v_publishTime',\n",
    "                    'v_description', 'v_thumbnail',\n",
    "                    'c_id','c_parentId',\n",
    "                    'c_author', 'c_published_at',\n",
    "                    'c_updated_at', 'c_like_count',\n",
    "                    'c_text'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Search for videos related to products iteratively\n",
    "    # Collect comments from each video and place it into an array\n",
    "    def create_video_df_from_search(self, products,\n",
    "                                    number_of_videos_per_product=5,\n",
    "                                    number_of_comments_per_video=100\n",
    "                                    ,number_of_replies_per_comment=100):\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            # get 25 first videos with the highest viewer counts \n",
    "            response = self.get_videos(query=product, maxResults=number_of_videos_per_product)\n",
    "            # Convert results to df\n",
    "            videos_df = api_handler.get_video_df(response)\n",
    "            # For each video get a maximum of 100 comments\n",
    "            # and place comments into an array\n",
    "            for _, video in videos_df.iterrows():\n",
    "                try:\n",
    "                    response = self.get_comments(video['videoId'], maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment)\n",
    "                    comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                except:\n",
    "                    # Function fails as the API returns 403 if the channel has comments disabled\n",
    "                    # place an empty entry instead it can be deleted later\n",
    "                    comments_df = pd.DataFrame(np.zeros((1, 14)),\n",
    "                                                columns=['product', 'v_title', 'v_videoId',\n",
    "                                                        'v_channelTitle', 'v_publishTime',\n",
    "                                                        'v_description', 'v_thumbnail',\n",
    "                                                        'c_id','c_parentId',\n",
    "                                                        'c_author', 'c_published_at',\n",
    "                                                        'c_updated_at', 'c_like_count',\n",
    "                                                        'c_text'])\n",
    "                    print('Unable to retrieve comments:', video.get('title', ''))\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "        return multiple_video_comments\n",
    "        \n",
    "    # alternative method by explicitely specifying videos\n",
    "    def create_video_df(self,products,videos,number_of_comments_per_video=100,number_of_replies_per_comment=100):\n",
    "        count=0\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            for video in videos[count]:\n",
    "                response = self.get_comments(video,maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment) \n",
    "                video=self.get_video_details(video)\n",
    "                comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "            count+=1\n",
    "        return multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen the Korean Drama called Squid Game to perform the sentiment analysis on. We specify the product in the products list, create a `api_handler` class object, use the `create_video_df_from_search()` function to automatically curate comments using the YouTube api call, and get a pandas Dataframe in return containing details about the videos and the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products=[\"Squid Game Korean Drama (2021)\"]\n",
    "\n",
    "youtube=api_handler(api_service_name, api_version, DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting replies: 750\n",
      "getting replies: 520\n",
      "getting replies: 497\n",
      "getting replies: 16\n",
      "getting replies: 62\n",
      "getting replies: 64\n",
      "getting replies: 129\n",
      "getting replies: 14\n",
      "getting replies: 504\n",
      "getting replies: 101\n",
      "getting replies: 350\n",
      "Unable to retrieve comments: Squid Game | Official Trailer | Netflix\n",
      "Unable to retrieve comments: Red light green light ü§Ø the death game ‚ò†Ô∏è | Squid Game | #kdrama #shorts\n",
      "Unable to retrieve comments: Sugar Honeycombs ü§Ø the death game ‚ò†Ô∏è | Squid Game | #shorts #kdrama\n",
      "getting replies: 3\n",
      "Unable to retrieve comments: Squid game remake (red light and green light)\n",
      "Unable to retrieve comments: Squid Game (2021) Explained in Hindi / Urdu | Squid Games Full Summarized ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä\n",
      "Unable to retrieve comments: Sugar Honeycomb No Blood - Squid Game 2\n",
      "Unable to retrieve comments: (PART 1) Ketika Terjebak Di Dalam Game Yang Mempertaruhkan Nyawa Ep 1 - 3 | ALUR CERITA SQUID GAME\n",
      "Unable to retrieve comments: Squid Game Season 2 Teaser Trailer | Life is a Bet | Netflix Series | TeaserPRO&#39;s Concept Version\n",
      "Unable to retrieve comments: Red Light Green Light No Blood - Squid Game 1\n",
      "Unable to retrieve comments: Squid Game stars take on the Dalgona Challenge [ENG SUB]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008391.0</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514504.0</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>399.0</td>\n",
       "      <td>Your videos are so interesting ‚ù§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>259.0</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19926</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19927</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19928</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19929</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19930</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19931 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product                            v_title  \\\n",
       "0      Squid Game Korean Drama (2021)  $456,000 Squid Game In Real Life!   \n",
       "1      Squid Game Korean Drama (2021)  $456,000 Squid Game In Real Life!   \n",
       "2      Squid Game Korean Drama (2021)  $456,000 Squid Game In Real Life!   \n",
       "3      Squid Game Korean Drama (2021)  $456,000 Squid Game In Real Life!   \n",
       "4      Squid Game Korean Drama (2021)  $456,000 Squid Game In Real Life!   \n",
       "...                               ...                                ...   \n",
       "19926                             0.0                                0.0   \n",
       "19927                             0.0                                0.0   \n",
       "19928                             0.0                                0.0   \n",
       "19929                             0.0                                0.0   \n",
       "19930                             0.0                                0.0   \n",
       "\n",
       "         v_videoId v_channelTitle         v_publishTime  \\\n",
       "0      0e3GPea1Tyg        MrBeast  2021-11-24T21:00:01Z   \n",
       "1      0e3GPea1Tyg        MrBeast  2021-11-24T21:00:01Z   \n",
       "2      0e3GPea1Tyg        MrBeast  2021-11-24T21:00:01Z   \n",
       "3      0e3GPea1Tyg        MrBeast  2021-11-24T21:00:01Z   \n",
       "4      0e3GPea1Tyg        MrBeast  2021-11-24T21:00:01Z   \n",
       "...            ...            ...                   ...   \n",
       "19926          0.0            0.0                   0.0   \n",
       "19927          0.0            0.0                   0.0   \n",
       "19928          0.0            0.0                   0.0   \n",
       "19929          0.0            0.0                   0.0   \n",
       "19930          0.0            0.0                   0.0   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "19926                                                0.0   \n",
       "19927                                                0.0   \n",
       "19928                                                0.0   \n",
       "19929                                                0.0   \n",
       "19930                                                0.0   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "19926                                             0.0   \n",
       "19927                                             0.0   \n",
       "19928                                             0.0   \n",
       "19929                                             0.0   \n",
       "19930                                             0.0   \n",
       "\n",
       "                             c_id c_parentId         c_author  \\\n",
       "0      UgzH8vliQSJKHQMGZjx4AaABAg                     MrBeast   \n",
       "1      UgwDhFNTCbfck5apuUJ4AaABAg                 DoodleChaos   \n",
       "2      UgzVlS_nKI4aXISU_ep4AaABAg                 mukul_editz   \n",
       "3      UgxykcUWbPcLhlL-Gy14AaABAg                 SpamR1_2013   \n",
       "4      Ugxu5B8dQ9-mZpfW-UV4AaABAg             user-cs9zv3gh1k   \n",
       "...                           ...        ...              ...   \n",
       "19926                         0.0        0.0              0.0   \n",
       "19927                         0.0        0.0              0.0   \n",
       "19928                         0.0        0.0              0.0   \n",
       "19929                         0.0        0.0              0.0   \n",
       "19930                         0.0        0.0              0.0   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z  2021-11-24T21:02:45Z     1008391.0   \n",
       "1      2021-11-24T22:07:54Z  2021-11-24T22:07:54Z      514504.0   \n",
       "2      2023-12-30T01:55:59Z  2023-12-30T01:55:59Z         399.0   \n",
       "3      2023-11-27T00:57:21Z  2023-11-27T00:57:21Z        1680.0   \n",
       "4      2024-01-30T20:17:02Z  2024-01-30T20:17:02Z         259.0   \n",
       "...                     ...                   ...           ...   \n",
       "19926                   0.0                   0.0           0.0   \n",
       "19927                   0.0                   0.0           0.0   \n",
       "19928                   0.0                   0.0           0.0   \n",
       "19929                   0.0                   0.0           0.0   \n",
       "19930                   0.0                   0.0           0.0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                       Your videos are so interesting ‚ù§  \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "19926                                                0.0  \n",
       "19927                                                0.0  \n",
       "19928                                                0.0  \n",
       "19929                                                0.0  \n",
       "19930                                                0.0  \n",
       "\n",
       "[19931 rows x 14 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_video_comments=youtube.create_video_df_from_search(products,number_of_videos_per_product=20,number_of_comments_per_video=1000,number_of_replies_per_comment=50)\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Analysis, Selection and Labeling:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from:\n",
    "\n",
    "https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis : As emojis do not provide any helpful information they should be removed from the text strings.\n",
    "def remove_emojis(data):\n",
    "    if isinstance(data, str):\n",
    "        # Remove html tags\n",
    "        data = BeautifulSoup(data, \"html.parser\").get_text()\n",
    "        # Remove emote, etc\n",
    "        emoj = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                        \"]+\", re.UNICODE)\n",
    "        # english_words = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "        return re.sub(emoj, '', data)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row containing NA values.\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_14348\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n",
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_14348\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Length Before: 19922\n",
      "DataFrame Length After: 16962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008391.0</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514504.0</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>399.0</td>\n",
       "      <td>Your videos are so interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>259.0</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19908</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî</td>\n",
       "      <td>MmQ-Fby7xEk</td>\n",
       "      <td>Alan Chikin Chow</td>\n",
       "      <td>2021-10-26T02:00:03Z</td>\n",
       "      <td>Hey Alan Army, I'm Alan Chikin Chow!! This vid...</td>\n",
       "      <td>https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg</td>\n",
       "      <td>Ugw13tRx01nkrgCF6mJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>Sia_0847</td>\n",
       "      <td>2022-09-21T14:28:02Z</td>\n",
       "      <td>2022-09-21T14:28:02Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Descendants of sun...OST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19909</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî</td>\n",
       "      <td>MmQ-Fby7xEk</td>\n",
       "      <td>Alan Chikin Chow</td>\n",
       "      <td>2021-10-26T02:00:03Z</td>\n",
       "      <td>Hey Alan Army, I'm Alan Chikin Chow!! This vid...</td>\n",
       "      <td>https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg</td>\n",
       "      <td>UgxdIEDnltajZ4-Lct94AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-yf9ww9pm5q</td>\n",
       "      <td>2021-11-15T07:11:17Z</td>\n",
       "      <td>2021-11-15T07:11:24Z</td>\n",
       "      <td>1.0</td>\n",
       "      <td>....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19910</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî</td>\n",
       "      <td>MmQ-Fby7xEk</td>\n",
       "      <td>Alan Chikin Chow</td>\n",
       "      <td>2021-10-26T02:00:03Z</td>\n",
       "      <td>Hey Alan Army, I'm Alan Chikin Chow!! This vid...</td>\n",
       "      <td>https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg</td>\n",
       "      <td>UgwjB576xwbQVer1Z2x4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>kpoplife9310</td>\n",
       "      <td>2021-10-26T04:34:03Z</td>\n",
       "      <td>2021-10-26T04:34:03Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Blackpink might be acting in squid game season...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19915</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî</td>\n",
       "      <td>MmQ-Fby7xEk</td>\n",
       "      <td>Alan Chikin Chow</td>\n",
       "      <td>2021-10-26T02:00:03Z</td>\n",
       "      <td>Hey Alan Army, I'm Alan Chikin Chow!! This vid...</td>\n",
       "      <td>https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg</td>\n",
       "      <td>UgwL5wEpxN0bIHUrvT14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>norajanne3402</td>\n",
       "      <td>2021-11-10T12:05:02Z</td>\n",
       "      <td>2021-11-10T12:05:02Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19917</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî</td>\n",
       "      <td>MmQ-Fby7xEk</td>\n",
       "      <td>Alan Chikin Chow</td>\n",
       "      <td>2021-10-26T02:00:03Z</td>\n",
       "      <td>Hey Alan Army, I'm Alan Chikin Chow!! This vid...</td>\n",
       "      <td>https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg</td>\n",
       "      <td>Ugw5ImJRuJqAMHyfqpZ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mutiahmutiah5857</td>\n",
       "      <td>2022-02-02T07:05:41Z</td>\n",
       "      <td>2022-02-02T07:05:41Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16962 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "19908  Squid Game Korean Drama (2021)   \n",
       "19909  Squid Game Korean Drama (2021)   \n",
       "19910  Squid Game Korean Drama (2021)   \n",
       "19915  Squid Game Korean Drama (2021)   \n",
       "19917  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                       v_title    v_videoId    v_channelTitle  \\\n",
       "0            $456,000 Squid Game In Real Life!  0e3GPea1Tyg           MrBeast   \n",
       "1            $456,000 Squid Game In Real Life!  0e3GPea1Tyg           MrBeast   \n",
       "2            $456,000 Squid Game In Real Life!  0e3GPea1Tyg           MrBeast   \n",
       "3            $456,000 Squid Game In Real Life!  0e3GPea1Tyg           MrBeast   \n",
       "4            $456,000 Squid Game In Real Life!  0e3GPea1Tyg           MrBeast   \n",
       "...                                        ...          ...               ...   \n",
       "19908  IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî  MmQ-Fby7xEk  Alan Chikin Chow   \n",
       "19909  IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî  MmQ-Fby7xEk  Alan Chikin Chow   \n",
       "19910  IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî  MmQ-Fby7xEk  Alan Chikin Chow   \n",
       "19915  IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî  MmQ-Fby7xEk  Alan Chikin Chow   \n",
       "19917  IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî  MmQ-Fby7xEk  Alan Chikin Chow   \n",
       "\n",
       "              v_publishTime  \\\n",
       "0      2021-11-24T21:00:01Z   \n",
       "1      2021-11-24T21:00:01Z   \n",
       "2      2021-11-24T21:00:01Z   \n",
       "3      2021-11-24T21:00:01Z   \n",
       "4      2021-11-24T21:00:01Z   \n",
       "...                     ...   \n",
       "19908  2021-10-26T02:00:03Z   \n",
       "19909  2021-10-26T02:00:03Z   \n",
       "19910  2021-10-26T02:00:03Z   \n",
       "19915  2021-10-26T02:00:03Z   \n",
       "19917  2021-10-26T02:00:03Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "19908  Hey Alan Army, I'm Alan Chikin Chow!! This vid...   \n",
       "19909  Hey Alan Army, I'm Alan Chikin Chow!! This vid...   \n",
       "19910  Hey Alan Army, I'm Alan Chikin Chow!! This vid...   \n",
       "19915  Hey Alan Army, I'm Alan Chikin Chow!! This vid...   \n",
       "19917  Hey Alan Army, I'm Alan Chikin Chow!! This vid...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "19908  https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg   \n",
       "19909  https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg   \n",
       "19910  https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg   \n",
       "19915  https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg   \n",
       "19917  https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg   \n",
       "\n",
       "                             c_id c_parentId          c_author  \\\n",
       "0      UgzH8vliQSJKHQMGZjx4AaABAg                      MrBeast   \n",
       "1      UgwDhFNTCbfck5apuUJ4AaABAg                  DoodleChaos   \n",
       "2      UgzVlS_nKI4aXISU_ep4AaABAg                  mukul_editz   \n",
       "3      UgxykcUWbPcLhlL-Gy14AaABAg                  SpamR1_2013   \n",
       "4      Ugxu5B8dQ9-mZpfW-UV4AaABAg              user-cs9zv3gh1k   \n",
       "...                           ...        ...               ...   \n",
       "19908  Ugw13tRx01nkrgCF6mJ4AaABAg                     Sia_0847   \n",
       "19909  UgxdIEDnltajZ4-Lct94AaABAg              user-yf9ww9pm5q   \n",
       "19910  UgwjB576xwbQVer1Z2x4AaABAg                 kpoplife9310   \n",
       "19915  UgwL5wEpxN0bIHUrvT14AaABAg                norajanne3402   \n",
       "19917  Ugw5ImJRuJqAMHyfqpZ4AaABAg             mutiahmutiah5857   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z  2021-11-24T21:02:45Z     1008391.0   \n",
       "1      2021-11-24T22:07:54Z  2021-11-24T22:07:54Z      514504.0   \n",
       "2      2023-12-30T01:55:59Z  2023-12-30T01:55:59Z         399.0   \n",
       "3      2023-11-27T00:57:21Z  2023-11-27T00:57:21Z        1680.0   \n",
       "4      2024-01-30T20:17:02Z  2024-01-30T20:17:02Z         259.0   \n",
       "...                     ...                   ...           ...   \n",
       "19908  2022-09-21T14:28:02Z  2022-09-21T14:28:02Z           0.0   \n",
       "19909  2021-11-15T07:11:17Z  2021-11-15T07:11:24Z           1.0   \n",
       "19910  2021-10-26T04:34:03Z  2021-10-26T04:34:03Z           0.0   \n",
       "19915  2021-11-10T12:05:02Z  2021-11-10T12:05:02Z           0.0   \n",
       "19917  2022-02-02T07:05:41Z  2022-02-02T07:05:41Z           0.0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                        Your videos are so interesting   \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "19908                          Descendants of sun...OST   \n",
       "19909                                              ....   \n",
       "19910  Blackpink might be acting in squid game season...  \n",
       "19915                                              love   \n",
       "19917                                                Iis  \n",
       "\n",
       "[16962 rows x 14 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove emojis from the text to be analyzed\n",
    "multiple_video_comments['c_text']=multiple_video_comments['c_text'].apply(remove_emojis)\n",
    "\n",
    "df_length_before = len(multiple_video_comments)\n",
    "print(\"DataFrame Length Before:\", df_length_before)\n",
    "\n",
    "# Drop duplicates\n",
    "multiple_video_comments.drop_duplicates(inplace=True)\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)\n",
    "# Drop rows with empty or text length <= 2 comments\n",
    "multiple_video_comments = multiple_video_comments[multiple_video_comments['c_text'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "df_length_after = len(multiple_video_comments)\n",
    "print(\"DataFrame Length After:\", df_length_after)\n",
    "\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://stackoverflow.com/questions/40375366/pandas-to-csv-checking-for-overwrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        # Define keywords related to the TV show\n",
    "        self.tv_show_keywords = ['Squid Game', 'Gi-hun', 'Sang-woo', 'Player', 'Red light, green light', 'Honeycomb',\n",
    "                            'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                            'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho'\n",
    "                            'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                            'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali', ]\n",
    "        # Setting threshold value for validating the relevance of the comment\n",
    "        self.threshold = 1\n",
    "\n",
    "    # Tokenize text and remove stop words\n",
    "    def preprocess_text(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    # Matching function to check relevance of the comments\n",
    "    def match_keywords(self, tokens):\n",
    "        return [token for token in tokens if token in self.tv_show_keywords]\n",
    "\n",
    "    # Scoring function to calculate how many tokens matched\n",
    "    def calculate_score(self, tokens):\n",
    "        return len(tokens)\n",
    "\n",
    "    # Validate function to validate the relevance based on threshold\n",
    "    def validate_relevance(self, score):\n",
    "        return score >= self.threshold\n",
    "\n",
    "    def filter_comments(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        irrelevant_keywords = ['HYVE', 'crypto', 'promotion', 'ad', 'spam', 'advertisement', 'spoiler', 'leak', 'promo', 'off-topic', 'clickbait',\n",
    "                            'self-promotion', '0:', '1:', '2:', '3:', '4:', '5:', '6:', '7:',\n",
    "                            '8:', '9:', '10:', '11:', '12:', '13:', '14:', '15:']\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                if detect(row['c_text']) == 'en' and not any(keyword in row['c_text'] for keyword in irrelevant_keywords):\n",
    "                    comments.append(row)\n",
    "                    c += 1\n",
    "            except Exception as e:  # Catch any exception\n",
    "                pass\n",
    "        print(\"Number of Filtered Comments: \", c)\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])  # Create a new DataFrame from the list of rows\n",
    "        new_df = new_df.sort_values(by = ['c_like_count'], ascending = False)\n",
    "        new_df.drop_duplicates(inplace=True)\n",
    "        new_df = new_df[:1000]\n",
    "        return new_df\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        for index, row in df.iterrows():\n",
    "            processed_text = self.preprocess_text(row['c_text'])\n",
    "            matched_keywords = self.match_keywords(processed_text)\n",
    "            score = self.calculate_score(matched_keywords)\n",
    "            is_relevant = self.validate_relevance(score)\n",
    "            if is_relevant == 1:\n",
    "                comments.append(row)\n",
    "                c += 1\n",
    "\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])\n",
    "        print(\"Number of Processed Comments: \", c)\n",
    "        new_df = self.filter_comments(new_df)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Processed Comments:  1066\n",
      "Number of Filtered Comments:  761\n"
     ]
    }
   ],
   "source": [
    "p = preprocessing()\n",
    "new_df = p.preprocess(multiple_video_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14418</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game (Behind The Scenes) #Shorts</td>\n",
       "      <td>4vb085gEgPc</td>\n",
       "      <td>Behind The Scenes</td>\n",
       "      <td>2022-03-20T16:43:54Z</td>\n",
       "      <td>This video gives you a chance to look BEHIND T...</td>\n",
       "      <td>https://i.ytimg.com/vi/4vb085gEgPc/default.jpg</td>\n",
       "      <td>UgxEeZvLDwVE2jcneuJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>_Taylo_</td>\n",
       "      <td>2022-06-09T00:13:48Z</td>\n",
       "      <td>2022-06-09T00:13:48Z</td>\n",
       "      <td>10273.0</td>\n",
       "      <td>Oh so the camera-man plays squid game too?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12425</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>SQUID GAME | RED LIGHT GREEN LIGHT SCENE</td>\n",
       "      <td>sH4Y450PSVM</td>\n",
       "      <td>memebappe</td>\n",
       "      <td>2021-10-14T18:52:37Z</td>\n",
       "      <td>BUY THE PERFECT CHRISTMAS GIFT    : https://am...</td>\n",
       "      <td>https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg</td>\n",
       "      <td>UgyXDA0Vdld5b5SMG2Z4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>silver-eyedfox7713</td>\n",
       "      <td>2022-01-16T01:30:47Z</td>\n",
       "      <td>2022-01-16T01:30:47Z</td>\n",
       "      <td>8215.0</td>\n",
       "      <td>This scene is the perfect introduction to how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgyK9zFNmsuAYM-q_uF4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>Andolin414</td>\n",
       "      <td>2021-12-24T19:46:29Z</td>\n",
       "      <td>2021-12-24T19:46:29Z</td>\n",
       "      <td>4783.0</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgywgT6PG7yhO5fAi8V4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>Ertyez</td>\n",
       "      <td>2021-11-25T13:22:12Z</td>\n",
       "      <td>2021-11-25T13:22:12Z</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>No words how incredible the set was! Good game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game (acapella)</td>\n",
       "      <td>RtPb4NMkT-s</td>\n",
       "      <td>MayTree</td>\n",
       "      <td>2021-10-01T12:00:13Z</td>\n",
       "      <td>Would you follow Maytree? :) www.instagram.com...</td>\n",
       "      <td>https://i.ytimg.com/vi/RtPb4NMkT-s/default.jpg</td>\n",
       "      <td>UgyG04Habjoo46OqVvt4AaABAg.9T8Y97-UvCI9T8nxit88u5</td>\n",
       "      <td>UgyG04Habjoo46OqVvt4AaABAg</td>\n",
       "      <td>hanzhrlyml</td>\n",
       "      <td>2021-10-06T06:07:30Z</td>\n",
       "      <td>2021-10-06T06:07:30Z</td>\n",
       "      <td>3371.0</td>\n",
       "      <td>Squid game have season 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10741</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]</td>\n",
       "      <td>iRBQGFN-PEY</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ</td>\n",
       "      <td>2021-09-26T07:31:20Z</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...</td>\n",
       "      <td>https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg</td>\n",
       "      <td>UgwRYYvaLRcmIxhOwjJ4AaABAg.9TBQL8cjxa_9TCFbgt2Ci3</td>\n",
       "      <td>UgwRYYvaLRcmIxhOwjJ4AaABAg</td>\n",
       "      <td>justsomeladywithamustache8909</td>\n",
       "      <td>2021-10-07T14:15:37Z</td>\n",
       "      <td>2021-10-07T14:15:37Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I watched squid game in one day,and then I wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10739</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]</td>\n",
       "      <td>iRBQGFN-PEY</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ</td>\n",
       "      <td>2021-09-26T07:31:20Z</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...</td>\n",
       "      <td>https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg</td>\n",
       "      <td>UgwRYYvaLRcmIxhOwjJ4AaABAg.9TBQL8cjxa_9TBu-bKlBhI</td>\n",
       "      <td>UgwRYYvaLRcmIxhOwjJ4AaABAg</td>\n",
       "      <td>powerfulgamer4400</td>\n",
       "      <td>2021-10-07T10:58:03Z</td>\n",
       "      <td>2021-10-07T10:58:03Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>You could just watch how to survive the squid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10687</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]</td>\n",
       "      <td>iRBQGFN-PEY</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ</td>\n",
       "      <td>2021-09-26T07:31:20Z</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...</td>\n",
       "      <td>https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg</td>\n",
       "      <td>UgymXJadovg28ock8pB4AaABAg.9T5eH23g-X39TGm183tcw5</td>\n",
       "      <td>UgymXJadovg28ock8pB4AaABAg</td>\n",
       "      <td>andaghana1328</td>\n",
       "      <td>2021-10-09T08:24:33Z</td>\n",
       "      <td>2021-10-09T08:24:33Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tell me how to death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10563</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]</td>\n",
       "      <td>iRBQGFN-PEY</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ</td>\n",
       "      <td>2021-09-26T07:31:20Z</td>\n",
       "      <td>ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...</td>\n",
       "      <td>https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg</td>\n",
       "      <td>UgzcaycmyE2iOflmrLh4AaABAg.9T-uAguE-Ce9T0giTQ10bE</td>\n",
       "      <td>UgzcaycmyE2iOflmrLh4AaABAg</td>\n",
       "      <td>cleverclovee</td>\n",
       "      <td>2021-10-03T02:30:20Z</td>\n",
       "      <td>2021-10-03T02:30:20Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Same- I was dying at the old mans death only t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19902</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî</td>\n",
       "      <td>MmQ-Fby7xEk</td>\n",
       "      <td>Alan Chikin Chow</td>\n",
       "      <td>2021-10-26T02:00:03Z</td>\n",
       "      <td>Hey Alan Army, I'm Alan Chikin Chow!! This vid...</td>\n",
       "      <td>https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg</td>\n",
       "      <td>Ugz_H4HGfabQDRqZi314AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>ateeqsid</td>\n",
       "      <td>2021-11-09T11:42:20Z</td>\n",
       "      <td>2021-11-09T11:42:20Z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Squid game is a k-drama but not romantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "14418  Squid Game Korean Drama (2021)   \n",
       "12425  Squid Game Korean Drama (2021)   \n",
       "836    Squid Game Korean Drama (2021)   \n",
       "805    Squid Game Korean Drama (2021)   \n",
       "3153   Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "10741  Squid Game Korean Drama (2021)   \n",
       "10739  Squid Game Korean Drama (2021)   \n",
       "10687  Squid Game Korean Drama (2021)   \n",
       "10563  Squid Game Korean Drama (2021)   \n",
       "19902  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                           v_title    v_videoId  \\\n",
       "14418       Squid Game (Behind The Scenes) #Shorts  4vb085gEgPc   \n",
       "12425     SQUID GAME | RED LIGHT GREEN LIGHT SCENE  sH4Y450PSVM   \n",
       "836              $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "805              $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3153                         Squid Game (acapella)  RtPb4NMkT-s   \n",
       "...                                            ...          ...   \n",
       "10741  Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]  iRBQGFN-PEY   \n",
       "10739  Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]  iRBQGFN-PEY   \n",
       "10687  Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]  iRBQGFN-PEY   \n",
       "10563  Ïò§ÏßïÏñ¥ Í≤åÏûÑ 3Î∂Ñ ÏöîÏïΩ [Squid Game 3 minutes summary]  iRBQGFN-PEY   \n",
       "19902      IF SQUID GAME WAS A ROMANTIC K-DRAMA‚Ä¶ üíî  MmQ-Fby7xEk   \n",
       "\n",
       "          v_channelTitle         v_publishTime  \\\n",
       "14418  Behind The Scenes  2022-03-20T16:43:54Z   \n",
       "12425         memebappe   2021-10-14T18:52:37Z   \n",
       "836              MrBeast  2021-11-24T21:00:01Z   \n",
       "805              MrBeast  2021-11-24T21:00:01Z   \n",
       "3153             MayTree  2021-10-01T12:00:13Z   \n",
       "...                  ...                   ...   \n",
       "10741               ÏÜåÎß•Í±∞ÌïÄ  2021-09-26T07:31:20Z   \n",
       "10739               ÏÜåÎß•Í±∞ÌïÄ  2021-09-26T07:31:20Z   \n",
       "10687               ÏÜåÎß•Í±∞ÌïÄ  2021-09-26T07:31:20Z   \n",
       "10563               ÏÜåÎß•Í±∞ÌïÄ  2021-09-26T07:31:20Z   \n",
       "19902   Alan Chikin Chow  2021-10-26T02:00:03Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "14418  This video gives you a chance to look BEHIND T...   \n",
       "12425  BUY THE PERFECT CHRISTMAS GIFT    : https://am...   \n",
       "836    MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "805    MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3153   Would you follow Maytree? :) www.instagram.com...   \n",
       "...                                                  ...   \n",
       "10741  ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...   \n",
       "10739  ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...   \n",
       "10687  ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...   \n",
       "10563  ÏÜåÎß•Í±∞ÌïÄ #Ïò§ÏßïÏñ¥Í≤åÏûÑ #Ïï†ÎãàÎ©îÏù¥ÏÖò #SquidGame ÏòÅÏÉÅ ÎÇ¥ Ïä§Ìè¨ÏùºÎü¨Î•º Ï£ºÏùòÌï¥Ï£ºÏÑ∏...   \n",
       "19902  Hey Alan Army, I'm Alan Chikin Chow!! This vid...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "14418  https://i.ytimg.com/vi/4vb085gEgPc/default.jpg   \n",
       "12425  https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg   \n",
       "836    https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "805    https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3153   https://i.ytimg.com/vi/RtPb4NMkT-s/default.jpg   \n",
       "...                                               ...   \n",
       "10741  https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg   \n",
       "10739  https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg   \n",
       "10687  https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg   \n",
       "10563  https://i.ytimg.com/vi/iRBQGFN-PEY/default.jpg   \n",
       "19902  https://i.ytimg.com/vi/MmQ-Fby7xEk/default.jpg   \n",
       "\n",
       "                                                    c_id  \\\n",
       "14418                         UgxEeZvLDwVE2jcneuJ4AaABAg   \n",
       "12425                         UgyXDA0Vdld5b5SMG2Z4AaABAg   \n",
       "836                           UgyK9zFNmsuAYM-q_uF4AaABAg   \n",
       "805                           UgywgT6PG7yhO5fAi8V4AaABAg   \n",
       "3153   UgyG04Habjoo46OqVvt4AaABAg.9T8Y97-UvCI9T8nxit88u5   \n",
       "...                                                  ...   \n",
       "10741  UgwRYYvaLRcmIxhOwjJ4AaABAg.9TBQL8cjxa_9TCFbgt2Ci3   \n",
       "10739  UgwRYYvaLRcmIxhOwjJ4AaABAg.9TBQL8cjxa_9TBu-bKlBhI   \n",
       "10687  UgymXJadovg28ock8pB4AaABAg.9T5eH23g-X39TGm183tcw5   \n",
       "10563  UgzcaycmyE2iOflmrLh4AaABAg.9T-uAguE-Ce9T0giTQ10bE   \n",
       "19902                         Ugz_H4HGfabQDRqZi314AaABAg   \n",
       "\n",
       "                       c_parentId                       c_author  \\\n",
       "14418                                                    _Taylo_   \n",
       "12425                                         silver-eyedfox7713   \n",
       "836                                                   Andolin414   \n",
       "805                                                       Ertyez   \n",
       "3153   UgyG04Habjoo46OqVvt4AaABAg                     hanzhrlyml   \n",
       "...                           ...                            ...   \n",
       "10741  UgwRYYvaLRcmIxhOwjJ4AaABAg  justsomeladywithamustache8909   \n",
       "10739  UgwRYYvaLRcmIxhOwjJ4AaABAg              powerfulgamer4400   \n",
       "10687  UgymXJadovg28ock8pB4AaABAg                  andaghana1328   \n",
       "10563  UgzcaycmyE2iOflmrLh4AaABAg                   cleverclovee   \n",
       "19902                                                   ateeqsid   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "14418  2022-06-09T00:13:48Z  2022-06-09T00:13:48Z       10273.0   \n",
       "12425  2022-01-16T01:30:47Z  2022-01-16T01:30:47Z        8215.0   \n",
       "836    2021-12-24T19:46:29Z  2021-12-24T19:46:29Z        4783.0   \n",
       "805    2021-11-25T13:22:12Z  2021-11-25T13:22:12Z        4405.0   \n",
       "3153   2021-10-06T06:07:30Z  2021-10-06T06:07:30Z        3371.0   \n",
       "...                     ...                   ...           ...   \n",
       "10741  2021-10-07T14:15:37Z  2021-10-07T14:15:37Z           0.0   \n",
       "10739  2021-10-07T10:58:03Z  2021-10-07T10:58:03Z           0.0   \n",
       "10687  2021-10-09T08:24:33Z  2021-10-09T08:24:33Z           0.0   \n",
       "10563  2021-10-03T02:30:20Z  2021-10-03T02:30:20Z           0.0   \n",
       "19902  2021-11-09T11:42:20Z  2021-11-09T11:42:20Z           0.0   \n",
       "\n",
       "                                                  c_text  \n",
       "14418         Oh so the camera-man plays squid game too?  \n",
       "12425  This scene is the perfect introduction to how ...  \n",
       "836    This version of the game is pretty much what t...  \n",
       "805    No words how incredible the set was! Good game...  \n",
       "3153                            Squid game have season 2  \n",
       "...                                                  ...  \n",
       "10741  I watched squid game in one day,and then I wat...  \n",
       "10739  You could just watch how to survive the squid ...  \n",
       "10687                               Tell me how to death  \n",
       "10563  Same- I was dying at the old mans death only t...  \n",
       "19902           Squid game is a k-drama but not romantic  \n",
       "\n",
       "[761 rows x 14 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling comments using Sentiment Lexicon - VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(c_text):\n",
    "    sentiment_Score = sentiment_lexicon.polarity_scores(c_text)\n",
    "    return sentiment_Score['compound']\n",
    "\n",
    "def check_sentiment(sentiment_score):\n",
    "    if sentiment_score > 0.00:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0.00:\n",
    "        return \"Negative\"\n",
    "    elif sentiment_score == 0:\n",
    "        return \"Neutral \"\n",
    "\n",
    "new_df['sentiment_score'] = new_df['c_text'].apply(get_sentiment_score)\n",
    "new_df['sentiment'] = new_df['sentiment_score'].apply(check_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Comments for each polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0258  Negative           This scene is the perfect introduction to how the games are a matter of life and death. The slow build up, the sudden reveal that the dude was really shot, and the ensuing chaos make this the perfect introduction to these games.\n",
      "    -0.0258  Negative           Game of Death- Bruce Lee would win\n",
      "    -0.0488  Negative           So basically hunger games but different, some faction or person becomes to powerful and makes the peasants play death games. Seems legit.\n",
      "    -0.0516  Negative           250 was second to die and we all know anyone loves squid game I meant all episodes\n",
      "    -0.0516  Negative           Idk but if blackpink were gonna be in squid game the i  will be so sipcy\n",
      "    -0.0516  Negative           I swear everyone has forgot about Squid game\n",
      "    -0.0516  Negative           Good thing the game isn,t unfair. Imagine getting shot on green light\n",
      "    -0.1027  Negative           This is hard game\n",
      "    -0.1027  Negative           @@sangdu957 idk there walking to the frist game so the music just poped up\n",
      "    -0.1027  Negative           nah this isnt squid game its cephalopod conundrums\n",
      "     0       Neutral            Squid game have season 2\n",
      "     0       Neutral            How did they make blood come out in the first game?\n",
      "     0       Neutral            I was in squid game\n",
      "     0       Neutral            Square Game season 3 TV show\n",
      "     0       Neutral            Me: imma gonna watch squid game\n",
      "                                Doll: *comes out the TV* hello there\n",
      "                                Me: *screams so loud*\n",
      "     0       Neutral            The robot squid game really shoots, doesn't it?\n",
      "     0       Neutral            so the blood in squid game is ketchup?!?!\n",
      "     0       Neutral            So we can go to the squid game and is the blood real\n",
      "     0       Neutral            Squid  game took 10 years to make\n",
      "     0       Neutral            You are telling me squid game isn‚Äôt real?!?!?!\n",
      "     0.985   Positive           Really soft and wonderful music. I fell in love with the Squid Game series and this special music .. I am from Iran. Travel to Iran. Iranians are very kind and hospitable. I work to show Iran to the world on YouTube. If you are interested in seeing Iran 2021 virtual walking tours, I will be happy to watch my channel videos. My content is of great quality. Thanks for the support.\n",
      "     0.9842  Positive           WOW! THAT'S SO EPIC I ENJOYED SO AMAZING THAT'S THE AMAZING SQUID GAME VIDEO OMG I LOVE IT AND THE END IS SO AWESOME AND AMAZING\n",
      "     0.9721  Positive           My cousin has a Roblox and I played squid game it‚Äôs very true I swear it‚Äôs very true it‚Äôs very true OK OK OK OK\n",
      "     0.9721  Positive           Hahaahhahahaa it was  so  funny  to watch  this video and I love this video so much and i will  give it a thumbs up I love you Kaycee and Rachel l hope you enjoyed  this game you are the only girl s that mekes me happy\n",
      "     0.9709  Positive           i know squid game is disliked for its extreme oversaturation and memes but imo its just what happened to among us, amazing content but for a period of time it's all you see and you get burnt out. I watched squiggame in the middle of it's decline in popularity and i must say the show is *really* good. The music that comes in as the guards arrive in the beginning of the game is so strange to hear, it almost seems like a different song because of how well it sets the mood, the whole time i was captivated, so i'm excited for season 2\n",
      "     0.9638  Positive           Tbf most probably ran for a second, but were smart enough to realize that if the budget for the game set they where in was that expensive, then they would totally be able to easily pick off evreybody who runs, and the reason they weren't caught is because the dolls vision was literally zooming around the playing field and goig  left right and center, and since the dols visik was freaking out, alot of people probably got away with moving, you can even see there are quite a few that are facing backwards whilst the instructions are being repeated, so yeah many started to run but didnt when they saw everyone who was running get shot and also realised its red light so best action is to not move\n",
      "     0.9611  Positive           Hi Kaycee, the squid game was really fun, good to see you with your family and friends, thanks a lot for bringing smile in my life, lots of love from kshirja kharvi\n",
      "     0.9595  Positive           Actor combination award. I'm proud of you.Thank you for the video. Squid game actor. His acting skills are the best. Then, what is the best gangster movie in Korea?If you're curious... Come. I'll enjoy the video\n",
      "     0.9555  Positive           No but 102 who sacrificed himself for his best friend to continue the game is so kind\n",
      "     0.9546  Positive           Yeah Kaycee we enjoyed the squid game,it was so funny and wonderful, lots of love from kshirja kharvi\n"
     ]
    }
   ],
   "source": [
    "def select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('sentiment')\n",
    "\n",
    "    # iterate over each polarity group\n",
    "    for sentiment, group in grouped:\n",
    "        # sort comments by sentiment score pick top 10\n",
    "        top_group_comments = group.sort_values(by='sentiment_score', ascending=False).head(top_n)[['sentiment_score', 'sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "# # top 10 comments for each polarity\n",
    "# for sentiment_score, sentiment, comment in top_comments:\n",
    "#     print(f\"Sentiment: {sentiment}, Sentiment Score: {sentiment_score}, Comment: {comment}\")\n",
    "\n",
    "# making it pretty~~~\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(top_comments, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Lexicon using TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0258  Negative           This scene is the perfect introduction to how the games are a matter of life and death. The slow build up, the sudden reveal that the dude was really shot, and the ensuing chaos make this the perfect introduction to these games.\n",
      "    -0.0258  Negative           Game of Death- Bruce Lee would win\n",
      "    -0.0488  Negative           So basically hunger games but different, some faction or person becomes to powerful and makes the peasants play death games. Seems legit.\n",
      "    -0.0516  Negative           250 was second to die and we all know anyone loves squid game I meant all episodes\n",
      "    -0.0516  Negative           Idk but if blackpink were gonna be in squid game the i  will be so sipcy\n",
      "    -0.0516  Negative           I swear everyone has forgot about Squid game\n",
      "    -0.0516  Negative           Good thing the game isn,t unfair. Imagine getting shot on green light\n",
      "    -0.1027  Negative           This is hard game\n",
      "    -0.1027  Negative           @@sangdu957 idk there walking to the frist game so the music just poped up\n",
      "    -0.1027  Negative           nah this isnt squid game its cephalopod conundrums\n",
      "     0       Neutral            Squid game have season 2\n",
      "     0       Neutral            How did they make blood come out in the first game?\n",
      "     0       Neutral            I was in squid game\n",
      "     0       Neutral            Square Game season 3 TV show\n",
      "     0       Neutral            Me: imma gonna watch squid game\n",
      "                                Doll: *comes out the TV* hello there\n",
      "                                Me: *screams so loud*\n",
      "     0       Neutral            The robot squid game really shoots, doesn't it?\n",
      "     0       Neutral            so the blood in squid game is ketchup?!?!\n",
      "     0       Neutral            So we can go to the squid game and is the blood real\n",
      "     0       Neutral            Squid  game took 10 years to make\n",
      "     0       Neutral            You are telling me squid game isn‚Äôt real?!?!?!\n",
      "     0.985   Positive           Really soft and wonderful music. I fell in love with the Squid Game series and this special music .. I am from Iran. Travel to Iran. Iranians are very kind and hospitable. I work to show Iran to the world on YouTube. If you are interested in seeing Iran 2021 virtual walking tours, I will be happy to watch my channel videos. My content is of great quality. Thanks for the support.\n",
      "     0.9842  Positive           WOW! THAT'S SO EPIC I ENJOYED SO AMAZING THAT'S THE AMAZING SQUID GAME VIDEO OMG I LOVE IT AND THE END IS SO AWESOME AND AMAZING\n",
      "     0.9721  Positive           My cousin has a Roblox and I played squid game it‚Äôs very true I swear it‚Äôs very true it‚Äôs very true OK OK OK OK\n",
      "     0.9721  Positive           Hahaahhahahaa it was  so  funny  to watch  this video and I love this video so much and i will  give it a thumbs up I love you Kaycee and Rachel l hope you enjoyed  this game you are the only girl s that mekes me happy\n",
      "     0.9709  Positive           i know squid game is disliked for its extreme oversaturation and memes but imo its just what happened to among us, amazing content but for a period of time it's all you see and you get burnt out. I watched squiggame in the middle of it's decline in popularity and i must say the show is *really* good. The music that comes in as the guards arrive in the beginning of the game is so strange to hear, it almost seems like a different song because of how well it sets the mood, the whole time i was captivated, so i'm excited for season 2\n",
      "     0.9638  Positive           Tbf most probably ran for a second, but were smart enough to realize that if the budget for the game set they where in was that expensive, then they would totally be able to easily pick off evreybody who runs, and the reason they weren't caught is because the dolls vision was literally zooming around the playing field and goig  left right and center, and since the dols visik was freaking out, alot of people probably got away with moving, you can even see there are quite a few that are facing backwards whilst the instructions are being repeated, so yeah many started to run but didnt when they saw everyone who was running get shot and also realised its red light so best action is to not move\n",
      "     0.9611  Positive           Hi Kaycee, the squid game was really fun, good to see you with your family and friends, thanks a lot for bringing smile in my life, lots of love from kshirja kharvi\n",
      "     0.9595  Positive           Actor combination award. I'm proud of you.Thank you for the video. Squid game actor. His acting skills are the best. Then, what is the best gangster movie in Korea?If you're curious... Come. I'll enjoy the video\n",
      "     0.9555  Positive           No but 102 who sacrificed himself for his best friend to continue the game is so kind\n",
      "     0.9546  Positive           Yeah Kaycee we enjoyed the squid game,it was so funny and wonderful, lots of love from kshirja kharvi\n"
     ]
    }
   ],
   "source": [
    "def text_blob_sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def texblob_check_sentiment(score):\n",
    "    if score == 0:\n",
    "        return 'Neutral'\n",
    "    elif score < 0.00:\n",
    "        return 'Negative'\n",
    "    elif score > 0.00:\n",
    "        return 'Positive'\n",
    "\n",
    "new_df['textblob_score'] = new_df['c_text'].apply(text_blob_sentiment_score)\n",
    "new_df['textblob_sentiment'] = new_df['textblob_score'].apply(texblob_check_sentiment)\n",
    "\n",
    "def textblob_select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('textblob_sentiment')\n",
    "\n",
    "    for sentiment, group in grouped:\n",
    "        top_group_comments = group.sort_values(by='textblob_score', ascending=False).head(top_n)[['textblob_score', 'textblob_sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "textblob_top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(textblob_top_comments, headers=headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14418    None\n",
       "12425    None\n",
       "836      None\n",
       "805      None\n",
       "3153     None\n",
       "         ... \n",
       "10741    None\n",
       "10739    None\n",
       "10687    None\n",
       "10563    None\n",
       "19902    None\n",
       "Length: 761, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['positive', 'negative', 'neutral']\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def col_to_txt(row):\n",
    "    sentiment = row['sentiment']  \n",
    "    c_text = row['c_text']\n",
    "    file_name = f\"{sentiment}_{row.c_id}.txt\"  \n",
    "    folder = f\"{sentiment.strip()}\"  \n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(c_text)\n",
    "\n",
    "new_df.apply(col_to_txt, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'final_comments_df.csv'\n",
    "# files_present = glob.glob(filename)\n",
    "# # will only write to disk if file doesnt exist\n",
    "# if not files_present:\n",
    "#     new_df.to_csv(filename, index=False)\n",
    "#     new_df\n",
    "# else:\n",
    "#     print (f'File Already Exists. Delete {filename}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the root directory containing the positive, negative, and neutral folders\n",
    "root_dir = ''\n",
    "\n",
    "# Define the directories for train and test sets\n",
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "try:\n",
    "    shutil.rmtree(os.path.join(root_dir, train_dir))\n",
    "    shutil.rmtree(os.path.join(root_dir, test_dir))\n",
    "except:\n",
    "    pass\n",
    "# Define the ratio for train-test split\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Iterate through each sentiment folder\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    # Get the list of file paths in the current sentiment folder\n",
    "    files = os.listdir(os.path.join(root_dir, sentiment))\n",
    "    # Shuffle the file paths\n",
    "    random.shuffle(files)\n",
    "    # Calculate the split index based on the split ratio\n",
    "    split_index = int(len(files) * split_ratio)\n",
    "    # Split the files into train and test sets\n",
    "    train_files = files[:split_index]\n",
    "    test_files = files[split_index:]\n",
    "    \n",
    "    # Move train files to train directory\n",
    "    for file in train_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(train_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "    # Move test files to test directory\n",
    "    for file in test_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(test_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Remove the sentiment folders\n",
    "try:\n",
    "    shutil.rmtree('negative')\n",
    "    shutil.rmtree('neutral')\n",
    "    shutil.rmtree('positive')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Text Analytics Pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(train_dir)\n",
    "reviews_test = load_files(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@@sqngry everyone fears death' 0\n",
      "b'Its nice but i like mrbeasts squid game more' 2\n",
      "b'This dude summed up 10 hours worth of squid game into 3 minutes? Wow. And it was an absolute masterpiece.\\r\\n\\r\\nHoly shit yall thanks fo the likes' 2\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(text_train[1],y_train[1])\n",
    "print(text_train[2],y_train[2])\n",
    "print(text_train[3],y_train[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect = CountVectorizer().fit(reviews_train)\n",
    "X_train = vect.transform(reviews_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_tokens = ['drama', 'film', 'cinema', 'actor', 'actress', 'director', 'plot',\n",
    "                         'scene', 'genre', 'subtitles', 'k-drama', 'kdrama', 'k-movie', 'television',\n",
    "                         'episode', 'screenplay', 'script', 'cinematography', 'soundtrack',\n",
    "                         'OST', 'character', 'plot twist', 'review', 'ratings', 'premiere',\n",
    "                         'streaming', 'watchlist', 'subbed', 'dubbed', 'sequel', 'game', 'song',\n",
    "                         'season', 'trailer', 'casting', 'fanbase', 'recommendation', 'goblin',\n",
    "                         'viewer', 'critic', 'Korean', 'entertainment', 'watched', 'guardian',\n",
    "                         'show', 'squid', 'watch', 'watching', 'acting', 'netflix', 'show',\n",
    "                         'end',\n",
    "                          'squid game', 'gi-hun', 'Sang-woo', 'Player', 'Red light', 'green light', 'Honeycomb',\n",
    "                            'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                            'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho'\n",
    "                            'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                            'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    return text_without_punctuation\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # stopwords punctuation etc\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    # split into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    # removes stopwords and numbers and stems from tokens makes sure its all lowercase too\n",
    "    tokens = [stemmer.stem(remove_punctuation(token)) for token in tokens if token.isalnum() and token.lower() not in product_tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.65      0.42        17\n",
      "           1       0.45      0.69      0.55        26\n",
      "           2       0.88      0.63      0.73       110\n",
      "\n",
      "    accuracy                           0.64       153\n",
      "   macro avg       0.55      0.66      0.57       153\n",
      "weighted avg       0.75      0.64      0.67       153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('preprocess', \n",
    "    TfidfVectorizer(\n",
    "                    encoding=\"utf-8\",\n",
    "                    strip_accents='ascii',\n",
    "                    lowercase=True,\n",
    "                    preprocessor=preprocess_text,\n",
    "                    # tokenizer=,\n",
    "                    # analyzer=,\n",
    "                    stop_words='english',\n",
    "                    norm='l2',\n",
    "                    ngram_range=(1, 1),\n",
    "                    max_df=0.09,\n",
    "                    min_df=0.003,\n",
    "                    max_features=500,\n",
    "                    binary=True,\n",
    "                    use_idf=True,\n",
    "                    smooth_idf=True,\n",
    "                    sublinear_tf=True\n",
    "                    )\n",
    "    # CountVectorizer(preprocessor=preprocess_text,ngram_range=(1, 1))\n",
    "     ), \n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "text_clf.fit(text_train, y_train)\n",
    "y_pred = text_clf.predict(text_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Run only till here and check coz the grid search would take long so better to adjust by looking at this only ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 31\u001b[0m\n\u001b[0;32m      6\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m: [TfidfVectorizer()],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# 'vectorizer__sublinear_tf': [True,False]\u001b[39;00m\n\u001b[0;32m     28\u001b[0m }\n\u001b[0;32m     30\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(text_clf, parameters, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__norm': ['l2','l1'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'vectorizer__use_idf': [True,False],\n",
    "    'vectorizer__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 889: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 925: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1033: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1069: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 893: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 929: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1037: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1073: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 897: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 933: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "results = grid_search.cv_results_\n",
    " \n",
    "scores = results['mean_test_score']\n",
    "\n",
    "params = results['params']\n",
    "\n",
    "top_models_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores[i], params[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94        79\n",
      "           1       0.96      0.95      0.95        74\n",
      "           2       0.97      1.00      0.98       217\n",
      "\n",
      "    accuracy                           0.97       370\n",
      "   macro avg       0.97      0.95      0.96       370\n",
      "weighted avg       0.97      0.97      0.97       370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_ct = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "parameters_ct = {\n",
    "    'vectorizer': [CountVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    'tfidf__norm': ['l2','l1'],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'tfidf__use_idf': [True,False],\n",
    "    'tfidf__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_ct = GridSearchCV(pipeline_ct, parameters_ct, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_ct.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_ct.best_params_)\n",
    "best_model_ct = grid_search_ct.best_estimator_\n",
    "y_pred_ct = best_model_ct.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 655: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 664: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 691: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 700: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 656: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 665: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 692: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 701: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 657: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 666: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "results_ct = grid_search_ct.cv_results_\n",
    " \n",
    "scores_ct = results_ct['mean_test_score']\n",
    "\n",
    "params_ct = results_ct['params']\n",
    "\n",
    "top_models_indices_ct = sorted(range(len(scores_ct)), key=lambda i: scores_ct[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices_ct:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores_ct[i], params_ct[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualization and Insights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Discussion and conclusion from experiments:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
