{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farheenfab/AppliedText_CW/blob/main/CW1-generate_dataset.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F20AA Coursework 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk \n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from langdetect import detect\n",
    "import shutil\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the api service name, version and developer key for the api call.\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyAWj_uzrhZL18X32S_P79pT1wnSYGpuA4k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/search/list#parameters\n",
    "\n",
    "https://developers.google.com/youtube/v3/docs/comments/list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a class called api_handler which contains functions such as `get_video_details()`, `get_videos()`, `get_video_df()`, `get_comments()`, `get_comment_replies()`, `get_comments_df()`, `create_video_df_from_search()`, `create_video_df()`. These functions help us by either manually retrieving the videos and comments or by automatically curating the videos and comments using the product given to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class api_handler:\n",
    "    def __init__(self, api_service_name, api_version, developer_key):\n",
    "        self.client = googleapiclient.discovery.build(api_service_name,\n",
    "                                                    api_version,\n",
    "                                                    developerKey=developer_key)\n",
    "        \n",
    "    # Search for videos details given id\n",
    "    def get_video_details(self, videoId, part=\"snippet\"):\n",
    "        request = self.client.videos().list(\n",
    "            part=part,\n",
    "            id=videoId\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        if 'items' in response:\n",
    "            video_details = response['items'][0]\n",
    "            snippet=video_details['snippet']\n",
    "            snippet['videoId']=videoId\n",
    "            snippet['id']=videoId\n",
    "            snippet['publishTime']=video_details.get('snippet', {}).get('publishedAt', {})\n",
    "            snippet['thumbnails']=video_details.get('snippet', {}).get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "            return snippet\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Search for videos given query\n",
    "    def get_videos(self,query,maxResults=5,part=\"snippet\"):\n",
    "        request = self.client.search().list(\n",
    "            part=part,\n",
    "            maxResults=maxResults,\n",
    "            # higher view count is likely to be more relevent \n",
    "            order=\"viewCount\",\n",
    "            q=query,  \n",
    "            # american region videos \n",
    "            regionCode=\"US\",\n",
    "            # english videos\n",
    "            relevanceLanguage=\"en\",\n",
    "            type=\"video\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        return response\n",
    "    \n",
    "    # Format Response from get_videos to dataframe\n",
    "    def get_video_df(response):\n",
    "        items=[]\n",
    "        for item in response['items']:\n",
    "            snippet=item.get('snippet', {})\n",
    "            items+=[{\n",
    "                'title':snippet.get('title', ''),\n",
    "                'videoId':item.get('id', {}).get('videoId', ''),\n",
    "                'channelTitle':snippet.get('channelTitle', ''),\n",
    "                'publishTime':snippet.get('publishTime', ''),\n",
    "                'description':snippet.get('description', ''),\n",
    "                'thumbnails':snippet.get('thumbnails', {}).get('default', {}).get('url', '')\n",
    "                }]\n",
    "        df=pd.DataFrame(items)\n",
    "        return df\n",
    "    \n",
    "    # Get comments from video\n",
    "    def get_comments(self,videoId,part=\"snippet\",maxResults=100,maxResultsDepth=100):\n",
    "        all_comments = []\n",
    "        f = 0\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0:\n",
    "            request = self.client.commentThreads().list(\n",
    "                part=part,\n",
    "                videoId=videoId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                order='relevance',\n",
    "                moderationStatus='published',\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if 'items' in response:\n",
    "                all_comments+=[response]\n",
    "                for item in response['items']:\n",
    "                    # extract the comment ID to get replies\n",
    "                    comment_id = item.get('snippet',{}).get('topLevelComment',{}).get('id','')\n",
    "                    if item.get('snippet',{}).get('totalReplyCount',0)>2:\n",
    "                        if f == 0:\n",
    "                            print('getting replies:',item.get('snippet',{}).get('totalReplyCount',0))\n",
    "                            f = 1\n",
    "                        replies = self.get_comment_replies(comment_id, maxResults=maxResultsDepth)\n",
    "                        all_comments += replies\n",
    "\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "    \n",
    "    # Get replies from comment \n",
    "    def get_comment_replies(self, commentId, part=\"snippet\", maxResults=100):\n",
    "        all_comments = []\n",
    "        nextPageToken = None\n",
    "        while maxResults > 0 and (nextPageToken != None or len(all_comments)==0):\n",
    "\n",
    "            request = self.client.comments().list(\n",
    "                part=part,\n",
    "                parentId=commentId,\n",
    "                maxResults=min(maxResults, 100),\n",
    "                textFormat='plainText',\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "\n",
    "            response = request.execute()\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "            if 'items' in response and len(response['items'])>0:\n",
    "                for item in response['items']:\n",
    "                    modified_response = {\n",
    "                        'items': [\n",
    "                            {\n",
    "                                'id':item.get('id'),\n",
    "                                'snippet': {\n",
    "                                    'topLevelComment': {\n",
    "                                        'snippet': item.get('snippet','')\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    all_comments += [modified_response]\n",
    "            maxResults -= min(maxResults, 100)\n",
    "            if nextPageToken is None:\n",
    "                break;    \n",
    "        return all_comments\n",
    "\n",
    "    # Format response from get_comments to dataframe\n",
    "    def get_comments_df(response, video,product):\n",
    "        comments = []\n",
    "        for pages in response:\n",
    "            for item in pages['items']:\n",
    "                comment = item.get('snippet', {}).get('topLevelComment', {}).get('snippet', {})\n",
    "                comments.append([\n",
    "                        product,\n",
    "                        video.get('title', ''),\n",
    "                        video.get('videoId', ''),\n",
    "                        video.get('channelTitle', ''),\n",
    "                        video.get('publishTime', ''),\n",
    "                        video.get('description', ''),\n",
    "                        video.get('thumbnails', ''),\n",
    "                        item.get('id', ''),  \n",
    "                        comment.get('parentId', ''),  \n",
    "                        comment.get('authorDisplayName', '')[1:],  \n",
    "                        comment.get('publishedAt', ''),\n",
    "                        comment.get('updatedAt', ''),\n",
    "                        comment.get('likeCount', ''),\n",
    "                        comment.get('textDisplay', '')\n",
    "                    ])\n",
    "\n",
    "        df = pd.DataFrame(comments,\n",
    "            columns=['product', 'v_title', 'v_videoId',\n",
    "                    'v_channelTitle', 'v_publishTime',\n",
    "                    'v_description', 'v_thumbnail',\n",
    "                    'c_id','c_parentId',\n",
    "                    'c_author', 'c_published_at',\n",
    "                    'c_updated_at', 'c_like_count',\n",
    "                    'c_text'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Search for videos related to products iteratively\n",
    "    # Collect comments from each video and place it into an array\n",
    "    def create_video_df_from_search(self, products,\n",
    "                                    number_of_videos_per_product=5,\n",
    "                                    number_of_comments_per_video=100\n",
    "                                    ,number_of_replies_per_comment=100):\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            # get 25 first videos with the highest viewer counts \n",
    "            response = self.get_videos(query=product, maxResults=number_of_videos_per_product)\n",
    "            # Convert results to df\n",
    "            videos_df = api_handler.get_video_df(response)\n",
    "            # For each video get a maximum of 100 comments\n",
    "            # and place comments into an array\n",
    "            for _, video in videos_df.iterrows():\n",
    "                try:\n",
    "                    response = self.get_comments(video['videoId'], maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment)\n",
    "                    comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                except:\n",
    "                    # Function fails as the API returns 403 if the channel has comments disabled\n",
    "                    # place an empty entry instead it can be deleted later\n",
    "                    comments_df = pd.DataFrame(np.zeros((1, 14)),\n",
    "                                                columns=['product', 'v_title', 'v_videoId',\n",
    "                                                        'v_channelTitle', 'v_publishTime',\n",
    "                                                        'v_description', 'v_thumbnail',\n",
    "                                                        'c_id','c_parentId',\n",
    "                                                        'c_author', 'c_published_at',\n",
    "                                                        'c_updated_at', 'c_like_count',\n",
    "                                                        'c_text'])\n",
    "                    print('Unable to retrieve comments:', video.get('title', ''))\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "        return multiple_video_comments\n",
    "        \n",
    "    # alternative method by explicitely specifying videos\n",
    "    def create_video_df(self,products,videos,number_of_comments_per_video=100,number_of_replies_per_comment=100):\n",
    "        count=0\n",
    "        multiple_video_comments = pd.DataFrame()\n",
    "        for product in products:\n",
    "            for video in videos[count]:\n",
    "                response = self.get_comments(video,maxResults=number_of_comments_per_video,maxResultsDepth=number_of_replies_per_comment) \n",
    "                video=self.get_video_details(video)\n",
    "                comments_df = api_handler.get_comments_df(response, video, product)\n",
    "                multiple_video_comments = pd.concat([multiple_video_comments, comments_df], ignore_index=True)\n",
    "            count+=1\n",
    "        return multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen the Korean Drama called Squid Game to perform the sentiment analysis on. We specify the product in the products list, create a `api_handler` class object, use the `create_video_df_from_search()` function to automatically curate comments using the YouTube api call, and get a pandas Dataframe in return containing details about the videos and the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "products=[\"Squid Game Korean Drama (2021)\"]\n",
    "\n",
    "youtube=api_handler(api_service_name, api_version, DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting replies: 750\n",
      "getting replies: 520\n",
      "getting replies: 497\n",
      "getting replies: 16\n",
      "getting replies: 62\n",
      "getting replies: 64\n",
      "getting replies: 129\n",
      "getting replies: 14\n",
      "getting replies: 504\n",
      "getting replies: 101\n",
      "getting replies: 350\n",
      "getting replies: 16\n",
      "getting replies: 5\n",
      "getting replies: 3\n",
      "getting replies: 318\n",
      "getting replies: 25\n",
      "getting replies: 230\n",
      "getting replies: 390\n",
      "getting replies: 16\n",
      "getting replies: 154\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008391</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514503</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>398</td>\n",
       "      <td>Your videos are so interesting ❤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1680</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>259</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18372</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgxxPW5727hHGtrArkl4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>aishwarya9372</td>\n",
       "      <td>2021-10-26T09:02:20Z</td>\n",
       "      <td>2021-10-26T09:02:20Z</td>\n",
       "      <td>551</td>\n",
       "      <td>\"Hey there welcome to Heaven, so how did you d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>Ugw5JSGTKeZFVPnZ1tl4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>Gamer_Waffles</td>\n",
       "      <td>2021-11-14T08:30:56Z</td>\n",
       "      <td>2021-11-14T08:30:56Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Where is Gi-hun and Sang-woo ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18374</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgwgZzo9IL2Juh6oZjt4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>christophernigro9137</td>\n",
       "      <td>2021-11-24T00:15:26Z</td>\n",
       "      <td>2021-11-24T00:15:26Z</td>\n",
       "      <td>0</td>\n",
       "      <td>So no matter how hard we pulled, we couldn’t win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18375</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgxvBSfSmPjKpZd6sRJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>itsamadworld2711</td>\n",
       "      <td>2021-10-12T15:35:55Z</td>\n",
       "      <td>2021-10-12T15:35:55Z</td>\n",
       "      <td>324</td>\n",
       "      <td>When she dropped the marble so that Sae-byeok ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18376</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgyJyzo0kicWU9Iy1y94AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mistyplayzz5484</td>\n",
       "      <td>2022-04-26T00:36:59Z</td>\n",
       "      <td>2022-04-26T00:36:59Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Oml i just now thought what would squid game b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18377 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "18372  Squid Game Korean Drama (2021)   \n",
       "18373  Squid Game Korean Drama (2021)   \n",
       "18374  Squid Game Korean Drama (2021)   \n",
       "18375  Squid Game Korean Drama (2021)   \n",
       "18376  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "0                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "1                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "2                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "4                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "...                                                  ...          ...   \n",
       "18372  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18373  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18374  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18375  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18376  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "\n",
       "          v_channelTitle         v_publishTime  \\\n",
       "0                MrBeast  2021-11-24T21:00:01Z   \n",
       "1                MrBeast  2021-11-24T21:00:01Z   \n",
       "2                MrBeast  2021-11-24T21:00:01Z   \n",
       "3                MrBeast  2021-11-24T21:00:01Z   \n",
       "4                MrBeast  2021-11-24T21:00:01Z   \n",
       "...                  ...                   ...   \n",
       "18372  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18373  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18374  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18375  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18376  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "18372  They may have survived the dalgona challenge i...   \n",
       "18373  They may have survived the dalgona challenge i...   \n",
       "18374  They may have survived the dalgona challenge i...   \n",
       "18375  They may have survived the dalgona challenge i...   \n",
       "18376  They may have survived the dalgona challenge i...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "18372  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18373  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18374  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18375  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18376  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "\n",
       "                             c_id c_parentId              c_author  \\\n",
       "0      UgzH8vliQSJKHQMGZjx4AaABAg                          MrBeast   \n",
       "1      UgwDhFNTCbfck5apuUJ4AaABAg                      DoodleChaos   \n",
       "2      UgzVlS_nKI4aXISU_ep4AaABAg                      mukul_editz   \n",
       "3      UgxykcUWbPcLhlL-Gy14AaABAg                      SpamR1_2013   \n",
       "4      Ugxu5B8dQ9-mZpfW-UV4AaABAg                  user-cs9zv3gh1k   \n",
       "...                           ...        ...                   ...   \n",
       "18372  UgxxPW5727hHGtrArkl4AaABAg                    aishwarya9372   \n",
       "18373  Ugw5JSGTKeZFVPnZ1tl4AaABAg                    Gamer_Waffles   \n",
       "18374  UgwgZzo9IL2Juh6oZjt4AaABAg             christophernigro9137   \n",
       "18375  UgxvBSfSmPjKpZd6sRJ4AaABAg                 itsamadworld2711   \n",
       "18376  UgyJyzo0kicWU9Iy1y94AaABAg                  mistyplayzz5484   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z  2021-11-24T21:02:45Z       1008391   \n",
       "1      2021-11-24T22:07:54Z  2021-11-24T22:07:54Z        514503   \n",
       "2      2023-12-30T01:55:59Z  2023-12-30T01:55:59Z           398   \n",
       "3      2023-11-27T00:57:21Z  2023-11-27T00:57:21Z          1680   \n",
       "4      2024-01-30T20:17:02Z  2024-01-30T20:17:02Z           259   \n",
       "...                     ...                   ...           ...   \n",
       "18372  2021-10-26T09:02:20Z  2021-10-26T09:02:20Z           551   \n",
       "18373  2021-11-14T08:30:56Z  2021-11-14T08:30:56Z             1   \n",
       "18374  2021-11-24T00:15:26Z  2021-11-24T00:15:26Z             0   \n",
       "18375  2021-10-12T15:35:55Z  2021-10-12T15:35:55Z           324   \n",
       "18376  2022-04-26T00:36:59Z  2022-04-26T00:36:59Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                       Your videos are so interesting ❤  \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "18372  \"Hey there welcome to Heaven, so how did you d...  \n",
       "18373                     Where is Gi-hun and Sang-woo ?  \n",
       "18374  So no matter how hard we pulled, we couldn’t win.  \n",
       "18375  When she dropped the marble so that Sae-byeok ...  \n",
       "18376  Oml i just now thought what would squid game b...  \n",
       "\n",
       "[18377 rows x 14 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_video_comments=youtube.create_video_df_from_search(products,number_of_videos_per_product=20,number_of_comments_per_video=1000,number_of_replies_per_comment=0)\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Analysis, Selection and Labeling:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from:\n",
    "\n",
    "https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis : As emojis do not provide any helpful information they should be removed from the text strings.\n",
    "def remove_emojis(data):\n",
    "    if isinstance(data, str):\n",
    "        # Remove html tags\n",
    "        data = BeautifulSoup(data, \"html.parser\").get_text()\n",
    "        # Remove emote, etc\n",
    "        emoj = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "                        \"]+\", re.UNICODE)\n",
    "        # english_words = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "        return re.sub(emoj, '', data)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row containing NA values.\n",
    "multiple_video_comments.dropna(subset=['c_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaq\\AppData\\Local\\Temp\\ipykernel_12924\\2754946649.py:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  data = BeautifulSoup(data, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Length Before: 18377\n",
      "DataFrame Length After: 15717\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzH8vliQSJKHQMGZjx4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>2021-11-24T21:02:45Z</td>\n",
       "      <td>1008391</td>\n",
       "      <td>Like I said in the video, subscribe if you hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgwDhFNTCbfck5apuUJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>DoodleChaos</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>2021-11-24T22:07:54Z</td>\n",
       "      <td>514503</td>\n",
       "      <td>Huge props to the set designers, everything wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgzVlS_nKI4aXISU_ep4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mukul_editz</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>2023-12-30T01:55:59Z</td>\n",
       "      <td>398</td>\n",
       "      <td>Your videos are so interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>UgxykcUWbPcLhlL-Gy14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SpamR1_2013</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>2023-11-27T00:57:21Z</td>\n",
       "      <td>1680</td>\n",
       "      <td>that guy who sacrificed himself on purpose for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>$456,000 Squid Game In Real Life!</td>\n",
       "      <td>0e3GPea1Tyg</td>\n",
       "      <td>MrBeast</td>\n",
       "      <td>2021-11-24T21:00:01Z</td>\n",
       "      <td>MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...</td>\n",
       "      <td>https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg</td>\n",
       "      <td>Ugxu5B8dQ9-mZpfW-UV4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>user-cs9zv3gh1k</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>2024-01-30T20:17:02Z</td>\n",
       "      <td>259</td>\n",
       "      <td>This version of the game is pretty much what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18372</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgxxPW5727hHGtrArkl4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>aishwarya9372</td>\n",
       "      <td>2021-10-26T09:02:20Z</td>\n",
       "      <td>2021-10-26T09:02:20Z</td>\n",
       "      <td>551</td>\n",
       "      <td>\"Hey there welcome to Heaven, so how did you d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18373</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>Ugw5JSGTKeZFVPnZ1tl4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>Gamer_Waffles</td>\n",
       "      <td>2021-11-14T08:30:56Z</td>\n",
       "      <td>2021-11-14T08:30:56Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Where is Gi-hun and Sang-woo ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18374</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgwgZzo9IL2Juh6oZjt4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>christophernigro9137</td>\n",
       "      <td>2021-11-24T00:15:26Z</td>\n",
       "      <td>2021-11-24T00:15:26Z</td>\n",
       "      <td>0</td>\n",
       "      <td>So no matter how hard we pulled, we couldn’t win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18375</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgxvBSfSmPjKpZd6sRJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>itsamadworld2711</td>\n",
       "      <td>2021-10-12T15:35:55Z</td>\n",
       "      <td>2021-10-12T15:35:55Z</td>\n",
       "      <td>324</td>\n",
       "      <td>When she dropped the marble so that Sae-byeok ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18376</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgyJyzo0kicWU9Iy1y94AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>mistyplayzz5484</td>\n",
       "      <td>2022-04-26T00:36:59Z</td>\n",
       "      <td>2022-04-26T00:36:59Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Oml i just now thought what would squid game b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15717 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0      Squid Game Korean Drama (2021)   \n",
       "1      Squid Game Korean Drama (2021)   \n",
       "2      Squid Game Korean Drama (2021)   \n",
       "3      Squid Game Korean Drama (2021)   \n",
       "4      Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "18372  Squid Game Korean Drama (2021)   \n",
       "18373  Squid Game Korean Drama (2021)   \n",
       "18374  Squid Game Korean Drama (2021)   \n",
       "18375  Squid Game Korean Drama (2021)   \n",
       "18376  Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "0                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "1                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "2                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "3                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "4                      $456,000 Squid Game In Real Life!  0e3GPea1Tyg   \n",
       "...                                                  ...          ...   \n",
       "18372  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18373  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18374  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18375  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "18376  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "\n",
       "          v_channelTitle         v_publishTime  \\\n",
       "0                MrBeast  2021-11-24T21:00:01Z   \n",
       "1                MrBeast  2021-11-24T21:00:01Z   \n",
       "2                MrBeast  2021-11-24T21:00:01Z   \n",
       "3                MrBeast  2021-11-24T21:00:01Z   \n",
       "4                MrBeast  2021-11-24T21:00:01Z   \n",
       "...                  ...                   ...   \n",
       "18372  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18373  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18374  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18375  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "18376  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "0      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "1      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "2      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "3      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "4      MAKE SURE YOU WATCH UNTIL GLASS BRIDGE IT'S IN...   \n",
       "...                                                  ...   \n",
       "18372  They may have survived the dalgona challenge i...   \n",
       "18373  They may have survived the dalgona challenge i...   \n",
       "18374  They may have survived the dalgona challenge i...   \n",
       "18375  They may have survived the dalgona challenge i...   \n",
       "18376  They may have survived the dalgona challenge i...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "0      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "1      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "2      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "3      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "4      https://i.ytimg.com/vi/0e3GPea1Tyg/default.jpg   \n",
       "...                                               ...   \n",
       "18372  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18373  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18374  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18375  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "18376  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "\n",
       "                             c_id c_parentId              c_author  \\\n",
       "0      UgzH8vliQSJKHQMGZjx4AaABAg                          MrBeast   \n",
       "1      UgwDhFNTCbfck5apuUJ4AaABAg                      DoodleChaos   \n",
       "2      UgzVlS_nKI4aXISU_ep4AaABAg                      mukul_editz   \n",
       "3      UgxykcUWbPcLhlL-Gy14AaABAg                      SpamR1_2013   \n",
       "4      Ugxu5B8dQ9-mZpfW-UV4AaABAg                  user-cs9zv3gh1k   \n",
       "...                           ...        ...                   ...   \n",
       "18372  UgxxPW5727hHGtrArkl4AaABAg                    aishwarya9372   \n",
       "18373  Ugw5JSGTKeZFVPnZ1tl4AaABAg                    Gamer_Waffles   \n",
       "18374  UgwgZzo9IL2Juh6oZjt4AaABAg             christophernigro9137   \n",
       "18375  UgxvBSfSmPjKpZd6sRJ4AaABAg                 itsamadworld2711   \n",
       "18376  UgyJyzo0kicWU9Iy1y94AaABAg                  mistyplayzz5484   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "0      2021-11-24T21:02:45Z  2021-11-24T21:02:45Z       1008391   \n",
       "1      2021-11-24T22:07:54Z  2021-11-24T22:07:54Z        514503   \n",
       "2      2023-12-30T01:55:59Z  2023-12-30T01:55:59Z           398   \n",
       "3      2023-11-27T00:57:21Z  2023-11-27T00:57:21Z          1680   \n",
       "4      2024-01-30T20:17:02Z  2024-01-30T20:17:02Z           259   \n",
       "...                     ...                   ...           ...   \n",
       "18372  2021-10-26T09:02:20Z  2021-10-26T09:02:20Z           551   \n",
       "18373  2021-11-14T08:30:56Z  2021-11-14T08:30:56Z             1   \n",
       "18374  2021-11-24T00:15:26Z  2021-11-24T00:15:26Z             0   \n",
       "18375  2021-10-12T15:35:55Z  2021-10-12T15:35:55Z           324   \n",
       "18376  2022-04-26T00:36:59Z  2022-04-26T00:36:59Z             0   \n",
       "\n",
       "                                                  c_text  \n",
       "0      Like I said in the video, subscribe if you hav...  \n",
       "1      Huge props to the set designers, everything wa...  \n",
       "2                        Your videos are so interesting   \n",
       "3      that guy who sacrificed himself on purpose for...  \n",
       "4      This version of the game is pretty much what t...  \n",
       "...                                                  ...  \n",
       "18372  \"Hey there welcome to Heaven, so how did you d...  \n",
       "18373                     Where is Gi-hun and Sang-woo ?  \n",
       "18374  So no matter how hard we pulled, we couldn’t win.  \n",
       "18375  When she dropped the marble so that Sae-byeok ...  \n",
       "18376  Oml i just now thought what would squid game b...  \n",
       "\n",
       "[15717 rows x 14 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove emojis from the text to be analyzed\n",
    "multiple_video_comments['c_text']=multiple_video_comments['c_text'].apply(remove_emojis)\n",
    "\n",
    "df_length_before = len(multiple_video_comments)\n",
    "print(\"DataFrame Length Before:\", df_length_before)\n",
    "\n",
    "# Drop duplicates\n",
    "multiple_video_comments.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop rows with empty or text length <= 2 comments\n",
    "multiple_video_comments = multiple_video_comments[multiple_video_comments['c_text'].apply(lambda x: len(x) > 2)]\n",
    "\n",
    "df_length_after = len(multiple_video_comments)\n",
    "print(\"DataFrame Length After:\", df_length_after)\n",
    "\n",
    "multiple_video_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "https://stackoverflow.com/questions/40375366/pandas-to-csv-checking-for-overwrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self):\n",
    "        # Define keywords related to the TV show\n",
    "        self.tv_show_keywords = ['Squid Game', 'Gi-hun', 'Sang-woo', 'Player', 'Red light, green light', 'Honeycomb',\n",
    "                            'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                            'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho'\n",
    "                            'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                            'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali', ]\n",
    "        # Setting threshold value for validating the relevance of the comment\n",
    "        self.threshold = 1\n",
    "\n",
    "    # Tokenize text and remove stop words\n",
    "    def preprocess_text(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    # Matching function to check relevance of the comments\n",
    "    def match_keywords(self, tokens):\n",
    "        return [token for token in tokens if token in self.tv_show_keywords]\n",
    "\n",
    "    # Scoring function to calculate how many tokens matched\n",
    "    def calculate_score(self, tokens):\n",
    "        return len(tokens)\n",
    "\n",
    "    # Validate function to validate the relevance based on threshold\n",
    "    def validate_relevance(self, score):\n",
    "        return score >= self.threshold\n",
    "\n",
    "    def filter_comments(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        irrelevant_keywords = ['HYVE', 'crypto', 'promotion', 'ad', 'spam', 'advertisement', 'spoiler', 'leak', 'promo', 'off-topic', 'clickbait',\n",
    "                            'self-promotion', '0:', '1:', '2:', '3:', '4:', '5:', '6:', '7:',\n",
    "                            '8:', '9:', '10:', '11:', '12:', '13:', '14:', '15:']\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                if detect(row['c_text']) == 'en' and not any(keyword in row['c_text'] for keyword in irrelevant_keywords):\n",
    "                    comments.append(row)\n",
    "                    c += 1\n",
    "            except Exception as e:  # Catch any exception\n",
    "                pass\n",
    "        print(\"Number of Filtered Comments: \", c)\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])  # Create a new DataFrame from the list of rows\n",
    "        new_df = new_df.sort_values(by = ['c_like_count'], ascending = False)\n",
    "        new_df.drop_duplicates(inplace=True)\n",
    "        new_df = new_df[:500]\n",
    "        return new_df\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        c = 0\n",
    "        comments = []\n",
    "        for index, row in df.iterrows():\n",
    "            processed_text = self.preprocess_text(row['c_text'])\n",
    "            matched_keywords = self.match_keywords(processed_text)\n",
    "            score = self.calculate_score(matched_keywords)\n",
    "            is_relevant = self.validate_relevance(score)\n",
    "            if is_relevant == 1:\n",
    "                comments.append(row)\n",
    "                c += 1\n",
    "\n",
    "        new_df = pd.DataFrame(comments, \n",
    "                    columns=['product', 'v_title', 'v_videoId',\n",
    "                        'v_channelTitle', 'v_publishTime',\n",
    "                        'v_description', 'v_thumbnail',\n",
    "                        'c_id','c_parentId',\n",
    "                        'c_author', 'c_published_at',\n",
    "                        'c_updated_at', 'c_like_count',\n",
    "                        'c_text'])\n",
    "        print(\"Number of Processed Comments: \", c)\n",
    "        new_df = self.filter_comments(new_df)\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Processed Comments:  1788\n",
      "Number of Filtered Comments:  1103\n"
     ]
    }
   ],
   "source": [
    "p = preprocessing()\n",
    "new_df = p.preprocess(multiple_video_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_videoId</th>\n",
       "      <th>v_channelTitle</th>\n",
       "      <th>v_publishTime</th>\n",
       "      <th>v_description</th>\n",
       "      <th>v_thumbnail</th>\n",
       "      <th>c_id</th>\n",
       "      <th>c_parentId</th>\n",
       "      <th>c_author</th>\n",
       "      <th>c_published_at</th>\n",
       "      <th>c_updated_at</th>\n",
       "      <th>c_like_count</th>\n",
       "      <th>c_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game (Behind The Scenes) #Shorts</td>\n",
       "      <td>4vb085gEgPc</td>\n",
       "      <td>Behind The Scenes</td>\n",
       "      <td>2022-03-20T16:43:54Z</td>\n",
       "      <td>This video gives you a chance to look BEHIND T...</td>\n",
       "      <td>https://i.ytimg.com/vi/4vb085gEgPc/default.jpg</td>\n",
       "      <td>UgxEeZvLDwVE2jcneuJ4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>_Taylo_</td>\n",
       "      <td>2022-06-09T00:13:48Z</td>\n",
       "      <td>2022-06-09T00:13:48Z</td>\n",
       "      <td>10273</td>\n",
       "      <td>Oh so the camera-man plays squid game too?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>SQUID GAME | RED LIGHT GREEN LIGHT SCENE</td>\n",
       "      <td>sH4Y450PSVM</td>\n",
       "      <td>memebappe</td>\n",
       "      <td>2021-10-14T18:52:37Z</td>\n",
       "      <td>BUY THE PERFECT CHRISTMAS GIFT    : https://am...</td>\n",
       "      <td>https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg</td>\n",
       "      <td>UgyXDA0Vdld5b5SMG2Z4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>silver-eyedfox7713</td>\n",
       "      <td>2022-01-16T01:30:47Z</td>\n",
       "      <td>2022-01-16T01:30:47Z</td>\n",
       "      <td>8215</td>\n",
       "      <td>This scene is the perfect introduction to how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17478</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgyBW8hKpZ3Tcxf0rJt4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>jeng6786</td>\n",
       "      <td>2021-10-12T02:08:48Z</td>\n",
       "      <td>2021-10-12T02:08:48Z</td>\n",
       "      <td>6886</td>\n",
       "      <td>Most memorable character : Ali. The marble sce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17430</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>Ugzvskgmdrku401Z3hF4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>ballinklong</td>\n",
       "      <td>2021-10-11T14:38:58Z</td>\n",
       "      <td>2021-10-11T14:38:58Z</td>\n",
       "      <td>6671</td>\n",
       "      <td>You realize Ali's personality is the only one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17472</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>Ugwyddf9lv0uoE5z15t4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>LittleCart</td>\n",
       "      <td>2021-10-12T11:58:45Z</td>\n",
       "      <td>2021-10-12T11:58:45Z</td>\n",
       "      <td>6274</td>\n",
       "      <td>Considering how popular Squid Game got, it's s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Red Light Green Light No Blood - Squid Game 1</td>\n",
       "      <td>Ww9HCin8ORs</td>\n",
       "      <td>PopMov</td>\n",
       "      <td>2021-10-06T20:56:52Z</td>\n",
       "      <td>I did this for a special person who wanted to ...</td>\n",
       "      <td>https://i.ytimg.com/vi/Ww9HCin8ORs/default.jpg</td>\n",
       "      <td>UgzDlVQDHeMnlAhnC4t4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>piggyy367</td>\n",
       "      <td>2021-11-26T11:39:13Z</td>\n",
       "      <td>2021-11-26T11:39:13Z</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes I can see squid game without getting hungr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13630</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Sugar Honeycomb No Blood - Squid Game 2</td>\n",
       "      <td>qE3TiUVd1Qc</td>\n",
       "      <td>PopMov</td>\n",
       "      <td>2021-10-07T18:44:36Z</td>\n",
       "      <td>Dont miss our new FRONTMAN song! See the music...</td>\n",
       "      <td>https://i.ytimg.com/vi/qE3TiUVd1Qc/default.jpg</td>\n",
       "      <td>UgyfYSZDz7Kqyvr2Lj94AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>abdullafahar7051</td>\n",
       "      <td>2022-02-15T14:50:17Z</td>\n",
       "      <td>2022-02-15T14:50:17Z</td>\n",
       "      <td>2</td>\n",
       "      <td>Finally, u just watched Sqiud Game peacefully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16807</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Red Light Green Light No Blood - Squid Game 1</td>\n",
       "      <td>Ww9HCin8ORs</td>\n",
       "      <td>PopMov</td>\n",
       "      <td>2021-10-06T20:56:52Z</td>\n",
       "      <td>I did this for a special person who wanted to ...</td>\n",
       "      <td>https://i.ytimg.com/vi/Ww9HCin8ORs/default.jpg</td>\n",
       "      <td>UgwFnHY3AISE6BWNAG94AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>therealprotosplaat319</td>\n",
       "      <td>2021-12-02T21:08:49Z</td>\n",
       "      <td>2021-12-02T21:08:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>Squid game finally has no blood finally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17585</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game stars take on the Dalgona Challenge...</td>\n",
       "      <td>TYd_pT9hZrM</td>\n",
       "      <td>Netflix K-Content</td>\n",
       "      <td>2021-10-09T09:00:10Z</td>\n",
       "      <td>They may have survived the dalgona challenge i...</td>\n",
       "      <td>https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg</td>\n",
       "      <td>UgwJLwRyFQL_ovPclp14AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>gameriliana7898</td>\n",
       "      <td>2021-10-24T23:25:18Z</td>\n",
       "      <td>2021-10-24T23:25:18Z</td>\n",
       "      <td>2</td>\n",
       "      <td>Girl trust me whenever I think of squid game I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9431</th>\n",
       "      <td>Squid Game Korean Drama (2021)</td>\n",
       "      <td>Squid Game | Official Trailer | Netflix</td>\n",
       "      <td>oqxAJKy0ii4</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>2021-09-02T00:00:02Z</td>\n",
       "      <td>A Netflix Series | Squid Game Survive or die W...</td>\n",
       "      <td>https://i.ytimg.com/vi/oqxAJKy0ii4/default.jpg</td>\n",
       "      <td>UgyxJI88j3eNzqD0doN4AaABAg</td>\n",
       "      <td></td>\n",
       "      <td>SayaRabbitholeSimp</td>\n",
       "      <td>2022-03-09T06:32:39Z</td>\n",
       "      <td>2022-03-09T06:32:39Z</td>\n",
       "      <td>2</td>\n",
       "      <td>I just watched Squid Game for the first time a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "4998   Squid Game Korean Drama (2021)   \n",
       "3996   Squid Game Korean Drama (2021)   \n",
       "17478  Squid Game Korean Drama (2021)   \n",
       "17430  Squid Game Korean Drama (2021)   \n",
       "17472  Squid Game Korean Drama (2021)   \n",
       "...                               ...   \n",
       "17292  Squid Game Korean Drama (2021)   \n",
       "13630  Squid Game Korean Drama (2021)   \n",
       "16807  Squid Game Korean Drama (2021)   \n",
       "17585  Squid Game Korean Drama (2021)   \n",
       "9431   Squid Game Korean Drama (2021)   \n",
       "\n",
       "                                                 v_title    v_videoId  \\\n",
       "4998              Squid Game (Behind The Scenes) #Shorts  4vb085gEgPc   \n",
       "3996            SQUID GAME | RED LIGHT GREEN LIGHT SCENE  sH4Y450PSVM   \n",
       "17478  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "17430  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "17472  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "...                                                  ...          ...   \n",
       "17292      Red Light Green Light No Blood - Squid Game 1  Ww9HCin8ORs   \n",
       "13630            Sugar Honeycomb No Blood - Squid Game 2  qE3TiUVd1Qc   \n",
       "16807      Red Light Green Light No Blood - Squid Game 1  Ww9HCin8ORs   \n",
       "17585  Squid Game stars take on the Dalgona Challenge...  TYd_pT9hZrM   \n",
       "9431             Squid Game | Official Trailer | Netflix  oqxAJKy0ii4   \n",
       "\n",
       "          v_channelTitle         v_publishTime  \\\n",
       "4998   Behind The Scenes  2022-03-20T16:43:54Z   \n",
       "3996          memebappe   2021-10-14T18:52:37Z   \n",
       "17478  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "17430  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "17472  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "...                  ...                   ...   \n",
       "17292             PopMov  2021-10-06T20:56:52Z   \n",
       "13630             PopMov  2021-10-07T18:44:36Z   \n",
       "16807             PopMov  2021-10-06T20:56:52Z   \n",
       "17585  Netflix K-Content  2021-10-09T09:00:10Z   \n",
       "9431             Netflix  2021-09-02T00:00:02Z   \n",
       "\n",
       "                                           v_description  \\\n",
       "4998   This video gives you a chance to look BEHIND T...   \n",
       "3996   BUY THE PERFECT CHRISTMAS GIFT    : https://am...   \n",
       "17478  They may have survived the dalgona challenge i...   \n",
       "17430  They may have survived the dalgona challenge i...   \n",
       "17472  They may have survived the dalgona challenge i...   \n",
       "...                                                  ...   \n",
       "17292  I did this for a special person who wanted to ...   \n",
       "13630  Dont miss our new FRONTMAN song! See the music...   \n",
       "16807  I did this for a special person who wanted to ...   \n",
       "17585  They may have survived the dalgona challenge i...   \n",
       "9431   A Netflix Series | Squid Game Survive or die W...   \n",
       "\n",
       "                                          v_thumbnail  \\\n",
       "4998   https://i.ytimg.com/vi/4vb085gEgPc/default.jpg   \n",
       "3996   https://i.ytimg.com/vi/sH4Y450PSVM/default.jpg   \n",
       "17478  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "17430  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "17472  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "...                                               ...   \n",
       "17292  https://i.ytimg.com/vi/Ww9HCin8ORs/default.jpg   \n",
       "13630  https://i.ytimg.com/vi/qE3TiUVd1Qc/default.jpg   \n",
       "16807  https://i.ytimg.com/vi/Ww9HCin8ORs/default.jpg   \n",
       "17585  https://i.ytimg.com/vi/TYd_pT9hZrM/default.jpg   \n",
       "9431   https://i.ytimg.com/vi/oqxAJKy0ii4/default.jpg   \n",
       "\n",
       "                             c_id c_parentId               c_author  \\\n",
       "4998   UgxEeZvLDwVE2jcneuJ4AaABAg                           _Taylo_   \n",
       "3996   UgyXDA0Vdld5b5SMG2Z4AaABAg                silver-eyedfox7713   \n",
       "17478  UgyBW8hKpZ3Tcxf0rJt4AaABAg                          jeng6786   \n",
       "17430  Ugzvskgmdrku401Z3hF4AaABAg                       ballinklong   \n",
       "17472  Ugwyddf9lv0uoE5z15t4AaABAg                        LittleCart   \n",
       "...                           ...        ...                    ...   \n",
       "17292  UgzDlVQDHeMnlAhnC4t4AaABAg                         piggyy367   \n",
       "13630  UgyfYSZDz7Kqyvr2Lj94AaABAg                  abdullafahar7051   \n",
       "16807  UgwFnHY3AISE6BWNAG94AaABAg             therealprotosplaat319   \n",
       "17585  UgwJLwRyFQL_ovPclp14AaABAg                   gameriliana7898   \n",
       "9431   UgyxJI88j3eNzqD0doN4AaABAg                SayaRabbitholeSimp   \n",
       "\n",
       "             c_published_at          c_updated_at  c_like_count  \\\n",
       "4998   2022-06-09T00:13:48Z  2022-06-09T00:13:48Z         10273   \n",
       "3996   2022-01-16T01:30:47Z  2022-01-16T01:30:47Z          8215   \n",
       "17478  2021-10-12T02:08:48Z  2021-10-12T02:08:48Z          6886   \n",
       "17430  2021-10-11T14:38:58Z  2021-10-11T14:38:58Z          6671   \n",
       "17472  2021-10-12T11:58:45Z  2021-10-12T11:58:45Z          6274   \n",
       "...                     ...                   ...           ...   \n",
       "17292  2021-11-26T11:39:13Z  2021-11-26T11:39:13Z             2   \n",
       "13630  2022-02-15T14:50:17Z  2022-02-15T14:50:17Z             2   \n",
       "16807  2021-12-02T21:08:49Z  2021-12-02T21:08:49Z             2   \n",
       "17585  2021-10-24T23:25:18Z  2021-10-24T23:25:18Z             2   \n",
       "9431   2022-03-09T06:32:39Z  2022-03-09T06:32:39Z             2   \n",
       "\n",
       "                                                  c_text  \n",
       "4998          Oh so the camera-man plays squid game too?  \n",
       "3996   This scene is the perfect introduction to how ...  \n",
       "17478  Most memorable character : Ali. The marble sce...  \n",
       "17430  You realize Ali's personality is the only one ...  \n",
       "17472  Considering how popular Squid Game got, it's s...  \n",
       "...                                                  ...  \n",
       "17292  Yes I can see squid game without getting hungr...  \n",
       "13630      Finally, u just watched Sqiud Game peacefully  \n",
       "16807            Squid game finally has no blood finally  \n",
       "17585  Girl trust me whenever I think of squid game I...  \n",
       "9431   I just watched Squid Game for the first time a...  \n",
       "\n",
       "[500 rows x 14 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling comments using Sentiment Lexicon - VADER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_lexicon = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(c_text):\n",
    "    sentiment_Score = sentiment_lexicon.polarity_scores(c_text)\n",
    "    return sentiment_Score['compound']\n",
    "\n",
    "def check_sentiment(sentiment_score):\n",
    "    if sentiment_score > 0.00:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0.00:\n",
    "        return \"Negative\"\n",
    "    elif sentiment_score == 0:\n",
    "        return \"Neutral \"\n",
    "\n",
    "new_df['sentiment_score'] = new_df['c_text'].apply(get_sentiment_score)\n",
    "new_df['sentiment'] = new_df['sentiment_score'].apply(check_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Comments for each polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0258  Negative           This scene is the perfect introduction to how the games are a matter of life and death. The slow build up, the sudden reveal that the dude was really shot, and the ensuing chaos make this the perfect introduction to these games.\n",
      "    -0.0258  Negative           *Obviously our hero will play the game again to find out who is behind it after the old man's death and to stop it forever ...*\n",
      "    -0.0258  Negative           If player 1 was the boss then who started the game after his death?\n",
      "                                You explanation  is amazing\n",
      "    -0.1027  Negative           An American squid game with the right Director and cast would go hard\n",
      "    -0.1027  Negative           Squid game cast and crew working hard for years making season 1\n",
      "\n",
      "                                Us:- Finishing it in 1 day and expecting a season 2 to come soon\n",
      "    -0.1513  Negative           After watching the whole series I felt really sorry for Ali\n",
      "    -0.1531  Negative           Hunters Should Be Forced To Play Squid Game.\n",
      "    -0.1761  Negative           This game / series is hitting really hard this year shout out to Koreans for making this\n",
      "    -0.2263  Negative           Si Ung's Wife Family was also having financial problems. So in next episode  we get to see his wife and her husband also playing The game\n",
      "    -0.2263  Negative           The Friendship: Gi-Hun & Il-Nam\n",
      "                                The Betrayal: Sangwoo & Ali\n",
      "                                The Sacrifice: Sae-byeok & Ji-Yeon\n",
      "     0       Neutral            Dalgona candy makers after squid game: \"Business is Boomin\"\n",
      "     0       Neutral            You are my favourite explainer of pictures mene apki sari videos to nahi deakhi hai mene bas it wali seres or squid game wali mene deaki hai or merko bahot pasand ayai\n",
      "     0       Neutral            Squid game changed a lot in 1 year\n",
      "     0       Neutral            I remember the hype of squid game\n",
      "     0       Neutral            What about the cookie game?\n",
      "     0       Neutral            Japan : Alice in Borderland\n",
      "                                Korea : Squid Game\n",
      "                                China : ????\n",
      "     0       Neutral            This has more views than the actual squid game. Let that sink in.\n",
      "     0       Neutral            Really squid game beginning is red paper blue paper\n",
      "     0       Neutral            Squid game but kids are able to see it (no blood squid game)\n",
      "     0       Neutral            Squid game slowly becomes nostalgic\n",
      "     0.9721  Positive           My cousin has a Roblox and I played squid game it’s very true I swear it’s very true it’s very true OK OK OK OK\n",
      "     0.9667  Positive           nobody talking about queen kim joo ryoung but she is so humble, so pretty it’s totally different from what you see from her in squid game. what a great actress she deserves everyone’s love.\n",
      "     0.9611  Positive           Hi Kaycee, the squid game was really fun, good to see you with your family and friends, thanks a lot for bringing smile in my life, lots of love from kshirja kharvi\n",
      "     0.9575  Positive           Thought this was going to be your average survival film I could just have on the in background. What I got was a 9 episode cult classic contender. Extremely well developed characters and some of the best acting I've seen in a long time. For this genre, I was not expecting to hate characters or care for others so deeply.... some scenes really hit deep. Like damn... and that's where the magic lies for Squid Game, this isn't your average Hollywood production where the spectacle is in the plot or overdramatic twists. Here the real entertainment is in watching the host's cynical world view play out on a very human level through the relationships that are created and destroyed between the characters. Make sure you watch it with subtitles, the native acting is too good to substitute.\n",
      "     0.9555  Positive           No but 102 who sacrificed himself for his best friend to continue the game is so kind\n",
      "     0.9543  Positive           I absolutely love Lee Yoomi (she’s so giggly and fun) and HoYeon Jung. Their friendship in and out of Squid Game is so stable- I also love the moment in Squid Game when Ji-yeong dropped her marble..\n",
      "     0.9337  Positive           Just finished this series last end of December and honestly speaking I just watched this trailer when I sent this comment. til now this series still hooked me with superb and excellent story. Kudos to Squid game team\n",
      "     0.9329  Positive           Well done!!!!\n",
      "                                I was literally waiting and wishing that you should post the explanation of squid game it's a bit emotional but so much interesting  please keep make these type of movie's explanation thank you\n",
      "     0.9324  Positive           The set looked amazing and that glass game looked as tense as I could ever\n",
      "                                imagine. Awesome job MrBeast. Congrats to the winner!\n",
      "     0.9323  Positive           Hi all, I just want to point out that a lot of the scenes in this concept teaser came from a Chinese survival/action film called \"Animal World\"(Dong Wu Shi Jie), with Yifeng Li and Michael Douglas. In a similar style as the Squid Game but with crazier visuals. As a Squid Game fan myself, I think you should enjoy that film too! Anyone that sees this comment, have a great day!\n"
     ]
    }
   ],
   "source": [
    "def select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('sentiment')\n",
    "\n",
    "    # iterate over each polarity group\n",
    "    for sentiment, group in grouped:\n",
    "        # sort comments by sentiment score pick top 10\n",
    "        top_group_comments = group.sort_values(by='sentiment_score', ascending=False).head(top_n)[['sentiment_score', 'sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "# # top 10 comments for each polarity\n",
    "# for sentiment_score, sentiment, comment in top_comments:\n",
    "#     print(f\"Sentiment: {sentiment}, Sentiment Score: {sentiment_score}, Comment: {comment}\")\n",
    "\n",
    "# making it pretty~~~\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(top_comments, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Lexicon using TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Sentiment Score    Comment\n",
      "-----------  -----------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "    -0.0258  Negative           This scene is the perfect introduction to how the games are a matter of life and death. The slow build up, the sudden reveal that the dude was really shot, and the ensuing chaos make this the perfect introduction to these games.\n",
      "    -0.0258  Negative           *Obviously our hero will play the game again to find out who is behind it after the old man's death and to stop it forever ...*\n",
      "    -0.0258  Negative           If player 1 was the boss then who started the game after his death?\n",
      "                                You explanation  is amazing\n",
      "    -0.1027  Negative           An American squid game with the right Director and cast would go hard\n",
      "    -0.1027  Negative           Squid game cast and crew working hard for years making season 1\n",
      "\n",
      "                                Us:- Finishing it in 1 day and expecting a season 2 to come soon\n",
      "    -0.1513  Negative           After watching the whole series I felt really sorry for Ali\n",
      "    -0.1531  Negative           Hunters Should Be Forced To Play Squid Game.\n",
      "    -0.1761  Negative           This game / series is hitting really hard this year shout out to Koreans for making this\n",
      "    -0.2263  Negative           Si Ung's Wife Family was also having financial problems. So in next episode  we get to see his wife and her husband also playing The game\n",
      "    -0.2263  Negative           The Friendship: Gi-Hun & Il-Nam\n",
      "                                The Betrayal: Sangwoo & Ali\n",
      "                                The Sacrifice: Sae-byeok & Ji-Yeon\n",
      "     0       Neutral            Dalgona candy makers after squid game: \"Business is Boomin\"\n",
      "     0       Neutral            You are my favourite explainer of pictures mene apki sari videos to nahi deakhi hai mene bas it wali seres or squid game wali mene deaki hai or merko bahot pasand ayai\n",
      "     0       Neutral            Squid game changed a lot in 1 year\n",
      "     0       Neutral            I remember the hype of squid game\n",
      "     0       Neutral            What about the cookie game?\n",
      "     0       Neutral            Japan : Alice in Borderland\n",
      "                                Korea : Squid Game\n",
      "                                China : ????\n",
      "     0       Neutral            This has more views than the actual squid game. Let that sink in.\n",
      "     0       Neutral            Really squid game beginning is red paper blue paper\n",
      "     0       Neutral            Squid game but kids are able to see it (no blood squid game)\n",
      "     0       Neutral            Squid game slowly becomes nostalgic\n",
      "     0.9721  Positive           My cousin has a Roblox and I played squid game it’s very true I swear it’s very true it’s very true OK OK OK OK\n",
      "     0.9667  Positive           nobody talking about queen kim joo ryoung but she is so humble, so pretty it’s totally different from what you see from her in squid game. what a great actress she deserves everyone’s love.\n",
      "     0.9611  Positive           Hi Kaycee, the squid game was really fun, good to see you with your family and friends, thanks a lot for bringing smile in my life, lots of love from kshirja kharvi\n",
      "     0.9575  Positive           Thought this was going to be your average survival film I could just have on the in background. What I got was a 9 episode cult classic contender. Extremely well developed characters and some of the best acting I've seen in a long time. For this genre, I was not expecting to hate characters or care for others so deeply.... some scenes really hit deep. Like damn... and that's where the magic lies for Squid Game, this isn't your average Hollywood production where the spectacle is in the plot or overdramatic twists. Here the real entertainment is in watching the host's cynical world view play out on a very human level through the relationships that are created and destroyed between the characters. Make sure you watch it with subtitles, the native acting is too good to substitute.\n",
      "     0.9555  Positive           No but 102 who sacrificed himself for his best friend to continue the game is so kind\n",
      "     0.9543  Positive           I absolutely love Lee Yoomi (she’s so giggly and fun) and HoYeon Jung. Their friendship in and out of Squid Game is so stable- I also love the moment in Squid Game when Ji-yeong dropped her marble..\n",
      "     0.9337  Positive           Just finished this series last end of December and honestly speaking I just watched this trailer when I sent this comment. til now this series still hooked me with superb and excellent story. Kudos to Squid game team\n",
      "     0.9329  Positive           Well done!!!!\n",
      "                                I was literally waiting and wishing that you should post the explanation of squid game it's a bit emotional but so much interesting  please keep make these type of movie's explanation thank you\n",
      "     0.9324  Positive           The set looked amazing and that glass game looked as tense as I could ever\n",
      "                                imagine. Awesome job MrBeast. Congrats to the winner!\n",
      "     0.9323  Positive           Hi all, I just want to point out that a lot of the scenes in this concept teaser came from a Chinese survival/action film called \"Animal World\"(Dong Wu Shi Jie), with Yifeng Li and Michael Douglas. In a similar style as the Squid Game but with crazier visuals. As a Squid Game fan myself, I think you should enjoy that film too! Anyone that sees this comment, have a great day!\n"
     ]
    }
   ],
   "source": [
    "def text_blob_sentiment_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def texblob_check_sentiment(score):\n",
    "    if score == 0:\n",
    "        return 'Neutral'\n",
    "    elif score < 0.00:\n",
    "        return 'Negative'\n",
    "    elif score > 0.00:\n",
    "        return 'Positive'\n",
    "\n",
    "new_df['textblob_score'] = new_df['c_text'].apply(text_blob_sentiment_score)\n",
    "new_df['textblob_sentiment'] = new_df['textblob_score'].apply(texblob_check_sentiment)\n",
    "\n",
    "def textblob_select_top_comments(df, top_n=10):\n",
    "    top_comments = []\n",
    "    grouped = df.groupby('textblob_sentiment')\n",
    "\n",
    "    for sentiment, group in grouped:\n",
    "        top_group_comments = group.sort_values(by='textblob_score', ascending=False).head(top_n)[['textblob_score', 'textblob_sentiment', 'c_text']].values.tolist()\n",
    "        top_comments.extend([(sentiment_score, sentiment, comment) for sentiment_score, sentiment, comment in top_group_comments])\n",
    "\n",
    "    return top_comments\n",
    "\n",
    "textblob_top_comments = select_top_comments(new_df, top_n=10)\n",
    "\n",
    "headers = [\"Sentiment\", \"Sentiment Score\", \"Comment\"]\n",
    "print(tabulate(textblob_top_comments, headers=headers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998     None\n",
       "3996     None\n",
       "17478    None\n",
       "17430    None\n",
       "17472    None\n",
       "         ... \n",
       "17292    None\n",
       "13630    None\n",
       "16807    None\n",
       "17585    None\n",
       "9431     None\n",
       "Length: 500, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = ['positive', 'negative', 'neutral']\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def col_to_txt(row):\n",
    "    sentiment = row['sentiment']  \n",
    "    c_text = row['c_text']\n",
    "    file_name = f\"{sentiment}_{row.c_id}.txt\"  \n",
    "    folder = f\"{sentiment.strip()}\"  \n",
    "    file_path = os.path.join(folder, file_name)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(c_text)\n",
    "\n",
    "new_df.apply(col_to_txt, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'final_comments_df.csv'\n",
    "# files_present = glob.glob(filename)\n",
    "# # will only write to disk if file doesnt exist\n",
    "# if not files_present:\n",
    "#     new_df.to_csv(filename, index=False)\n",
    "#     new_df\n",
    "# else:\n",
    "#     print (f'File Already Exists. Delete {filename}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'positive'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Iterate through each sentiment folder\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentiment \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Get the list of file paths in the current sentiment folder\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Shuffle the file paths\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(files)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'positive'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the root directory containing the positive, negative, and neutral folders\n",
    "root_dir = ''\n",
    "\n",
    "# Define the directories for train and test sets\n",
    "train_dir = 'data/train'\n",
    "test_dir = 'data/test'\n",
    "try:\n",
    "    shutil.rmtree(os.path.join(root_dir, 'negative'))\n",
    "    shutil.rmtree(os.path.join(root_dir, 'neutral'))\n",
    "    shutil.rmtree(os.path.join(root_dir, 'positive'))\n",
    "except:\n",
    "    pass\n",
    "# Define the ratio for train-test split\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Iterate through each sentiment folder\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    # Get the list of file paths in the current sentiment folder\n",
    "    files = os.listdir(os.path.join(root_dir, sentiment))\n",
    "    # Shuffle the file paths\n",
    "    random.shuffle(files)\n",
    "    # Calculate the split index based on the split ratio\n",
    "    split_index = int(len(files) * split_ratio)\n",
    "    # Split the files into train and test sets\n",
    "    train_files = files[:split_index]\n",
    "    test_files = files[split_index:]\n",
    "    \n",
    "    # Move train files to train directory\n",
    "    for file in train_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(train_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "    \n",
    "    # Move test files to test directory\n",
    "    for file in test_files:\n",
    "        src = os.path.join(root_dir, sentiment, file)\n",
    "        dst = os.path.join(test_dir, sentiment, file)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        # Check if the file already exists in the destination directory\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Remove the sentiment folders\n",
    "try:\n",
    "    shutil.rmtree('negative')\n",
    "    shutil.rmtree( 'neutral')\n",
    "    shutil.rmtree('positive')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Text Analytics Pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(train_dir)\n",
    "reviews_test = load_files(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe2\\x80\\x9cMaybe the real Squid Game are the friends \\r\\nwe lost along the way.\\xe2\\x80\\x9d :(' 0\n",
      "b'I can surely say that part 2 of squid game will come' 2\n",
      "b'The squid game is a death game ' 0\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(text_train[1],y_train[1])\n",
    "print(text_train[2],y_train[2])\n",
    "print(text_train[3],y_train[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 5 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect = CountVectorizer().fit(reviews_train)\n",
    "X_train = vect.transform(reviews_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_tokens = ['drama', 'film', 'cinema', 'actor', 'actress', 'director', 'plot',\n",
    "                         'scene', 'genre', 'subtitles', 'k-drama', 'kdrama', 'k-movie', 'television',\n",
    "                         'episode', 'screenplay', 'script', 'cinematography', 'soundtrack',\n",
    "                         'OST', 'character', 'plot twist', 'review', 'ratings', 'premiere',\n",
    "                         'streaming', 'watchlist', 'subbed', 'dubbed', 'sequel', 'game', 'song',\n",
    "                         'season', 'trailer', 'casting', 'fanbase', 'recommendation', 'goblin',\n",
    "                         'viewer', 'critic', 'Korean', 'entertainment', 'watched', 'guardian',\n",
    "                         'show', 'squid', 'watch', 'watching', 'acting', 'netflix', 'show',\n",
    "                         'end',\n",
    "                          'squid game', 'gi-hun', 'Sang-woo', 'Player', 'Red light', 'green light', 'Honeycomb',\n",
    "                            'Tug of war', 'Marbles', 'Front man', 'VIPs', 'Doll', 'Coffin', 'Square', 'Triangle', \n",
    "                            'Circle', 'Death game', 'death', 'Survival game', 'Money', 'prize', 'Il-nam', 'Hwang Jun-ho'\n",
    "                            'director', 'Cho Sang-woo', 'Masked man', 'Childhood', 'game', 'Pink soldier', 'Betrayal',\n",
    "                            'Seong Gi-hun', 'Survival', 'Games', 'Competition', 'Squid', 'Masks', 'ali', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Define the pattern to match punctuation\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    # Replace punctuation with an empty string\n",
    "    text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    return text_without_punctuation\n",
    "\n",
    "# Text Processing\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # stopwords punctuation etc\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    # stemmer = PorterStemmer()\n",
    "    # split into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    # removes stopwords and numbers and stems from tokens makes sure its all lowercase too\n",
    "    tokens = [stemmer.stem(remove_punctuation(token)) for token in tokens if token.isalnum() and token.lower() not in product_tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90       130\n",
      "           1       0.86      0.96      0.91       137\n",
      "           2       0.99      0.92      0.95       451\n",
      "\n",
      "    accuracy                           0.93       718\n",
      "   macro avg       0.90      0.95      0.92       718\n",
      "weighted avg       0.94      0.93      0.94       718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('preprocess', \n",
    "    TfidfVectorizer(\n",
    "                    encoding=\"utf-8\",\n",
    "                    strip_accents='ascii',\n",
    "                    lowercase=True,\n",
    "                    preprocessor=preprocess_text,\n",
    "                    # tokenizer=,\n",
    "                    # analyzer=,\n",
    "                    stop_words='english',\n",
    "                    norm='l2',\n",
    "                    ngram_range=(1, 1),\n",
    "                    max_df=0.09,\n",
    "                    min_df=0.003,\n",
    "                    max_features=500,\n",
    "                    binary=True,\n",
    "                    use_idf=True,\n",
    "                    smooth_idf=True,\n",
    "                    sublinear_tf=True\n",
    "                    )\n",
    "    # CountVectorizer(preprocessor=preprocess_text,ngram_range=(1, 1))\n",
    "     ), \n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "text_clf.fit(text_train, y_train)\n",
    "y_pred = text_clf.predict(text_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Run only till here and check coz the grid search would take long so better to adjust by looking at this only ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 31\u001b[0m\n\u001b[0;32m      6\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m: [TfidfVectorizer()],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# 'vectorizer__sublinear_tf': [True,False]\u001b[39;00m\n\u001b[0;32m     28\u001b[0m }\n\u001b[0;32m     30\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(text_clf, parameters, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__norm': ['l2','l1'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'vectorizer__use_idf': [True,False],\n",
    "    'vectorizer__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 889: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 925: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1033: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1069: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 893: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 929: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1037: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 1073: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 897: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n",
      "Model 933: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'vectorizer': TfidfVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__norm': 'l2', 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__smooth_idf': True, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii', 'vectorizer__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "results = grid_search.cv_results_\n",
    " \n",
    "scores = results['mean_test_score']\n",
    "\n",
    "params = results['params']\n",
    "\n",
    "top_models_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores[i], params[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ishaq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94        79\n",
      "           1       0.96      0.95      0.95        74\n",
      "           2       0.97      1.00      0.98       217\n",
      "\n",
      "    accuracy                           0.97       370\n",
      "   macro avg       0.97      0.95      0.96       370\n",
      "weighted avg       0.97      0.97      0.97       370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_ct = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "\n",
    "parameters_ct = {\n",
    "    'vectorizer': [CountVectorizer()],\n",
    "    'classifier': [\n",
    "        MultinomialNB(),\n",
    "        SVC(),\n",
    "        LogisticRegression()\n",
    "    ],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'vectorizer__preprocessor': [preprocess_text],\n",
    "    'vectorizer__encoding': ['utf-8'],\n",
    "    'vectorizer__binary': [False, True],\n",
    "    'vectorizer__lowercase': [False, True],\n",
    "    'vectorizer__encoding': [\"utf-8\"],\n",
    "    'vectorizer__strip_accents': ['ascii'],\n",
    "    'vectorizer__stop_words': ['english'],\n",
    "    'vectorizer__max_df': [0.1,0.09,0.08,0.07],\n",
    "    'vectorizer__min_df': [0.004,0.003,0.002],\n",
    "    'tfidf__norm': ['l2','l1'],\n",
    "    # 'vectorizer__max_features': [500],\n",
    "    'tfidf__use_idf': [True,False],\n",
    "    'tfidf__smooth_idf': [True],\n",
    "    # 'vectorizer__sublinear_tf': [True,False]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_ct = GridSearchCV(pipeline_ct, parameters_ct, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "grid_search_ct.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_search_ct.best_params_)\n",
    "best_model_ct = grid_search_ct.best_estimator_\n",
    "y_pred_ct = best_model_ct.predict(text_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 655: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 664: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 691: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 700: Mean Test Score - 0.9204, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 1), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 656: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 665: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 692: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 701: Mean Test Score - 0.9169, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': True, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 2), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 657: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.1, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n",
      "Model 666: Mean Test Score - 0.9143, Parameters - {'classifier': SVC(), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__use_idf': True, 'vectorizer': CountVectorizer(), 'vectorizer__binary': True, 'vectorizer__encoding': 'utf-8', 'vectorizer__lowercase': False, 'vectorizer__max_df': 0.09, 'vectorizer__min_df': 0.002, 'vectorizer__ngram_range': (1, 3), 'vectorizer__preprocessor': <function preprocess_text at 0x0000025C8002BDC0>, 'vectorizer__stop_words': 'english', 'vectorizer__strip_accents': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "results_ct = grid_search_ct.cv_results_\n",
    " \n",
    "scores_ct = results_ct['mean_test_score']\n",
    "\n",
    "params_ct = results_ct['params']\n",
    "\n",
    "top_models_indices_ct = sorted(range(len(scores_ct)), key=lambda i: scores_ct[i], reverse=True)[:10]\n",
    " \n",
    "for i in top_models_indices_ct:\n",
    "    print(\"Model {}: Mean Test Score - {:.4f}, Parameters - {}\".format(i+1, scores_ct[i], params_ct[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualization and Insights:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Discussion and conclusion from experiments:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
